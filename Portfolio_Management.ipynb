{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d71177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/mxqr91nj7135zkb7ptp3mq5c0000gn/T/ipykernel_12380/3166594014.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  mlp.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "# 1. Import libraries:\n",
    "%matplotlib inline\n",
    "import os\n",
    "import quandl\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as spo\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "from financial_data import *\n",
    "import tensorflow as tf\n",
    "mlp.style.use('seaborn')\n",
    "quandl.save_key('HtwBLPt3k37yZHTvy15K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96aa0536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol     Security             GICS Sector               GICS Sub-Industry  \\\n",
       "0    MMM           3M             Industrials        Industrial Conglomerates   \n",
       "1    AOS  A. O. Smith             Industrials               Building Products   \n",
       "2    ABT       Abbott             Health Care           Health Care Equipment   \n",
       "3   ABBV       AbbVie             Health Care                 Pharmaceuticals   \n",
       "4    ACN    Accenture  Information Technology  IT Consulting & Other Services   \n",
       "\n",
       "     Headquarters Location  Date added      CIK      Founded  \n",
       "0    Saint Paul, Minnesota  1957-03-04    66740         1902  \n",
       "1     Milwaukee, Wisconsin  2017-07-26    91142         1916  \n",
       "2  North Chicago, Illinois  1957-03-04     1800         1888  \n",
       "3  North Chicago, Illinois  2012-12-31  1551152  2013 (1888)  \n",
       "4          Dublin, Ireland  2011-07-06  1467373         1989  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "sp500 = pd.read_html(sp_url, header=0)[0]\n",
    "sp500.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f7b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stocks in the universe is: 243\n"
     ]
    }
   ],
   "source": [
    "# Correct invalid dates:\n",
    "sp500.loc[sp500[sp500['Date added']=='1983-11-30 (1957-03-04)'].index,'Date added'] = '1983-11-30'\n",
    "sp500.loc[sp500[sp500['Date added']=='2001?'].index,'Date added'] = '2001-01-01'\n",
    "# Filter firms that entered the index after December 2015:\n",
    "sp500['Date added'] = pd.to_datetime(sp500['Date added'],format='%Y-%m-%d')\n",
    "sp500 = sp500[sp500['Date added']<'2007-01-01']\n",
    "print(\"The number of stocks in the universe is:\", sp500.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0cd3d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIS', 'BIIB', 'GILD', 'ETR', 'PEP', 'NVDA', 'NI', 'BBWI', 'SWK', 'CBRE']\n"
     ]
    }
   ],
   "source": [
    "n_stocks = 10\n",
    "np.random.seed(1792)\n",
    "universe_tickers = sp500['Symbol'].unique()\n",
    "tickers = list(np.random.choice(universe_tickers,replace=False,size=n_stocks))\n",
    "print(tickers)\n",
    "# sp500[sp500['Symbol'].isin(portfolio_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1573d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  10 of 10 completed\n",
      "            Adj Close                                                \\\n",
      "                 BBWI        BIIB       CBRE        DIS         ETR   \n",
      "Date                                                                  \n",
      "2007-01-03  10.077243   49.330002  33.650002  28.317097   46.684509   \n",
      "2007-01-04   9.317530   49.750000  33.340000  28.540653   47.236816   \n",
      "2007-01-05   9.310719   49.759998  33.099998  28.308819   46.071968   \n",
      "2007-01-08   9.130158   50.020000  33.220001  28.565489   45.725525   \n",
      "2007-01-09   9.266429   49.500000  33.970001  28.524092   45.695412   \n",
      "...               ...         ...        ...        ...         ...   \n",
      "2022-12-23  41.287586  279.160004  76.669998  88.010002  109.924477   \n",
      "2022-12-27  41.297432  274.769989  76.489998  86.370003  111.164909   \n",
      "2022-12-28  40.116104  274.040009  75.410004  84.169998  109.478691   \n",
      "2022-12-29  41.002106  276.000000  77.550003  87.180000  110.564072   \n",
      "2022-12-30  41.484478  276.920013  76.959999  86.879997  109.023216   \n",
      "\n",
      "                                                                     ...  \\\n",
      "                 GILD         NI        NVDA         PEP        SWK  ...   \n",
      "Date                                                                 ...   \n",
      "2007-01-03  11.958693   4.979852    5.516786   38.873486  34.064186  ...   \n",
      "2007-01-04  12.155637   4.961384    5.490793   39.140011  34.278870  ...   \n",
      "2007-01-05  12.159424   4.836224    5.146760   39.016048  34.144688  ...   \n",
      "2007-01-08  12.163209   4.828014    5.184986   39.102818  34.017227  ...   \n",
      "2007-01-09  12.310921   4.842379    5.084069   39.263954  34.634388  ...   \n",
      "...               ...        ...         ...         ...        ...  ...   \n",
      "2022-12-23  83.147980  26.970551  152.005920  178.524826  71.870193  ...   \n",
      "2022-12-27  83.403076  27.058151  141.159790  179.318222  72.327049  ...   \n",
      "2022-12-28  82.971390  26.814819  140.310074  178.025269  70.159462  ...   \n",
      "2022-12-29  83.648346  27.097084  145.978073  178.250565  73.785049  ...   \n",
      "2022-12-30  84.227188  26.688292  146.088028  176.957626  73.017166  ...   \n",
      "\n",
      "                  Low                                                \\\n",
      "                 BBWI        BIIB       CBRE        DIS         ETR   \n",
      "Date                                                                  \n",
      "2007-01-03  23.540825   48.200001  33.139999  33.531136   92.089996   \n",
      "2007-01-04  21.891672   48.880001  33.099998  33.708706   92.599998   \n",
      "2007-01-05  21.147940   49.509998  32.799999  33.531136   91.279999   \n",
      "2007-01-08  21.471302   48.910000  32.500000  33.610054   90.870003   \n",
      "2007-01-09  21.827002   49.299999  33.060001  33.491676   90.540001   \n",
      "...               ...         ...        ...        ...         ...   \n",
      "2022-12-23  40.150002  276.070007  75.150002  85.769997  112.339996   \n",
      "2022-12-27  41.810001  273.380005  76.099998  85.959999  113.190002   \n",
      "2022-12-28  40.630001  272.640015  75.379997  84.070000  112.930000   \n",
      "2022-12-29  41.180000  274.299988  75.769997  84.970001  112.500000   \n",
      "2022-12-30  40.880001  272.200012  76.089996  85.230003  111.489998   \n",
      "\n",
      "                                                                     \n",
      "                 GILD         NI        NVDA         PEP        SWK  \n",
      "Date                                                                 \n",
      "2007-01-03  15.480000   9.469548    5.798333   62.450001  50.049999  \n",
      "2007-01-04  15.617500   9.469548    5.838333   62.500000  50.259998  \n",
      "2007-01-05  15.962500   9.229862    5.570000   62.700001  50.590000  \n",
      "2007-01-08  15.877500   9.182711    5.533333   62.860001  49.950001  \n",
      "2007-01-09  16.129999   9.210216    5.535000   63.009998  50.680000  \n",
      "...               ...        ...         ...         ...        ...  \n",
      "2022-12-23  84.279999  27.330000  148.830002  180.449997  72.599998  \n",
      "2022-12-27  84.690002  27.549999  140.559998  182.270004  73.250000  \n",
      "2022-12-28  84.449997  27.520000  138.839996  181.639999  72.160004  \n",
      "2022-12-29  84.610001  27.639999  142.270004  181.889999  72.769997  \n",
      "2022-12-30  84.779999  27.230000  142.330002  179.289993  74.330002  \n",
      "\n",
      "[4028 rows x 50 columns]\n",
      "the dataframe before the columns were renamed\n",
      "MultiIndex([('Adj Close', 'BBWI'),\n",
      "            ('Adj Close', 'BIIB'),\n",
      "            ('Adj Close', 'CBRE'),\n",
      "            ('Adj Close',  'DIS'),\n",
      "            ('Adj Close',  'ETR'),\n",
      "            ('Adj Close', 'GILD'),\n",
      "            ('Adj Close',   'NI'),\n",
      "            ('Adj Close', 'NVDA'),\n",
      "            ('Adj Close',  'PEP'),\n",
      "            ('Adj Close',  'SWK'),\n",
      "            (   'Volume', 'BBWI'),\n",
      "            (   'Volume', 'BIIB'),\n",
      "            (   'Volume', 'CBRE'),\n",
      "            (   'Volume',  'DIS'),\n",
      "            (   'Volume',  'ETR'),\n",
      "            (   'Volume', 'GILD'),\n",
      "            (   'Volume',   'NI'),\n",
      "            (   'Volume', 'NVDA'),\n",
      "            (   'Volume',  'PEP'),\n",
      "            (   'Volume',  'SWK'),\n",
      "            (     'Open', 'BBWI'),\n",
      "            (     'Open', 'BIIB'),\n",
      "            (     'Open', 'CBRE'),\n",
      "            (     'Open',  'DIS'),\n",
      "            (     'Open',  'ETR'),\n",
      "            (     'Open', 'GILD'),\n",
      "            (     'Open',   'NI'),\n",
      "            (     'Open', 'NVDA'),\n",
      "            (     'Open',  'PEP'),\n",
      "            (     'Open',  'SWK'),\n",
      "            (     'High', 'BBWI'),\n",
      "            (     'High', 'BIIB'),\n",
      "            (     'High', 'CBRE'),\n",
      "            (     'High',  'DIS'),\n",
      "            (     'High',  'ETR'),\n",
      "            (     'High', 'GILD'),\n",
      "            (     'High',   'NI'),\n",
      "            (     'High', 'NVDA'),\n",
      "            (     'High',  'PEP'),\n",
      "            (     'High',  'SWK'),\n",
      "            (      'Low', 'BBWI'),\n",
      "            (      'Low', 'BIIB'),\n",
      "            (      'Low', 'CBRE'),\n",
      "            (      'Low',  'DIS'),\n",
      "            (      'Low',  'ETR'),\n",
      "            (      'Low', 'GILD'),\n",
      "            (      'Low',   'NI'),\n",
      "            (      'Low', 'NVDA'),\n",
      "            (      'Low',  'PEP'),\n",
      "            (      'Low',  'SWK')],\n",
      "           )\n",
      "Index(['Adj Close_BBWI', 'Adj Close_BIIB', 'Adj Close_CBRE', 'Adj Close_DIS',\n",
      "       'Adj Close_ETR', 'Adj Close_GILD', 'Adj Close_NI', 'Adj Close_NVDA',\n",
      "       'Adj Close_PEP', 'Adj Close_SWK', 'Volume_BBWI', 'Volume_BIIB',\n",
      "       'Volume_CBRE', 'Volume_DIS', 'Volume_ETR', 'Volume_GILD', 'Volume_NI',\n",
      "       'Volume_NVDA', 'Volume_PEP', 'Volume_SWK', 'Open_BBWI', 'Open_BIIB',\n",
      "       'Open_CBRE', 'Open_DIS', 'Open_ETR', 'Open_GILD', 'Open_NI',\n",
      "       'Open_NVDA', 'Open_PEP', 'Open_SWK', 'High_BBWI', 'High_BIIB',\n",
      "       'High_CBRE', 'High_DIS', 'High_ETR', 'High_GILD', 'High_NI',\n",
      "       'High_NVDA', 'High_PEP', 'High_SWK', 'Low_BBWI', 'Low_BIIB', 'Low_CBRE',\n",
      "       'Low_DIS', 'Low_ETR', 'Low_GILD', 'Low_NI', 'Low_NVDA', 'Low_PEP',\n",
      "       'Low_SWK'],\n",
      "      dtype='object')\n",
      "the dataframe after the columns have been renamed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close_BBWI</th>\n",
       "      <th>Close_BIIB</th>\n",
       "      <th>Close_CBRE</th>\n",
       "      <th>Close_DIS</th>\n",
       "      <th>Close_ETR</th>\n",
       "      <th>Close_GILD</th>\n",
       "      <th>Close_NI</th>\n",
       "      <th>Close_NVDA</th>\n",
       "      <th>Close_PEP</th>\n",
       "      <th>Close_SWK</th>\n",
       "      <th>...</th>\n",
       "      <th>Low_BBWI</th>\n",
       "      <th>Low_BIIB</th>\n",
       "      <th>Low_CBRE</th>\n",
       "      <th>Low_DIS</th>\n",
       "      <th>Low_ETR</th>\n",
       "      <th>Low_GILD</th>\n",
       "      <th>Low_NI</th>\n",
       "      <th>Low_NVDA</th>\n",
       "      <th>Low_PEP</th>\n",
       "      <th>Low_SWK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>10.077243</td>\n",
       "      <td>49.330002</td>\n",
       "      <td>33.650002</td>\n",
       "      <td>28.317097</td>\n",
       "      <td>46.684509</td>\n",
       "      <td>11.958693</td>\n",
       "      <td>4.979852</td>\n",
       "      <td>5.516786</td>\n",
       "      <td>38.873486</td>\n",
       "      <td>34.064186</td>\n",
       "      <td>...</td>\n",
       "      <td>23.540825</td>\n",
       "      <td>48.200001</td>\n",
       "      <td>33.139999</td>\n",
       "      <td>33.531136</td>\n",
       "      <td>92.089996</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>9.469548</td>\n",
       "      <td>5.798333</td>\n",
       "      <td>62.450001</td>\n",
       "      <td>50.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>9.317530</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>33.340000</td>\n",
       "      <td>28.540653</td>\n",
       "      <td>47.236816</td>\n",
       "      <td>12.155637</td>\n",
       "      <td>4.961384</td>\n",
       "      <td>5.490793</td>\n",
       "      <td>39.140011</td>\n",
       "      <td>34.278870</td>\n",
       "      <td>...</td>\n",
       "      <td>21.891672</td>\n",
       "      <td>48.880001</td>\n",
       "      <td>33.099998</td>\n",
       "      <td>33.708706</td>\n",
       "      <td>92.599998</td>\n",
       "      <td>15.617500</td>\n",
       "      <td>9.469548</td>\n",
       "      <td>5.838333</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>50.259998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>9.310719</td>\n",
       "      <td>49.759998</td>\n",
       "      <td>33.099998</td>\n",
       "      <td>28.308819</td>\n",
       "      <td>46.071968</td>\n",
       "      <td>12.159424</td>\n",
       "      <td>4.836224</td>\n",
       "      <td>5.146760</td>\n",
       "      <td>39.016048</td>\n",
       "      <td>34.144688</td>\n",
       "      <td>...</td>\n",
       "      <td>21.147940</td>\n",
       "      <td>49.509998</td>\n",
       "      <td>32.799999</td>\n",
       "      <td>33.531136</td>\n",
       "      <td>91.279999</td>\n",
       "      <td>15.962500</td>\n",
       "      <td>9.229862</td>\n",
       "      <td>5.570000</td>\n",
       "      <td>62.700001</td>\n",
       "      <td>50.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>9.130158</td>\n",
       "      <td>50.020000</td>\n",
       "      <td>33.220001</td>\n",
       "      <td>28.565489</td>\n",
       "      <td>45.725525</td>\n",
       "      <td>12.163209</td>\n",
       "      <td>4.828014</td>\n",
       "      <td>5.184986</td>\n",
       "      <td>39.102818</td>\n",
       "      <td>34.017227</td>\n",
       "      <td>...</td>\n",
       "      <td>21.471302</td>\n",
       "      <td>48.910000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>33.610054</td>\n",
       "      <td>90.870003</td>\n",
       "      <td>15.877500</td>\n",
       "      <td>9.182711</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>49.950001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-09</th>\n",
       "      <td>9.266429</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>33.970001</td>\n",
       "      <td>28.524092</td>\n",
       "      <td>45.695412</td>\n",
       "      <td>12.310921</td>\n",
       "      <td>4.842379</td>\n",
       "      <td>5.084069</td>\n",
       "      <td>39.263954</td>\n",
       "      <td>34.634388</td>\n",
       "      <td>...</td>\n",
       "      <td>21.827002</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>33.060001</td>\n",
       "      <td>33.491676</td>\n",
       "      <td>90.540001</td>\n",
       "      <td>16.129999</td>\n",
       "      <td>9.210216</td>\n",
       "      <td>5.535000</td>\n",
       "      <td>63.009998</td>\n",
       "      <td>50.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>41.287586</td>\n",
       "      <td>279.160004</td>\n",
       "      <td>76.669998</td>\n",
       "      <td>88.010002</td>\n",
       "      <td>109.924477</td>\n",
       "      <td>83.147980</td>\n",
       "      <td>26.970551</td>\n",
       "      <td>152.005920</td>\n",
       "      <td>178.524826</td>\n",
       "      <td>71.870193</td>\n",
       "      <td>...</td>\n",
       "      <td>40.150002</td>\n",
       "      <td>276.070007</td>\n",
       "      <td>75.150002</td>\n",
       "      <td>85.769997</td>\n",
       "      <td>112.339996</td>\n",
       "      <td>84.279999</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>148.830002</td>\n",
       "      <td>180.449997</td>\n",
       "      <td>72.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>41.297432</td>\n",
       "      <td>274.769989</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>86.370003</td>\n",
       "      <td>111.164909</td>\n",
       "      <td>83.403076</td>\n",
       "      <td>27.058151</td>\n",
       "      <td>141.159790</td>\n",
       "      <td>179.318222</td>\n",
       "      <td>72.327049</td>\n",
       "      <td>...</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>273.380005</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>85.959999</td>\n",
       "      <td>113.190002</td>\n",
       "      <td>84.690002</td>\n",
       "      <td>27.549999</td>\n",
       "      <td>140.559998</td>\n",
       "      <td>182.270004</td>\n",
       "      <td>73.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>40.116104</td>\n",
       "      <td>274.040009</td>\n",
       "      <td>75.410004</td>\n",
       "      <td>84.169998</td>\n",
       "      <td>109.478691</td>\n",
       "      <td>82.971390</td>\n",
       "      <td>26.814819</td>\n",
       "      <td>140.310074</td>\n",
       "      <td>178.025269</td>\n",
       "      <td>70.159462</td>\n",
       "      <td>...</td>\n",
       "      <td>40.630001</td>\n",
       "      <td>272.640015</td>\n",
       "      <td>75.379997</td>\n",
       "      <td>84.070000</td>\n",
       "      <td>112.930000</td>\n",
       "      <td>84.449997</td>\n",
       "      <td>27.520000</td>\n",
       "      <td>138.839996</td>\n",
       "      <td>181.639999</td>\n",
       "      <td>72.160004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>41.002106</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>77.550003</td>\n",
       "      <td>87.180000</td>\n",
       "      <td>110.564072</td>\n",
       "      <td>83.648346</td>\n",
       "      <td>27.097084</td>\n",
       "      <td>145.978073</td>\n",
       "      <td>178.250565</td>\n",
       "      <td>73.785049</td>\n",
       "      <td>...</td>\n",
       "      <td>41.180000</td>\n",
       "      <td>274.299988</td>\n",
       "      <td>75.769997</td>\n",
       "      <td>84.970001</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>84.610001</td>\n",
       "      <td>27.639999</td>\n",
       "      <td>142.270004</td>\n",
       "      <td>181.889999</td>\n",
       "      <td>72.769997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>41.484478</td>\n",
       "      <td>276.920013</td>\n",
       "      <td>76.959999</td>\n",
       "      <td>86.879997</td>\n",
       "      <td>109.023216</td>\n",
       "      <td>84.227188</td>\n",
       "      <td>26.688292</td>\n",
       "      <td>146.088028</td>\n",
       "      <td>176.957626</td>\n",
       "      <td>73.017166</td>\n",
       "      <td>...</td>\n",
       "      <td>40.880001</td>\n",
       "      <td>272.200012</td>\n",
       "      <td>76.089996</td>\n",
       "      <td>85.230003</td>\n",
       "      <td>111.489998</td>\n",
       "      <td>84.779999</td>\n",
       "      <td>27.230000</td>\n",
       "      <td>142.330002</td>\n",
       "      <td>179.289993</td>\n",
       "      <td>74.330002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4028 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close_BBWI  Close_BIIB  Close_CBRE  Close_DIS   Close_ETR  \\\n",
       "Date                                                                    \n",
       "2007-01-03   10.077243   49.330002   33.650002  28.317097   46.684509   \n",
       "2007-01-04    9.317530   49.750000   33.340000  28.540653   47.236816   \n",
       "2007-01-05    9.310719   49.759998   33.099998  28.308819   46.071968   \n",
       "2007-01-08    9.130158   50.020000   33.220001  28.565489   45.725525   \n",
       "2007-01-09    9.266429   49.500000   33.970001  28.524092   45.695412   \n",
       "...                ...         ...         ...        ...         ...   \n",
       "2022-12-23   41.287586  279.160004   76.669998  88.010002  109.924477   \n",
       "2022-12-27   41.297432  274.769989   76.489998  86.370003  111.164909   \n",
       "2022-12-28   40.116104  274.040009   75.410004  84.169998  109.478691   \n",
       "2022-12-29   41.002106  276.000000   77.550003  87.180000  110.564072   \n",
       "2022-12-30   41.484478  276.920013   76.959999  86.879997  109.023216   \n",
       "\n",
       "            Close_GILD   Close_NI  Close_NVDA   Close_PEP  Close_SWK  ...  \\\n",
       "Date                                                                  ...   \n",
       "2007-01-03   11.958693   4.979852    5.516786   38.873486  34.064186  ...   \n",
       "2007-01-04   12.155637   4.961384    5.490793   39.140011  34.278870  ...   \n",
       "2007-01-05   12.159424   4.836224    5.146760   39.016048  34.144688  ...   \n",
       "2007-01-08   12.163209   4.828014    5.184986   39.102818  34.017227  ...   \n",
       "2007-01-09   12.310921   4.842379    5.084069   39.263954  34.634388  ...   \n",
       "...                ...        ...         ...         ...        ...  ...   \n",
       "2022-12-23   83.147980  26.970551  152.005920  178.524826  71.870193  ...   \n",
       "2022-12-27   83.403076  27.058151  141.159790  179.318222  72.327049  ...   \n",
       "2022-12-28   82.971390  26.814819  140.310074  178.025269  70.159462  ...   \n",
       "2022-12-29   83.648346  27.097084  145.978073  178.250565  73.785049  ...   \n",
       "2022-12-30   84.227188  26.688292  146.088028  176.957626  73.017166  ...   \n",
       "\n",
       "             Low_BBWI    Low_BIIB   Low_CBRE    Low_DIS     Low_ETR  \\\n",
       "Date                                                                  \n",
       "2007-01-03  23.540825   48.200001  33.139999  33.531136   92.089996   \n",
       "2007-01-04  21.891672   48.880001  33.099998  33.708706   92.599998   \n",
       "2007-01-05  21.147940   49.509998  32.799999  33.531136   91.279999   \n",
       "2007-01-08  21.471302   48.910000  32.500000  33.610054   90.870003   \n",
       "2007-01-09  21.827002   49.299999  33.060001  33.491676   90.540001   \n",
       "...               ...         ...        ...        ...         ...   \n",
       "2022-12-23  40.150002  276.070007  75.150002  85.769997  112.339996   \n",
       "2022-12-27  41.810001  273.380005  76.099998  85.959999  113.190002   \n",
       "2022-12-28  40.630001  272.640015  75.379997  84.070000  112.930000   \n",
       "2022-12-29  41.180000  274.299988  75.769997  84.970001  112.500000   \n",
       "2022-12-30  40.880001  272.200012  76.089996  85.230003  111.489998   \n",
       "\n",
       "             Low_GILD     Low_NI    Low_NVDA     Low_PEP    Low_SWK  \n",
       "Date                                                                 \n",
       "2007-01-03  15.480000   9.469548    5.798333   62.450001  50.049999  \n",
       "2007-01-04  15.617500   9.469548    5.838333   62.500000  50.259998  \n",
       "2007-01-05  15.962500   9.229862    5.570000   62.700001  50.590000  \n",
       "2007-01-08  15.877500   9.182711    5.533333   62.860001  49.950001  \n",
       "2007-01-09  16.129999   9.210216    5.535000   63.009998  50.680000  \n",
       "...               ...        ...         ...         ...        ...  \n",
       "2022-12-23  84.279999  27.330000  148.830002  180.449997  72.599998  \n",
       "2022-12-27  84.690002  27.549999  140.559998  182.270004  73.250000  \n",
       "2022-12-28  84.449997  27.520000  138.839996  181.639999  72.160004  \n",
       "2022-12-29  84.610001  27.639999  142.270004  181.889999  72.769997  \n",
       "2022-12-30  84.779999  27.230000  142.330002  179.289993  74.330002  \n",
       "\n",
       "[4028 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tickers = ['AAPL', 'GOOGL', 'MSFT','SPY']\n",
    "# start_date='2007-01-01'\n",
    "# end_date='2022-12-31'\n",
    "# columns = ['Adj Close', 'Volume']\n",
    "# # Download data\n",
    "# data = yf.download(tickers, start=start_date, end=end_date)[columns]\n",
    "\n",
    "# # 'data' will be a Pandas DataFrame containing the historical data for the specified tickers\n",
    "# print(data)\n",
    "\n",
    "\n",
    "def download_data(tickers, start_date, end_date, columns):\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)\n",
    "    data=data[columns]\n",
    "    return data\n",
    "\n",
    "# Define the tickers, start date, end date, and columns you want to download\n",
    "# tickers = ['AAPL', 'GOOGL', 'MSFT']\n",
    "start_date = '2007-01-01'\n",
    "end_date = '2022-12-31'\n",
    "columns = [ 'Adj Close', 'Volume','Open','High','Low']\n",
    "\n",
    "# Download the data\n",
    "data = download_data(tickers, start_date, end_date, columns)\n",
    "print(data)\n",
    "\n",
    "\n",
    "def one_lvl_colnames(df,cols,tickers):\n",
    "    \"\"\"This function changes a multi-level column indexation into a one level\n",
    "    column indexation\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    df (pandas Dataframe): dataframe with the columns whose indexation will be \n",
    "        flattened.\n",
    "    tickers (list|string): list/string with the tickers (s) in the data frame df.\n",
    "    cols (list|string): list/string with the name of the columns (e.g. 'Adj Close',\n",
    "        'High', 'Close', etc.) that are in the dataframe df.\n",
    "    \n",
    "    Ouputs:\n",
    "    -------\n",
    "    df (pandas Dataframe): dataframe with the same information as df, but \n",
    "        with one level of indexation.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"the dataframe before the columns were renamed\")\n",
    "#     print(df.columns)\n",
    "    df_not_renamed=df.copy()\n",
    "#     df_not_renamed.drop(\"Open\",inplace=True)\n",
    "    print(df_not_renamed.columns)\n",
    "    # Define important variables:\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    if isinstance(cols, str):\n",
    "        \n",
    "        cols = [cols]\n",
    "        print(cols)\n",
    "\n",
    "    # For multi-level column indexing:\n",
    "    if isinstance(df.columns.values[0], tuple):\n",
    "\n",
    "        # Define important varibles\n",
    "        columns = df.columns.values\n",
    "        new_cols = []\n",
    "\n",
    "        # Itarate through the multi-level column names and flatten them:\n",
    "        for col in columns:\n",
    "            temp = []\n",
    "            for name in col:\n",
    "                if name != '':\n",
    "                    temp.append(name)\n",
    "            new_temp = '_'.join(temp)\n",
    "            new_cols.append(new_temp)\n",
    "        \n",
    "        # Change the column names:\n",
    "        df.columns = new_cols\n",
    "        print(df.columns)\n",
    "    \n",
    "    # For uni-level colum indexing:\n",
    "    elif isinstance(df.columns.values[0], str):\n",
    "        \n",
    "        # Define new names:\n",
    "        col_names = [column+'_'+ticker for column in cols\\\n",
    "                     for ticker in tickers]\n",
    "        df.columns = col_names\n",
    "    \n",
    "    print(\"the dataframe after the columns have been renamed\") \n",
    "#     print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "df2=pd.DataFrame(one_lvl_colnames(data,cols=columns,tickers=tickers))\n",
    "df2_columns=list(df2.columns)\n",
    "df2.columns = [col.replace('Adj Close', 'Close') for col in df2_columns]\n",
    "df2.fillna(method='ffill',inplace=True)\n",
    "df2.fillna(method='bfill',inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abafca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_statistics(df,cols='Adj Close',tickers=None,functions=None,\n",
    "                      window=20,bollinger=False,roll_linewidth=1.5,**kwargs):\n",
    "        '''This method extracts the rolling statistics from a time series, and\n",
    "        can plot the rolling window with the data, adding the Bollinger bands.\n",
    "        \n",
    "        Inputs:\n",
    "        -------\n",
    "        column (string, default=None): the column from which you want compute \n",
    "            the rolling function.\n",
    "        tickers (str|list, default=None): the ticker(s) from which you want to know the \n",
    "            information.\n",
    "        functions (function|string|list): function(s) that will be rolled through the \n",
    "            time series.\n",
    "        window (int): the window of the rolling data\n",
    "            \n",
    "        OUTPUTS:\n",
    "            rolled (pandas series): a series with the rolling statistics specified\n",
    "\n",
    "        '''\n",
    "        # Define important varibles:\n",
    "#         df = self.df\n",
    "#         if isinstance(tickers,type(None)):\n",
    "#             tickers = self.get_tickers()\n",
    "        col_names = cols\n",
    "        if isinstance(functions,type(None)):\n",
    "            functions = [momentum, simple_moving_average, bollinger_bands]\n",
    "        elif not isinstance(functions,list):\n",
    "            functions = [functions]\n",
    "\n",
    "        # Define the actual dataframe analized\n",
    "        df = df[col_names]\n",
    "\n",
    "        # Compute the rolling statistics:\n",
    "        rolling_stats = df.rolling(window).agg(functions)\n",
    "        \n",
    "        return rolling_stats\n",
    "        \n",
    "#         print(rolling_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2180b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "BBWI\n",
      "                close    volume       open       high        low\n",
      "Date                                                            \n",
      "2007-01-03  10.077243   9288633  24.252222  24.276476  23.540825\n",
      "2007-01-04   9.317530  17476831  22.546482  22.756668  21.891672\n",
      "2007-01-05   9.310719   9970096  21.980598  22.433306  21.147940\n",
      "2007-01-08   9.130158   9207238  22.029102  22.465643  21.471302\n",
      "2007-01-09   9.266429  10808659  21.843168  22.465643  21.827002\n",
      "...               ...       ...        ...        ...        ...\n",
      "2022-12-23  41.287586   2461500  40.330002  41.950001  40.150002\n",
      "2022-12-27  41.297432   3022500  41.939999  42.889999  41.810001\n",
      "2022-12-28  40.116104   2138000  41.779999  42.169998  40.630001\n",
      "2022-12-29  41.002106   1610500  41.349998  42.000000  41.180000\n",
      "2022-12-30  41.484478   2237000  41.099998  42.209999  40.880001\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                close    volume       open       high        low        cci  \\\n",
      "Date                                                                          \n",
      "2007-01-03  10.077243   9288633  24.252222  24.276476  23.540825        NaN   \n",
      "2007-01-04   9.317530  17476831  22.546482  22.756668  21.891672 -66.666667   \n",
      "2007-01-05   9.310719   9970096  21.980598  22.433306  21.147940 -68.036202   \n",
      "2007-01-08   9.130158   9207238  22.029102  22.465643  21.471302 -53.794519   \n",
      "2007-01-09   9.266429  10808659  21.843168  22.465643  21.827002 -33.004998   \n",
      "...               ...       ...        ...        ...        ...        ...   \n",
      "2022-12-23  41.287586   2461500  40.330002  41.950001  40.150002 -26.952877   \n",
      "2022-12-27  41.297432   3022500  41.939999  42.889999  41.810001  46.300247   \n",
      "2022-12-28  40.116104   2138000  41.779999  42.169998  40.630001 -29.822199   \n",
      "2022-12-29  41.002106   1610500  41.349998  42.000000  41.180000  15.208497   \n",
      "2022-12-30  41.484478   2237000  41.099998  42.209999  40.880001  37.972571   \n",
      "\n",
      "              stochrsi       mfi        bop  supertrend_ub  supertrend_lb  \\\n",
      "Date                                                                        \n",
      "2007-01-03         NaN  0.500000 -19.268619      66.506350     -18.689049   \n",
      "2007-01-04         NaN  0.500000 -15.293658      62.557724     -17.909384   \n",
      "2007-01-05         NaN  0.500000  -9.857021      61.706626     -17.909384   \n",
      "2007-01-08         NaN  0.500000 -12.972356      61.706626     -17.821891   \n",
      "2007-01-09  100.000000  0.500000 -19.692961      61.706626     -17.693890   \n",
      "...                ...       ...        ...            ...            ...   \n",
      "2022-12-23   50.075647  0.271783   0.531992      46.013339      36.396209   \n",
      "2022-12-27   50.368039  0.338524  -0.594970      46.013339      36.489214   \n",
      "2022-12-28    6.012837  0.345384  -1.080453      46.013339      36.489214   \n",
      "2022-12-29   43.168578  0.393974  -0.424260      46.013339      36.489214   \n",
      "2022-12-30   62.418304  0.358997   0.289083      46.013339      36.489214   \n",
      "\n",
      "            supertrend    eribull    eribear       cti        qqe  qqel  \\\n",
      "Date                                                                      \n",
      "2007-01-03   66.506350  14.199233  13.463582  0.000000   0.000000   0.0   \n",
      "2007-01-04   62.557724  12.787956  11.922960  0.000000   0.000000   0.0   \n",
      "2007-01-05   61.706626  12.558592  11.273226  0.000000   0.000000   0.0   \n",
      "2007-01-08   61.706626  12.697295  11.702954  0.000000   0.000000   0.0   \n",
      "2007-01-09   61.706626  12.768997  12.130356  0.000000   0.000000   0.0   \n",
      "...                ...        ...        ...       ...        ...   ...   \n",
      "2022-12-23   36.396209   1.387834  -0.412165 -0.626140  58.759495   0.0   \n",
      "2022-12-27   36.489214   2.222795   1.142797 -0.425823  58.759495   0.0   \n",
      "2022-12-28   36.489214   1.581522   0.041525 -0.382264  58.759495   0.0   \n",
      "2022-12-29   36.489214   1.352434   0.532434 -0.170032  58.759495   0.0   \n",
      "2022-12-30   36.489214   1.442874   0.112876  0.214672  58.759495   0.0   \n",
      "\n",
      "                 qqes       ker  \n",
      "Date                             \n",
      "2007-01-03   0.000000  0.000000  \n",
      "2007-01-04   0.000000  1.000000  \n",
      "2007-01-05   0.000000  1.000000  \n",
      "2007-01-08   0.000000  1.000000  \n",
      "2007-01-09   0.000000  0.748429  \n",
      "...               ...       ...  \n",
      "2022-12-23  58.759495  0.061978  \n",
      "2022-12-27  58.759495  0.034366  \n",
      "2022-12-28  58.759495  0.243723  \n",
      "2022-12-29  58.759495  0.076102  \n",
      "2022-12-30  58.759495  0.051931  \n",
      "\n",
      "[4028 rows x 19 columns]\n",
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "BIIB\n",
      "                 close   volume        open        high         low\n",
      "Date                                                               \n",
      "2007-01-03   49.330002  3833500   49.279999   50.250000   48.200001\n",
      "2007-01-04   49.750000  2884300   49.270000   50.099998   48.880001\n",
      "2007-01-05   49.759998  2230400   49.990002   50.330002   49.509998\n",
      "2007-01-08   50.020000  3160200   50.000000   50.160000   48.910000\n",
      "2007-01-09   49.500000  2816500   49.880001   50.139999   49.299999\n",
      "...                ...      ...         ...         ...         ...\n",
      "2022-12-23  279.160004   624800  280.450012  280.450012  276.070007\n",
      "2022-12-27  274.769989   638600  279.890015  279.890015  273.380005\n",
      "2022-12-28  274.040009   521000  275.640015  276.920013  272.640015\n",
      "2022-12-29  276.000000   594000  274.829987  279.140015  274.299988\n",
      "2022-12-30  276.920013   640700  274.980011  277.149994  272.200012\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                 close   volume        open        high         low  \\\n",
      "Date                                                                  \n",
      "2007-01-03   49.330002  3833500   49.279999   50.250000   48.200001   \n",
      "2007-01-04   49.750000  2884300   49.270000   50.099998   48.880001   \n",
      "2007-01-05   49.759998  2230400   49.990002   50.330002   49.509998   \n",
      "2007-01-08   50.020000  3160200   50.000000   50.160000   48.910000   \n",
      "2007-01-09   49.500000  2816500   49.880001   50.139999   49.299999   \n",
      "...                ...      ...         ...         ...         ...   \n",
      "2022-12-23  279.160004   624800  280.450012  280.450012  276.070007   \n",
      "2022-12-27  274.769989   638600  279.890015  279.890015  273.380005   \n",
      "2022-12-28  274.040009   521000  275.640015  276.920013  272.640015   \n",
      "2022-12-29  276.000000   594000  274.829987  279.140015  274.299988   \n",
      "2022-12-30  276.920013   640700  274.980011  277.149994  272.200012   \n",
      "\n",
      "                   cci   stochrsi       mfi       bop  supertrend_ub  \\\n",
      "Date                                                                   \n",
      "2007-01-03         NaN        NaN  0.500000  0.024392      55.374998   \n",
      "2007-01-04   66.666667        NaN  0.500000  0.393443      54.348884   \n",
      "2007-01-05   97.111977        NaN  0.500000 -0.280491      53.919324   \n",
      "2007-01-08   35.474059        NaN  0.500000  0.016000      53.464903   \n",
      "2007-01-09   16.288392   0.000000  0.500000 -0.452382      53.324661   \n",
      "...                ...        ...       ...       ...            ...   \n",
      "2022-12-23 -168.219485   0.000000  0.234940 -0.294522     299.126919   \n",
      "2022-12-27 -168.306558   0.000000  0.241585 -0.786485     297.406427   \n",
      "2022-12-28 -156.632806   0.000000  0.251803 -0.373833     294.984902   \n",
      "2022-12-29 -104.636180  19.227311  0.314250  0.241737     294.984902   \n",
      "2022-12-30  -99.314755  28.296153  0.322954  0.391921     294.172072   \n",
      "\n",
      "            supertrend_lb  supertrend   eribull    eribear       cti  \\\n",
      "Date                                                                   \n",
      "2007-01-03      43.075003   55.374998  0.919998  -1.130001  0.000000   \n",
      "2007-01-04      44.631116   54.348884  0.709997  -0.510001  0.000000   \n",
      "2007-01-05      45.920676   53.919324  0.887144   0.067140  0.000000   \n",
      "2007-01-08      45.920676   53.464903  0.634693  -0.615307  0.000000   \n",
      "2007-01-09      46.115338   53.324661  0.618308  -0.221693  0.000000   \n",
      "...                   ...         ...       ...        ...       ...   \n",
      "2022-12-23     257.393101  299.126919 -5.956845 -10.336850 -0.699009   \n",
      "2022-12-27     257.393101  297.406427 -4.854433 -11.364443 -0.764786   \n",
      "2022-12-28     257.393101  294.984902 -6.295229 -10.575227 -0.889266   \n",
      "2022-12-29     257.393101  294.984902 -3.044479  -7.884505 -0.888977   \n",
      "2022-12-30     257.393101  294.172072 -4.282431  -9.232412 -0.853851   \n",
      "\n",
      "                  qqe  qqel       qqes       ker  \n",
      "Date                                              \n",
      "2007-01-03   0.000000   0.0   0.000000  0.000000  \n",
      "2007-01-04   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-05   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-08   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-09   0.000000   0.0   0.000000  0.140494  \n",
      "...               ...   ...        ...       ...  \n",
      "2022-12-23  49.138925   0.0  49.138925  0.227555  \n",
      "2022-12-27  46.756338   0.0  46.756338  0.597883  \n",
      "2022-12-28  44.980886   0.0  44.980886  0.648077  \n",
      "2022-12-29  44.751224   0.0  44.751224  0.496009  \n",
      "2022-12-30  44.751224   0.0  44.751224  0.301572  \n",
      "\n",
      "[4028 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "CBRE\n",
      "                close   volume       open       high        low\n",
      "Date                                                           \n",
      "2007-01-03  33.650002  2079600  33.400002  33.830002  33.139999\n",
      "2007-01-04  33.340000  1354900  33.810001  34.000000  33.099998\n",
      "2007-01-05  33.099998  1374100  33.340000  33.599998  32.799999\n",
      "2007-01-08  33.220001   960400  33.410000  33.410000  32.500000\n",
      "2007-01-09  33.970001  1457300  33.380001  34.200001  33.060001\n",
      "...               ...      ...        ...        ...        ...\n",
      "2022-12-23  76.669998   675000  75.279999  76.669998  75.150002\n",
      "2022-12-27  76.489998   599500  76.820000  77.120003  76.099998\n",
      "2022-12-28  75.410004   746600  76.599998  76.910004  75.379997\n",
      "2022-12-29  77.550003   884900  75.870003  77.669998  75.769997\n",
      "2022-12-30  76.959999  1175600  76.720001  77.320000  76.089996\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                close   volume       open       high        low         cci  \\\n",
      "Date                                                                          \n",
      "2007-01-03  33.650002  2079600  33.400002  33.830002  33.139999         NaN   \n",
      "2007-01-04  33.340000  1354900  33.810001  34.000000  33.099998  -66.666667   \n",
      "2007-01-05  33.099998  1374100  33.340000  33.599998  32.799999 -100.000000   \n",
      "2007-01-08  33.220001   960400  33.410000  33.410000  32.500000  -86.968148   \n",
      "2007-01-09  33.970001  1457300  33.380001  34.200001  33.060001  100.306946   \n",
      "...               ...      ...        ...        ...        ...         ...   \n",
      "2022-12-23  76.669998   675000  75.279999  76.669998  75.150002  -34.504067   \n",
      "2022-12-27  76.489998   599500  76.820000  77.120003  76.099998   -6.013576   \n",
      "2022-12-28  75.410004   746600  76.599998  76.910004  75.379997  -46.738794   \n",
      "2022-12-29  77.550003   884900  75.870003  77.669998  75.769997   22.360229   \n",
      "2022-12-30  76.959999  1175600  76.720001  77.320000  76.089996    5.983182   \n",
      "\n",
      "              stochrsi       mfi       bop  supertrend_ub  supertrend_lb  \\\n",
      "Date                                                                       \n",
      "2007-01-03         NaN  0.500000  0.362318      35.555008      31.414993   \n",
      "2007-01-04         NaN  0.500000 -0.522223      35.555008      31.414993   \n",
      "2007-01-05         NaN  0.500000 -0.300002      35.555008      31.414993   \n",
      "2007-01-08  100.000000  0.500000 -0.208790      35.445343      31.414993   \n",
      "2007-01-09  100.000000  0.500000  0.517544      35.445343      31.414993   \n",
      "...                ...       ...       ...            ...            ...   \n",
      "2022-12-23   35.829868  0.280723  0.914475      81.295059      73.217118   \n",
      "2022-12-27   32.089520  0.315817 -0.323530      81.295059      73.217118   \n",
      "2022-12-28    9.913344  0.323377 -0.777771      81.295059      73.217118   \n",
      "2022-12-29   51.982018  0.372501  0.884210      81.295059      73.217118   \n",
      "2022-12-30   39.498040  0.370532  0.195120      81.295059      73.217118   \n",
      "\n",
      "            supertrend   eribull   eribear       cti        qqe  qqel  \\\n",
      "Date                                                                    \n",
      "2007-01-03   35.555008  0.180000 -0.510002  0.000000   0.000000   0.0   \n",
      "2007-01-04   35.555008  0.394284 -0.505717  0.000000   0.000000   0.0   \n",
      "2007-01-05   35.555008  0.066528 -0.733471  0.000000   0.000000   0.0   \n",
      "2007-01-08   35.445343 -0.078689 -0.988689  0.000000   0.000000   0.0   \n",
      "2007-01-09   35.445343  0.642553 -0.497447  0.000000   0.000000   0.0   \n",
      "...                ...       ...       ...       ...        ...   ...   \n",
      "2022-12-23   73.217118  0.179983 -1.340014 -0.276182  55.194492   0.0   \n",
      "2022-12-27   73.217118  0.629990 -0.390015 -0.347800  55.194492   0.0   \n",
      "2022-12-28   73.217118  0.574278 -0.955729 -0.540510  55.194492   0.0   \n",
      "2022-12-29   73.217118  1.160804 -0.739197 -0.470721  55.194492   0.0   \n",
      "2022-12-30   73.217118  0.746405 -0.483598 -0.166159  55.194492   0.0   \n",
      "\n",
      "                 qqes       ker  \n",
      "Date                             \n",
      "2007-01-03   0.000000  0.000000  \n",
      "2007-01-04   0.000000  1.000000  \n",
      "2007-01-05   0.000000  1.000000  \n",
      "2007-01-08   0.000000  0.641786  \n",
      "2007-01-09   0.000000  0.225351  \n",
      "...               ...       ...  \n",
      "2022-12-23  55.194492  0.065406  \n",
      "2022-12-27  55.194492  0.000819  \n",
      "2022-12-28  55.194492  0.448137  \n",
      "2022-12-29  55.194492  0.160069  \n",
      "2022-12-30  55.194492  0.046875  \n",
      "\n",
      "[4028 rows x 19 columns]\n",
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "DIS\n",
      "                close    volume       open       high        low\n",
      "Date                                                            \n",
      "2007-01-03  28.317097  13562595  33.748165  34.073711  33.531136\n",
      "2007-01-04  28.540653   9806285  33.738300  34.083576  33.708706\n",
      "2007-01-05  28.308819  10551445  33.807354  33.975060  33.531136\n",
      "2007-01-08  28.565489   9479676  33.728436  34.162495  33.610054\n",
      "2007-01-09  28.524092  11588444  34.034248  34.221684  33.491676\n",
      "...               ...       ...        ...        ...        ...\n",
      "2022-12-23  88.010002  11171600  86.059998  88.070000  85.769997\n",
      "2022-12-27  86.370003  11561400  87.419998  87.940002  85.959999\n",
      "2022-12-28  84.169998  12399500  86.080002  86.690002  84.070000\n",
      "2022-12-29  87.180000  13045100  85.250000  88.239998  84.970001\n",
      "2022-12-30  86.879997  23231000  85.730003  87.120003  85.230003\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                close    volume       open       high        low         cci  \\\n",
      "Date                                                                           \n",
      "2007-01-03  28.317097  13562595  33.748165  34.073711  33.531136         NaN   \n",
      "2007-01-04  28.540653   9806285  33.738300  34.083576  33.708706   66.666667   \n",
      "2007-01-05  28.308819  10551445  33.807354  33.975060  33.531136  -67.266918   \n",
      "2007-01-08  28.565489   9479676  33.728436  34.162495  33.610054   67.395014   \n",
      "2007-01-09  28.524092  11588444  34.034248  34.221684  33.491676   34.655974   \n",
      "...               ...       ...        ...        ...        ...         ...   \n",
      "2022-12-23  88.010002  11171600  86.059998  88.070000  85.769997  -79.811941   \n",
      "2022-12-27  86.370003  11561400  87.419998  87.940002  85.959999  -78.497097   \n",
      "2022-12-28  84.169998  12399500  86.080002  86.690002  84.070000 -100.034965   \n",
      "2022-12-29  87.180000  13045100  85.250000  88.239998  84.970001  -54.054532   \n",
      "2022-12-30  86.879997  23231000  85.730003  87.120003  85.230003  -55.642434   \n",
      "\n",
      "             stochrsi       mfi        bop  supertrend_ub  supertrend_lb  \\\n",
      "Date                                                                       \n",
      "2007-01-03        NaN  0.500000 -10.009787      51.072268      16.532579   \n",
      "2007-01-04        NaN  0.500000 -13.865188      51.072268      16.610952   \n",
      "2007-01-05   0.000000  0.500000 -12.386210      50.686429      16.819766   \n",
      "2007-01-08  38.614106  0.500000  -9.345704      50.686429      16.819766   \n",
      "2007-01-09  30.559012  0.500000  -7.548086      50.686429      16.819766   \n",
      "...               ...       ...        ...            ...            ...   \n",
      "2022-12-23  44.867435  0.392203   0.847827      94.827871      77.985373   \n",
      "2022-12-27  25.496638  0.393707  -0.530300      94.827871      78.191225   \n",
      "2022-12-28   2.058954  0.391571  -0.729008      94.074579      78.191225   \n",
      "2022-12-29  58.554808  0.400212   0.590215      94.074579      78.191225   \n",
      "2022-12-30  54.916148  0.331202   0.608463      94.074579      78.191225   \n",
      "\n",
      "            supertrend   eribull   eribear       cti        qqe  qqel  \\\n",
      "Date                                                                    \n",
      "2007-01-03   51.072268  5.756615  5.214039  0.000000   0.000000   0.0   \n",
      "2007-01-04   51.072268  5.734543  5.359673  0.000000   0.000000   0.0   \n",
      "2007-01-05   50.686429  5.631771  5.187847  0.000000   0.000000   0.0   \n",
      "2007-01-08   50.686429  5.787463  5.235023  0.000000   0.000000   0.0   \n",
      "2007-01-09   50.686429  5.825358  5.095351  0.000000   0.000000   0.0   \n",
      "...                ...       ...       ...       ...        ...   ...   \n",
      "2022-12-23   94.827871 -1.980303 -4.280306 -0.841567  42.758088   0.0   \n",
      "2022-12-27   94.827871 -1.584543 -3.564547 -0.872248  42.758088   0.0   \n",
      "2022-12-28   94.074579 -2.069608 -4.689611 -0.891082  41.656477   0.0   \n",
      "2022-12-29   94.074579 -0.293954 -3.563951 -0.806779  41.656477   0.0   \n",
      "2022-12-30   94.074579 -1.177670 -3.067669 -0.697686  41.656477   0.0   \n",
      "\n",
      "                 qqes       ker  \n",
      "Date                             \n",
      "2007-01-03   0.000000  0.000000  \n",
      "2007-01-04   0.000000  1.000000  \n",
      "2007-01-05   0.000000  0.018178  \n",
      "2007-01-08   0.000000  0.348835  \n",
      "2007-01-09   0.000000  0.274727  \n",
      "...               ...       ...  \n",
      "2022-12-23  42.758088  0.407745  \n",
      "2022-12-27  42.758088  0.612713  \n",
      "2022-12-28  41.656477  0.671128  \n",
      "2022-12-29  41.656477  0.384022  \n",
      "2022-12-30  41.656477  0.244084  \n",
      "\n",
      "[4028 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "ETR\n",
      "                 close   volume        open        high         low\n",
      "Date                                                               \n",
      "2007-01-03   46.684509  1319900   92.320000   93.419998   92.089996\n",
      "2007-01-04   47.236816  2610100   92.989998   94.160004   92.599998\n",
      "2007-01-05   46.071968  2269500   93.599998   93.690002   91.279999\n",
      "2007-01-08   45.725525  1686800   91.559998   92.459999   90.870003\n",
      "2007-01-09   45.695412  1029700   91.169998   91.620003   90.540001\n",
      "...                ...      ...         ...         ...         ...\n",
      "2022-12-23  109.924477  1166400  112.550003  113.629997  112.339996\n",
      "2022-12-27  111.164909   968500  113.639999  115.059998  113.190002\n",
      "2022-12-28  109.478691  1011700  115.160004  115.199997  112.930000\n",
      "2022-12-29  110.564072   834100  113.599998  115.059998  112.500000\n",
      "2022-12-30  109.023216   906400  114.139999  114.290001  111.489998\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                 close   volume        open        high         low  \\\n",
      "Date                                                                  \n",
      "2007-01-03   46.684509  1319900   92.320000   93.419998   92.089996   \n",
      "2007-01-04   47.236816  2610100   92.989998   94.160004   92.599998   \n",
      "2007-01-05   46.071968  2269500   93.599998   93.690002   91.279999   \n",
      "2007-01-08   45.725525  1686800   91.559998   92.459999   90.870003   \n",
      "2007-01-09   45.695412  1029700   91.169998   91.620003   90.540001   \n",
      "...                ...      ...         ...         ...         ...   \n",
      "2022-12-23  109.924477  1166400  112.550003  113.629997  112.339996   \n",
      "2022-12-27  111.164909   968500  113.639999  115.059998  113.190002   \n",
      "2022-12-28  109.478691  1011700  115.160004  115.199997  112.930000   \n",
      "2022-12-29  110.564072   834100  113.599998  115.059998  112.500000   \n",
      "2022-12-30  109.023216   906400  114.139999  114.290001  111.489998   \n",
      "\n",
      "                   cci   stochrsi       mfi        bop  supertrend_ub  \\\n",
      "Date                                                                    \n",
      "2007-01-03         NaN        NaN  0.500000 -34.312352     232.961464   \n",
      "2007-01-04   66.666667        NaN  0.500000 -29.328865     232.961464   \n",
      "2007-01-05  -86.341016   0.000000  0.500000 -19.721144     232.961464   \n",
      "2007-01-08 -110.130165   0.000000  0.500000 -28.826779     231.895249   \n",
      "2007-01-09 -104.402329   0.000000  0.500000 -42.106027     230.722741   \n",
      "...                ...        ...       ...        ...            ...   \n",
      "2022-12-23  -84.585253  11.750740  0.506408  -2.035290     125.766472   \n",
      "2022-12-27  -38.738475  30.868226  0.509237  -1.323581     125.766472   \n",
      "2022-12-28  -51.656698   4.944260  0.503691  -2.502785     125.766472   \n",
      "2022-12-29  -40.543045  21.524299  0.500551  -1.185910     125.766472   \n",
      "2022-12-30  -71.521677   0.000000  0.456976  -1.827421     125.766472   \n",
      "\n",
      "            supertrend_lb  supertrend    eribull    eribear       cti  \\\n",
      "Date                                                                    \n",
      "2007-01-03     -47.451469  232.961464  46.735489  45.405487  0.000000   \n",
      "2007-01-04     -47.451469  232.961464  47.396593  45.836588  0.000000   \n",
      "2007-01-05     -47.451469  232.961464  47.025370  44.615366  0.000000   \n",
      "2007-01-08     -47.451469  231.895249  45.929525  44.339528  0.000000   \n",
      "2007-01-09     -47.451469  230.722741  45.208823  44.128821  0.000000   \n",
      "...                   ...         ...        ...        ...       ...   \n",
      "2022-12-23     104.140373  104.140373   2.625544   1.335543 -0.832168   \n",
      "2022-12-27     104.140373  104.140373   4.032622   2.162627 -0.791100   \n",
      "2022-12-28     104.140373  104.140373   4.393862   2.123865 -0.811727   \n",
      "2022-12-29     104.140373  104.140373   4.288443   1.728445 -0.704930   \n",
      "2022-12-30     104.140373  104.140373   3.768209   0.968206 -0.644103   \n",
      "\n",
      "                  qqe  qqel       qqes       ker  \n",
      "Date                                              \n",
      "2007-01-03   0.000000   0.0   0.000000  0.000000  \n",
      "2007-01-04   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-05   0.000000   0.0   0.000000  0.356719  \n",
      "2007-01-08   0.000000   0.0   0.000000  0.464715  \n",
      "2007-01-09   0.000000   0.0   0.000000  0.472413  \n",
      "...               ...   ...        ...       ...  \n",
      "2022-12-23  55.172406   0.0  55.172406  0.319702  \n",
      "2022-12-27  55.172406   0.0  55.172406  0.351094  \n",
      "2022-12-28  55.172406   0.0  55.172406  0.440984  \n",
      "2022-12-29  55.172406   0.0  55.172406  0.328799  \n",
      "2022-12-30  54.331727   0.0  54.331727  0.325741  \n",
      "\n",
      "[4028 rows x 19 columns]\n",
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "GILD\n",
      "                close    volume       open       high        low\n",
      "Date                                                            \n",
      "2007-01-03  11.958693  33461200  16.320000  16.535000  15.480000\n",
      "2007-01-04  12.155637  18666400  15.780000  16.157499  15.617500\n",
      "2007-01-05  12.159424  14022000  16.055000  16.205000  15.962500\n",
      "2007-01-08  12.163209  12984000  15.995000  16.084999  15.877500\n",
      "2007-01-09  12.310921  20593600  16.237499  16.377501  16.129999\n",
      "...               ...       ...        ...        ...        ...\n",
      "2022-12-23  83.147980   3955400  85.220001  85.250000  84.279999\n",
      "2022-12-27  83.403076   3455900  85.250000  85.470001  84.690002\n",
      "2022-12-28  82.971390   3285900  85.400002  85.830002  84.449997\n",
      "2022-12-29  83.648346   3464100  84.910004  85.570000  84.610001\n",
      "2022-12-30  84.227188   3831000  85.330002  85.889999  84.779999\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                close    volume       open       high        low         cci  \\\n",
      "Date                                                                           \n",
      "2007-01-03  11.958693  33461200  16.320000  16.535000  15.480000         NaN   \n",
      "2007-01-04  12.155637  18666400  15.780000  16.157499  15.617500  -66.666667   \n",
      "2007-01-05  12.159424  14022000  16.055000  16.205000  15.962500  100.000000   \n",
      "2007-01-08  12.163209  12984000  15.995000  16.084999  15.877500   17.736088   \n",
      "2007-01-09  12.310921  20593600  16.237499  16.377501  16.129999  143.994736   \n",
      "...               ...       ...        ...        ...        ...         ...   \n",
      "2022-12-23  83.147980   3955400  85.220001  85.250000  84.279999 -103.916739   \n",
      "2022-12-27  83.403076   3455900  85.250000  85.470001  84.690002  -76.826057   \n",
      "2022-12-28  82.971390   3285900  85.400002  85.830002  84.449997  -70.634146   \n",
      "2022-12-29  83.648346   3464100  84.910004  85.570000  84.610001  -53.409599   \n",
      "2022-12-30  84.227188   3831000  85.330002  85.889999  84.779999  -28.143084   \n",
      "\n",
      "             stochrsi       mfi        bop  supertrend_ub  supertrend_lb  \\\n",
      "Date                                                                       \n",
      "2007-01-03        NaN  0.500000  -4.133939      29.736422       2.278578   \n",
      "2007-01-04        NaN  0.500000  -6.711796      29.029199       2.745801   \n",
      "2007-01-05        NaN  0.500000 -16.064213      28.869420       3.298079   \n",
      "2007-01-08        NaN  0.500000 -18.466506      28.485992       3.476507   \n",
      "2007-01-09        NaN  0.500000 -15.864876      28.485992       3.717142   \n",
      "...               ...       ...        ...            ...            ...   \n",
      "2022-12-23   0.000000  0.166563  -2.136102      91.021392      78.875683   \n",
      "2022-12-27   4.775389  0.199841  -2.367855      91.021392      78.875683   \n",
      "2022-12-28   0.000000  0.209959  -1.759857      91.021392      78.875683   \n",
      "2022-12-29  16.471254  0.187511  -1.314228      91.021392      78.875683   \n",
      "2022-12-30  29.668133  0.231504  -0.993525      91.021392      78.875683   \n",
      "\n",
      "            supertrend   eribull   eribear       cti        qqe  qqel  \\\n",
      "Date                                                                    \n",
      "2007-01-03   29.736422  4.576307  3.521307  0.000000   0.000000   0.0   \n",
      "2007-01-04   29.029199  4.170672  3.630673  0.000000   0.000000   0.0   \n",
      "2007-01-05   28.869420  4.193516  3.951016  0.000000   0.000000   0.0   \n",
      "2007-01-08   28.485992  4.051840  3.844341  0.000000   0.000000   0.0   \n",
      "2007-01-09   28.485992  4.304661  4.057160  0.000000   0.000000   0.0   \n",
      "...                ...       ...       ...       ...        ...   ...   \n",
      "2022-12-23   78.875683  0.895287 -0.074714 -0.903681  59.284355   0.0   \n",
      "2022-12-27   78.875683  1.251237  0.471238 -0.882358  58.589169   0.0   \n",
      "2022-12-28   78.875683  1.789434  0.409429 -0.917740  57.352054   0.0   \n",
      "2022-12-29   78.875683  1.585463  0.625464 -0.861716  57.352054   0.0   \n",
      "2022-12-30   78.875683  1.870798  0.760798 -0.702355  57.352054   0.0   \n",
      "\n",
      "                 qqes       ker  \n",
      "Date                             \n",
      "2007-01-03   0.000000  0.000000  \n",
      "2007-01-04   0.000000  1.000000  \n",
      "2007-01-05   0.000000  1.000000  \n",
      "2007-01-08   0.000000  1.000000  \n",
      "2007-01-09   0.000000  1.000000  \n",
      "...               ...       ...  \n",
      "2022-12-23  59.284355  0.403718  \n",
      "2022-12-27  58.589169  0.476623  \n",
      "2022-12-28  57.352054  0.564644  \n",
      "2022-12-29  57.352054  0.489095  \n",
      "2022-12-30  57.352054  0.140656  \n",
      "\n",
      "[4028 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "NI\n",
      "                close   volume       open       high        low\n",
      "Date                                                           \n",
      "2007-01-03   4.979852  3227315   9.469548   9.622790   9.469548\n",
      "2007-01-04   4.961384  2680140   9.536346   9.591356   9.469548\n",
      "2007-01-05   4.836224  4132062   9.481336   9.481336   9.229862\n",
      "2007-01-08   4.828014  3449748   9.261297   9.343811   9.182711\n",
      "2007-01-09   4.842379  4279927   9.253438   9.300589   9.210216\n",
      "...               ...      ...        ...        ...        ...\n",
      "2022-12-23  26.970551  1588400  27.330000  27.709999  27.330000\n",
      "2022-12-27  27.058151  1994900  27.740000  27.879999  27.549999\n",
      "2022-12-28  26.814819  1944100  27.780001  28.020000  27.520000\n",
      "2022-12-29  27.097084  1431600  27.639999  27.920000  27.639999\n",
      "2022-12-30  26.688292  2526900  27.820000  27.870001  27.230000\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                close   volume       open       high        low         cci  \\\n",
      "Date                                                                          \n",
      "2007-01-03   4.979852  3227315   9.469548   9.622790   9.469548         NaN   \n",
      "2007-01-04   4.961384  2680140   9.536346   9.591356   9.469548  -66.666667   \n",
      "2007-01-05   4.836224  4132062   9.481336   9.481336   9.229862 -100.000000   \n",
      "2007-01-08   4.828014  3449748   9.261297   9.343811   9.182711  -88.232876   \n",
      "2007-01-09   4.842379  4279927   9.253438   9.300589   9.210216  -69.954923   \n",
      "...               ...      ...        ...        ...        ...         ...   \n",
      "2022-12-23  26.970551  1588400  27.330000  27.709999  27.330000   12.777623   \n",
      "2022-12-27  27.058151  1994900  27.740000  27.879999  27.549999   52.455274   \n",
      "2022-12-28  26.814819  1944100  27.780001  28.020000  27.520000   40.843992   \n",
      "2022-12-29  27.097084  1431600  27.639999  27.920000  27.639999   63.295786   \n",
      "2022-12-30  26.688292  2526900  27.820000  27.870001  27.230000   -8.882861   \n",
      "\n",
      "              stochrsi       mfi        bop  supertrend_ub  supertrend_lb  \\\n",
      "Date                                                                        \n",
      "2007-01-03         NaN  0.500000 -29.298056      23.474984      -4.382645   \n",
      "2007-01-04         NaN  0.500000 -37.558783      23.410369      -4.349465   \n",
      "2007-01-05         NaN  0.500000 -18.471580      23.120832      -4.349465   \n",
      "2007-01-08         NaN  0.500000 -27.518763      22.960980      -4.349465   \n",
      "2007-01-09  100.000000  0.500000 -48.809461      22.888531      -4.349465   \n",
      "...                ...       ...        ...            ...            ...   \n",
      "2022-12-23   52.995216  0.447503  -0.945922      28.274639      24.894747   \n",
      "2022-12-27   63.484781  0.434036  -2.066208      28.274639      24.894747   \n",
      "2022-12-28   35.074915  0.467603  -1.930363      28.274639      24.894747   \n",
      "2022-12-29   60.618175  0.430891  -1.938979      28.274639      24.894747   \n",
      "2022-12-30   15.455362  0.447247  -1.768290      28.274639      24.894747   \n",
      "\n",
      "            supertrend   eribull   eribear       cti        qqe  qqel  \\\n",
      "Date                                                                    \n",
      "2007-01-03   23.474984  4.642938  4.489696  0.000000   0.000000   0.0   \n",
      "2007-01-04   23.410369  4.614142  4.492334  0.000000   0.000000   0.0   \n",
      "2007-01-05   23.120832  4.524263  4.272790  0.000000   0.000000   0.0   \n",
      "2007-01-08   22.960980  4.405175  4.244075  0.000000   0.000000   0.0   \n",
      "2007-01-09   22.888531  4.375704  4.285331  0.000000   0.000000   0.0   \n",
      "...                ...       ...       ...       ...        ...   ...   \n",
      "2022-12-23   28.274639  0.997882  0.617883 -0.412513  57.736577   0.0   \n",
      "2022-12-27   28.274639  1.118449  0.788449 -0.186689  57.736577   0.0   \n",
      "2022-12-28   28.274639  1.250840  0.750840 -0.255726  57.736577   0.0   \n",
      "2022-12-29   28.274639  1.103994  0.823993  0.135932  57.736577   0.0   \n",
      "2022-12-30   28.274639  1.072239  0.432238  0.328217  57.736577   0.0   \n",
      "\n",
      "                 qqes       ker  \n",
      "Date                             \n",
      "2007-01-03   0.000000  0.000000  \n",
      "2007-01-04   0.000000  1.000000  \n",
      "2007-01-05   0.000000  1.000000  \n",
      "2007-01-08   0.000000  1.000000  \n",
      "2007-01-09   0.000000  0.827143  \n",
      "...               ...       ...  \n",
      "2022-12-23  57.736577  0.121212  \n",
      "2022-12-27  57.736577  0.137255  \n",
      "2022-12-28  57.736577  0.177572  \n",
      "2022-12-29  57.736577  0.008621  \n",
      "2022-12-30  57.736577  0.008475  \n",
      "\n",
      "[4028 rows x 19 columns]\n",
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "NVDA\n",
      "                 close     volume        open        high         low\n",
      "Date                                                                 \n",
      "2007-01-03    5.516786  115482000    6.178333    6.253333    5.798333\n",
      "2007-01-04    5.490793   79729800    5.991667    6.013333    5.838333\n",
      "2007-01-05    5.146760  124334400    5.843333    5.866667    5.570000\n",
      "2007-01-08    5.184986   65727000    5.630000    5.760000    5.533333\n",
      "2007-01-09    5.084069   76416600    5.660000    5.698333    5.535000\n",
      "...                ...        ...         ...         ...         ...\n",
      "2022-12-23  152.005920   34932600  151.960007  153.389999  148.830002\n",
      "2022-12-27  141.159790   46490200  150.740005  151.000000  140.559998\n",
      "2022-12-28  140.310074   35106600  139.270004  142.619995  138.839996\n",
      "2022-12-29  145.978073   35492300  144.020004  146.830002  142.270004\n",
      "2022-12-30  146.088028   31049000  143.339996  146.289993  142.330002\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                 close     volume        open        high         low  \\\n",
      "Date                                                                    \n",
      "2007-01-03    5.516786  115482000    6.178333    6.253333    5.798333   \n",
      "2007-01-04    5.490793   79729800    5.991667    6.013333    5.838333   \n",
      "2007-01-05    5.146760  124334400    5.843333    5.866667    5.570000   \n",
      "2007-01-08    5.184986   65727000    5.630000    5.760000    5.533333   \n",
      "2007-01-09    5.084069   76416600    5.660000    5.698333    5.535000   \n",
      "...                ...        ...         ...         ...         ...   \n",
      "2022-12-23  152.005920   34932600  151.960007  153.389999  148.830002   \n",
      "2022-12-27  141.159790   46490200  150.740005  151.000000  140.559998   \n",
      "2022-12-28  140.310074   35106600  139.270004  142.619995  138.839996   \n",
      "2022-12-29  145.978073   35492300  144.020004  146.830002  142.270004   \n",
      "2022-12-30  146.088028   31049000  143.339996  146.289993  142.330002   \n",
      "\n",
      "                   cci    stochrsi       mfi       bop  supertrend_ub  \\\n",
      "Date                                                                    \n",
      "2007-01-03         NaN         NaN  0.500000 -1.453949       8.235474   \n",
      "2007-01-04  -66.666667         NaN  0.500000 -2.862142       7.762140   \n",
      "2007-01-05 -100.000000         NaN  0.500000 -2.347998       7.300706   \n",
      "2007-01-08  -74.245423  100.000000  0.500000 -1.963291       7.300694   \n",
      "2007-01-09  -75.406018   77.764912  0.500000 -3.526117       7.244399   \n",
      "...                ...         ...       ...       ...            ...   \n",
      "2022-12-23 -145.777958    0.000000  0.399487  0.010069     173.975078   \n",
      "2022-12-27 -173.658551    0.000000  0.395637 -0.917645     169.464554   \n",
      "2022-12-28 -162.467849    0.000000  0.399486  0.275151     163.532797   \n",
      "2022-12-29 -105.498267   17.097363  0.378743  0.429401     163.532797   \n",
      "2022-12-30  -88.937602   17.423955  0.310450  0.693949     163.532797   \n",
      "\n",
      "            supertrend_lb  supertrend    eribull    eribear       cti  \\\n",
      "Date                                                                    \n",
      "2007-01-03       3.816192    8.235474   0.736547   0.281547  0.000000   \n",
      "2007-01-04       4.089526    7.762140   0.500260   0.325260  0.000000   \n",
      "2007-01-05       4.135961    7.300706   0.405924   0.109258  0.000000   \n",
      "2007-01-08       4.135961    7.300694   0.338652   0.111984  0.000000   \n",
      "2007-01-09       4.135961    7.244399   0.325167   0.161834  0.000000   \n",
      "...                   ...         ...        ...        ...       ...   \n",
      "2022-12-23     128.244923  173.975078  -9.399085 -13.959082 -0.831193   \n",
      "2022-12-27     128.244923  169.464554  -8.699185 -19.139187 -0.892057   \n",
      "2022-12-28     128.244923  163.532797 -14.309317 -18.089315 -0.956924   \n",
      "2022-12-29     128.244923  163.532797  -8.534847 -13.094845 -0.957281   \n",
      "2022-12-30     128.244923  163.532797  -7.749596 -11.709587 -0.929410   \n",
      "\n",
      "                  qqe  qqel       qqes       ker  \n",
      "Date                                              \n",
      "2007-01-03   0.000000   0.0   0.000000  0.000000  \n",
      "2007-01-04   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-05   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-08   0.000000   0.0   0.000000  0.812733  \n",
      "2007-01-09   0.000000   0.0   0.000000  0.849849  \n",
      "...               ...   ...        ...       ...  \n",
      "2022-12-23  53.941030   0.0  53.941030  0.376389  \n",
      "2022-12-27  50.243180   0.0  50.243180  0.641730  \n",
      "2022-12-28  47.667333   0.0  47.667333  0.829088  \n",
      "2022-12-29  47.667333   0.0  47.667333  0.609688  \n",
      "2022-12-30  47.667333   0.0  47.667333  0.540453  \n",
      "\n",
      "[4028 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "PEP\n",
      "                 close   volume        open        high         low\n",
      "Date                                                               \n",
      "2007-01-03   38.873486  6161600   62.700001   63.349998   62.450001\n",
      "2007-01-04   39.140011  5414300   62.700001   63.250000   62.500000\n",
      "2007-01-05   39.016048  4542400   62.700001   63.220001   62.700001\n",
      "2007-01-08   39.102818  6122600   63.000000   63.270000   62.860001\n",
      "2007-01-09   39.263954  4916500   63.049999   63.470001   63.009998\n",
      "...                ...      ...         ...         ...         ...\n",
      "2022-12-23  178.524826  2197800  180.910004  182.550003  180.449997\n",
      "2022-12-27  179.318222  3045000  183.279999  183.610001  182.270004\n",
      "2022-12-28  178.025269  2694300  184.100006  184.539993  181.639999\n",
      "2022-12-29  178.250565  2549200  181.919998  182.860001  181.889999\n",
      "2022-12-30  176.957626  3136200  181.380005  181.960007  179.289993\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                 close   volume        open        high         low  \\\n",
      "Date                                                                  \n",
      "2007-01-03   38.873486  6161600   62.700001   63.349998   62.450001   \n",
      "2007-01-04   39.140011  5414300   62.700001   63.250000   62.500000   \n",
      "2007-01-05   39.016048  4542400   62.700001   63.220001   62.700001   \n",
      "2007-01-08   39.102818  6122600   63.000000   63.270000   62.860001   \n",
      "2007-01-09   39.263954  4916500   63.049999   63.470001   63.009998   \n",
      "...                ...      ...         ...         ...         ...   \n",
      "2022-12-23  178.524826  2197800  180.910004  182.550003  180.449997   \n",
      "2022-12-27  179.318222  3045000  183.279999  183.610001  182.270004   \n",
      "2022-12-28  178.025269  2694300  184.100006  184.539993  181.639999   \n",
      "2022-12-29  178.250565  2549200  181.919998  182.860001  181.889999   \n",
      "2022-12-30  176.957626  3136200  181.380005  181.960007  179.289993   \n",
      "\n",
      "                   cci   stochrsi       mfi        bop  supertrend_ub  \\\n",
      "Date                                                                    \n",
      "2007-01-03         NaN        NaN  0.500000 -26.473973     136.329538   \n",
      "2007-01-04   66.666667        NaN  0.500000 -31.413320     136.148986   \n",
      "2007-01-05   64.414648   0.000000  0.500000 -45.546022     135.863480   \n",
      "2007-01-08  132.029785  20.099999  0.500000 -58.285833     135.863480   \n",
      "2007-01-09  137.508042  43.009377  0.500000 -51.708468     135.863480   \n",
      "...                ...        ...       ...        ...            ...   \n",
      "2022-12-23  -27.462774  51.423440  0.387887  -1.135796     196.524020   \n",
      "2022-12-27   42.298540  72.203582  0.446895  -2.956558     196.524020   \n",
      "2022-12-28   19.545365  32.881388  0.456875  -2.094741     196.524020   \n",
      "2022-12-29   -1.132379  39.471179  0.408744  -3.782916     196.524020   \n",
      "2022-12-30  -85.124104   0.961501  0.362264  -1.656313     196.340887   \n",
      "\n",
      "            supertrend_lb  supertrend    eribull    eribear       cti  \\\n",
      "Date                                                                    \n",
      "2007-01-03     -10.529539  136.329538  24.476513  23.576515  0.000000   \n",
      "2007-01-04     -10.398986  136.148986  24.338439  23.588439  0.000000   \n",
      "2007-01-05      -9.943478  135.863480  24.293514  23.773513  0.000000   \n",
      "2007-01-08      -9.799046  135.863480  24.318323  23.908323  0.000000   \n",
      "2007-01-09      -9.678835  135.863480  24.473713  24.013710  0.000000   \n",
      "...                   ...         ...        ...        ...       ...   \n",
      "2022-12-23     167.693079  167.693079   4.268299   2.168293 -0.589189   \n",
      "2022-12-27     167.693079  167.693079   5.180223   3.840226 -0.339998   \n",
      "2022-12-28     167.693079  167.693079   6.168002   3.268008 -0.280710   \n",
      "2022-12-29     167.693079  167.693079   4.505356   3.535355 -0.059576   \n",
      "2022-12-30     167.693079  167.693079   3.804936   1.134923  0.075182   \n",
      "\n",
      "                  qqe  qqel       qqes       ker  \n",
      "Date                                              \n",
      "2007-01-03   0.000000   0.0   0.000000  0.000000  \n",
      "2007-01-04   0.000000   0.0   0.000000  1.000000  \n",
      "2007-01-05   0.000000   0.0   0.000000  0.365089  \n",
      "2007-01-08   0.000000   0.0   0.000000  0.480521  \n",
      "2007-01-09   0.000000   0.0   0.000000  0.611643  \n",
      "...               ...   ...        ...       ...  \n",
      "2022-12-23  56.366488   0.0  56.366488  0.087138  \n",
      "2022-12-27  56.366488   0.0  56.366488  0.093946  \n",
      "2022-12-28  56.366488   0.0  56.366488  0.196296  \n",
      "2022-12-29  56.366488   0.0  56.366488  0.131178  \n",
      "2022-12-30  55.527836   0.0  55.527836  0.046964  \n",
      "\n",
      "[4028 rows x 19 columns]\n",
      "*******************************\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "SWK\n",
      "                close   volume       open       high        low\n",
      "Date                                                           \n",
      "2007-01-03  34.064186   666400  50.450001  50.799999  50.049999\n",
      "2007-01-04  34.278870   753200  50.750000  51.290001  50.259998\n",
      "2007-01-05  34.144688  1146400  51.000000  51.189999  50.590000\n",
      "2007-01-08  34.017227   750800  50.700001  50.860001  49.950001\n",
      "2007-01-09  34.634388  1000100  50.959999  51.730000  50.680000\n",
      "...               ...      ...        ...        ...        ...\n",
      "2022-12-23  71.870193   891200  73.290001  74.139999  72.599998\n",
      "2022-12-27  72.327049  1128100  73.940002  74.650002  73.250000\n",
      "2022-12-28  70.159462  1225600  74.459999  75.110001  72.160004\n",
      "2022-12-29  73.785049  1517000  72.879997  76.400002  72.769997\n",
      "2022-12-30  73.017166  1479000  75.000000  75.699997  74.330002\n",
      "\n",
      "[4028 rows x 5 columns]\n",
      "                close   volume       open       high        low         cci  \\\n",
      "Date                                                                          \n",
      "2007-01-03  34.064186   666400  50.450001  50.799999  50.049999         NaN   \n",
      "2007-01-04  34.278870   753200  50.750000  51.290001  50.259998   66.666667   \n",
      "2007-01-05  34.144688  1146400  51.000000  51.189999  50.590000   57.465582   \n",
      "2007-01-08  34.017227   750800  50.700001  50.860001  49.950001  -72.428753   \n",
      "2007-01-09  34.634388  1000100  50.959999  51.730000  50.680000  133.039989   \n",
      "...               ...      ...        ...        ...        ...         ...   \n",
      "2022-12-23  71.870193   891200  73.290001  74.139999  72.599998 -102.259958   \n",
      "2022-12-27  72.327049  1128100  73.940002  74.650002  73.250000  -74.373317   \n",
      "2022-12-28  70.159462  1225600  74.459999  75.110001  72.160004  -83.875878   \n",
      "2022-12-29  73.785049  1517000  72.879997  76.400002  72.769997  -35.701097   \n",
      "2022-12-30  73.017166  1479000  75.000000  75.699997  74.330002  -29.852658   \n",
      "\n",
      "             stochrsi       mfi        bop  supertrend_ub  supertrend_lb  \\\n",
      "Date                                                                       \n",
      "2007-01-03        NaN  0.500000 -21.847753     100.632439       0.217560   \n",
      "2007-01-04        NaN  0.500000 -15.991348     100.632439       0.217560   \n",
      "2007-01-05   0.000000  0.500000 -28.092259     100.632439       0.217560   \n",
      "2007-01-08   0.000000  0.500000 -18.332721     100.632439       0.217560   \n",
      "2007-01-09  60.322489  0.500000 -15.548212     100.632439       0.217560   \n",
      "...               ...       ...        ...            ...            ...   \n",
      "2022-12-23  10.566741  0.291036  -0.921952      83.126923      70.814800   \n",
      "2022-12-27  18.099673  0.363544  -1.152108      83.126923      70.814800   \n",
      "2022-12-28   0.000000  0.369272  -1.457811      83.126923      70.814800   \n",
      "2022-12-29  50.989844  0.376644   0.249325      83.126923      63.607289   \n",
      "2022-12-30  42.631826  0.365112  -1.447329      83.126923      64.411066   \n",
      "\n",
      "            supertrend    eribull    eribear       cti        qqe  qqel  \\\n",
      "Date                                                                      \n",
      "2007-01-03  100.632439  16.735813  15.985813  0.000000   0.000000   0.0   \n",
      "2007-01-04  100.632439  17.195146  16.165143  0.000000   0.000000   0.0   \n",
      "2007-01-05  100.632439  17.088025  16.488026  0.000000   0.000000   0.0   \n",
      "2007-01-08  100.632439  16.770133  15.860133  0.000000   0.000000   0.0   \n",
      "2007-01-09  100.632439  17.562343  16.512344  0.000000   0.000000   0.0   \n",
      "...                ...        ...        ...       ...        ...   ...   \n",
      "2022-12-23   83.126923  -0.146384  -1.686385 -0.816813  46.068515   0.0   \n",
      "2022-12-27   83.126923   0.643523  -0.756479 -0.860545  46.068515   0.0   \n",
      "2022-12-28   83.126923   1.653096  -1.296901 -0.895187  45.085775   0.0   \n",
      "2022-12-29   83.126923   2.896219  -0.733786 -0.774089  45.085775   0.0   \n",
      "2022-12-30   83.126923   2.265731   0.895736 -0.617490  45.085775   0.0   \n",
      "\n",
      "                 qqes       ker  \n",
      "Date                             \n",
      "2007-01-03   0.000000  0.000000  \n",
      "2007-01-04   0.000000  1.000000  \n",
      "2007-01-05   0.000000  0.230752  \n",
      "2007-01-08   0.000000  0.098586  \n",
      "2007-01-09   0.000000  0.521453  \n",
      "...               ...       ...  \n",
      "2022-12-23  46.068515  0.384729  \n",
      "2022-12-27  46.068515  0.444613  \n",
      "2022-12-28  45.085775  0.823118  \n",
      "2022-12-29  45.085775  0.355394  \n",
      "2022-12-30  45.085775  0.232466  \n",
      "\n",
      "[4028 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci</th>\n",
       "      <th>BBWI_stochrsi</th>\n",
       "      <th>BBWI_mfi</th>\n",
       "      <th>BBWI_bop</th>\n",
       "      <th>BBWI_supertrend_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>SWK_eribull</th>\n",
       "      <th>SWK_eribear</th>\n",
       "      <th>SWK_cti</th>\n",
       "      <th>SWK_qqe</th>\n",
       "      <th>SWK_qqel</th>\n",
       "      <th>SWK_qqes</th>\n",
       "      <th>SWK_ker</th>\n",
       "      <th>SWK_momentum</th>\n",
       "      <th>SWK_simple_moving_average</th>\n",
       "      <th>SWK_bollinger_bands</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>10.077243</td>\n",
       "      <td>9288633</td>\n",
       "      <td>24.252222</td>\n",
       "      <td>24.276476</td>\n",
       "      <td>23.540825</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.268619</td>\n",
       "      <td>66.506350</td>\n",
       "      <td>...</td>\n",
       "      <td>16.735813</td>\n",
       "      <td>15.985813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>9.317530</td>\n",
       "      <td>17476831</td>\n",
       "      <td>22.546482</td>\n",
       "      <td>22.756668</td>\n",
       "      <td>21.891672</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-15.293658</td>\n",
       "      <td>62.557724</td>\n",
       "      <td>...</td>\n",
       "      <td>17.195146</td>\n",
       "      <td>16.165143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>9.310719</td>\n",
       "      <td>9970096</td>\n",
       "      <td>21.980598</td>\n",
       "      <td>22.433306</td>\n",
       "      <td>21.147940</td>\n",
       "      <td>-68.036202</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-9.857021</td>\n",
       "      <td>61.706626</td>\n",
       "      <td>...</td>\n",
       "      <td>17.088025</td>\n",
       "      <td>16.488026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230752</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>9.130158</td>\n",
       "      <td>9207238</td>\n",
       "      <td>22.029102</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.471302</td>\n",
       "      <td>-53.794519</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-12.972356</td>\n",
       "      <td>61.706626</td>\n",
       "      <td>...</td>\n",
       "      <td>16.770133</td>\n",
       "      <td>15.860133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098586</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-09</th>\n",
       "      <td>9.266429</td>\n",
       "      <td>10808659</td>\n",
       "      <td>21.843168</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.827002</td>\n",
       "      <td>-33.004998</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.692961</td>\n",
       "      <td>61.706626</td>\n",
       "      <td>...</td>\n",
       "      <td>17.562343</td>\n",
       "      <td>16.512344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521453</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>41.287586</td>\n",
       "      <td>2461500</td>\n",
       "      <td>40.330002</td>\n",
       "      <td>41.950001</td>\n",
       "      <td>40.150002</td>\n",
       "      <td>-26.952877</td>\n",
       "      <td>50.075647</td>\n",
       "      <td>0.271783</td>\n",
       "      <td>0.531992</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146384</td>\n",
       "      <td>-1.686385</td>\n",
       "      <td>-0.816813</td>\n",
       "      <td>46.068515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.068515</td>\n",
       "      <td>0.384729</td>\n",
       "      <td>0.935280</td>\n",
       "      <td>-0.054770</td>\n",
       "      <td>-0.699713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>41.297432</td>\n",
       "      <td>3022500</td>\n",
       "      <td>41.939999</td>\n",
       "      <td>42.889999</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>46.300247</td>\n",
       "      <td>50.368039</td>\n",
       "      <td>0.338524</td>\n",
       "      <td>-0.594970</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643523</td>\n",
       "      <td>-0.756479</td>\n",
       "      <td>-0.860545</td>\n",
       "      <td>46.068515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.068515</td>\n",
       "      <td>0.444613</td>\n",
       "      <td>0.933509</td>\n",
       "      <td>-0.045928</td>\n",
       "      <td>-0.565087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>40.116104</td>\n",
       "      <td>2138000</td>\n",
       "      <td>41.779999</td>\n",
       "      <td>42.169998</td>\n",
       "      <td>40.630001</td>\n",
       "      <td>-29.822199</td>\n",
       "      <td>6.012837</td>\n",
       "      <td>0.345384</td>\n",
       "      <td>-1.080453</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653096</td>\n",
       "      <td>-1.296901</td>\n",
       "      <td>-0.895187</td>\n",
       "      <td>45.085775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.085775</td>\n",
       "      <td>0.823118</td>\n",
       "      <td>0.883260</td>\n",
       "      <td>-0.070031</td>\n",
       "      <td>-0.800777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>41.002106</td>\n",
       "      <td>1610500</td>\n",
       "      <td>41.349998</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>41.180000</td>\n",
       "      <td>15.208497</td>\n",
       "      <td>43.168578</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>-0.424260</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>2.896219</td>\n",
       "      <td>-0.733786</td>\n",
       "      <td>-0.774089</td>\n",
       "      <td>45.085775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.085775</td>\n",
       "      <td>0.355394</td>\n",
       "      <td>0.922133</td>\n",
       "      <td>-0.018299</td>\n",
       "      <td>-0.216329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>41.484478</td>\n",
       "      <td>2237000</td>\n",
       "      <td>41.099998</td>\n",
       "      <td>42.209999</td>\n",
       "      <td>40.880001</td>\n",
       "      <td>37.972571</td>\n",
       "      <td>62.418304</td>\n",
       "      <td>0.358997</td>\n",
       "      <td>0.289083</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>2.265731</td>\n",
       "      <td>0.895736</td>\n",
       "      <td>-0.617490</td>\n",
       "      <td>45.085775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.085775</td>\n",
       "      <td>0.232466</td>\n",
       "      <td>0.916768</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>-0.299260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4028 rows Ã— 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "Date                                                                   \n",
       "2007-01-03   10.077243      9288633  24.252222  24.276476  23.540825   \n",
       "2007-01-04    9.317530     17476831  22.546482  22.756668  21.891672   \n",
       "2007-01-05    9.310719      9970096  21.980598  22.433306  21.147940   \n",
       "2007-01-08    9.130158      9207238  22.029102  22.465643  21.471302   \n",
       "2007-01-09    9.266429     10808659  21.843168  22.465643  21.827002   \n",
       "...                ...          ...        ...        ...        ...   \n",
       "2022-12-23   41.287586      2461500  40.330002  41.950001  40.150002   \n",
       "2022-12-27   41.297432      3022500  41.939999  42.889999  41.810001   \n",
       "2022-12-28   40.116104      2138000  41.779999  42.169998  40.630001   \n",
       "2022-12-29   41.002106      1610500  41.349998  42.000000  41.180000   \n",
       "2022-12-30   41.484478      2237000  41.099998  42.209999  40.880001   \n",
       "\n",
       "             BBWI_cci  BBWI_stochrsi  BBWI_mfi   BBWI_bop  BBWI_supertrend_ub  \\\n",
       "Date                                                                            \n",
       "2007-01-03 -66.666667     100.000000  0.500000 -19.268619           66.506350   \n",
       "2007-01-04 -66.666667     100.000000  0.500000 -15.293658           62.557724   \n",
       "2007-01-05 -68.036202     100.000000  0.500000  -9.857021           61.706626   \n",
       "2007-01-08 -53.794519     100.000000  0.500000 -12.972356           61.706626   \n",
       "2007-01-09 -33.004998     100.000000  0.500000 -19.692961           61.706626   \n",
       "...               ...            ...       ...        ...                 ...   \n",
       "2022-12-23 -26.952877      50.075647  0.271783   0.531992           46.013339   \n",
       "2022-12-27  46.300247      50.368039  0.338524  -0.594970           46.013339   \n",
       "2022-12-28 -29.822199       6.012837  0.345384  -1.080453           46.013339   \n",
       "2022-12-29  15.208497      43.168578  0.393974  -0.424260           46.013339   \n",
       "2022-12-30  37.972571      62.418304  0.358997   0.289083           46.013339   \n",
       "\n",
       "            ...  SWK_eribull  SWK_eribear   SWK_cti    SWK_qqe  SWK_qqel  \\\n",
       "Date        ...                                                            \n",
       "2007-01-03  ...    16.735813    15.985813  0.000000   0.000000       0.0   \n",
       "2007-01-04  ...    17.195146    16.165143  0.000000   0.000000       0.0   \n",
       "2007-01-05  ...    17.088025    16.488026  0.000000   0.000000       0.0   \n",
       "2007-01-08  ...    16.770133    15.860133  0.000000   0.000000       0.0   \n",
       "2007-01-09  ...    17.562343    16.512344  0.000000   0.000000       0.0   \n",
       "...         ...          ...          ...       ...        ...       ...   \n",
       "2022-12-23  ...    -0.146384    -1.686385 -0.816813  46.068515       0.0   \n",
       "2022-12-27  ...     0.643523    -0.756479 -0.860545  46.068515       0.0   \n",
       "2022-12-28  ...     1.653096    -1.296901 -0.895187  45.085775       0.0   \n",
       "2022-12-29  ...     2.896219    -0.733786 -0.774089  45.085775       0.0   \n",
       "2022-12-30  ...     2.265731     0.895736 -0.617490  45.085775       0.0   \n",
       "\n",
       "             SWK_qqes   SWK_ker  SWK_momentum  SWK_simple_moving_average  \\\n",
       "Date                                                                       \n",
       "2007-01-03   0.000000  0.000000      1.127610                   0.074882   \n",
       "2007-01-04   0.000000  1.000000      1.127610                   0.074882   \n",
       "2007-01-05   0.000000  0.230752      1.127610                   0.074882   \n",
       "2007-01-08   0.000000  0.098586      1.127610                   0.074882   \n",
       "2007-01-09   0.000000  0.521453      1.127610                   0.074882   \n",
       "...               ...       ...           ...                        ...   \n",
       "2022-12-23  46.068515  0.384729      0.935280                  -0.054770   \n",
       "2022-12-27  46.068515  0.444613      0.933509                  -0.045928   \n",
       "2022-12-28  45.085775  0.823118      0.883260                  -0.070031   \n",
       "2022-12-29  45.085775  0.355394      0.922133                  -0.018299   \n",
       "2022-12-30  45.085775  0.232466      0.916768                  -0.023972   \n",
       "\n",
       "            SWK_bollinger_bands  \n",
       "Date                             \n",
       "2007-01-03             1.098085  \n",
       "2007-01-04             1.098085  \n",
       "2007-01-05             1.098085  \n",
       "2007-01-08             1.098085  \n",
       "2007-01-09             1.098085  \n",
       "...                         ...  \n",
       "2022-12-23            -0.699713  \n",
       "2022-12-27            -0.565087  \n",
       "2022-12-28            -0.800777  \n",
       "2022-12-29            -0.216329  \n",
       "2022-12-30            -0.299260  \n",
       "\n",
       "[4028 rows x 220 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stockstats import StockDataFrame\n",
    "def extract_and_wrap_ticker_data(df, columns):\n",
    "    # Split column names into parts\n",
    "    parts = [col.split('_') for col in columns]\n",
    "\n",
    "    # Create a dictionary to store information about each ticker\n",
    "    ticker_info = {}\n",
    "\n",
    "    # Iterate through the parts list\n",
    "    for part in parts:\n",
    "        data_type = part[0]  # e.g., 'Adj Close', 'Close', etc.\n",
    "        ticker = part[1]     # e.g., 'AAPL', 'GOOGL', etc.\n",
    "\n",
    "        # Check if the ticker is already in the dictionary\n",
    "        if ticker not in ticker_info:\n",
    "            ticker_info[ticker] = {}\n",
    "\n",
    "        # Add information for the ticker\n",
    "        ticker_info[ticker][data_type] = df[f'{data_type}_{ticker}']\n",
    "\n",
    "    # Create a dictionary to store stockstats DataFrames for each ticker\n",
    "    ticker_dataframes = pd.DataFrame()\n",
    "#     var_importance_all=\n",
    "\n",
    "    # Iterate through ticker_info and wrap data with stockstats\n",
    "    for ticker, data in ticker_info.items():\n",
    "        print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        df = pd.DataFrame(data)\n",
    "        print(ticker)\n",
    "#         print(df)\n",
    "        \n",
    "        stock_df = StockDataFrame.retype(df)\n",
    "        print(stock_df)\n",
    "\n",
    "\n",
    "#         stock_df[\"close\"] = stock_df[\"close\"].pct_change(1)\n",
    "#         stock_df=stock_df.dropna(subset=[\"close\"])\n",
    "#         print(stock_df)\n",
    "        \n",
    "#         stock_df['macds']=stock_df['macds']\n",
    "        stock_df['cci']=stock_df['cci']\n",
    "        stock_df['stochrsi']=stock_df['stochrsi']\n",
    "#         stock_df['ichimoku cloud']=stock_df['ichimoku cloud']\n",
    "        stock_df['mfi']=stock_df['mfi']\n",
    "        stock_df['bop']=stock_df['bop']\n",
    "        stock_df['supertrend']=stock_df['supertrend']\n",
    "        stock_df['eribull']=stock_df['eribull']\n",
    "        \n",
    "        stock_df['cti']=stock_df['cti']\n",
    "        stock_df['qqe']=stock_df['qqe']\n",
    "        stock_df['ker']=stock_df['ker']\n",
    "        print(stock_df)\n",
    "        roll_stat=rolling_statistics(stock_df,cols='close',\n",
    "                      window=20)\n",
    "        stock_df = pd.concat([stock_df,roll_stat],axis=1)\n",
    "\n",
    "\n",
    "#         print(stock_df)\n",
    "\n",
    "        stock_df.fillna(method='ffill',inplace=True)\n",
    "        stock_df.fillna(method='bfill',inplace=True)\n",
    "#         check_var_importance(stock_df)\n",
    "\n",
    "        print(\"*******************************\")\n",
    "\n",
    "        stock_df.columns = [ticker+\"_\" + col for col in pd.DataFrame(stock_df).columns]\n",
    "        ticker_dataframes = pd.concat([ticker_dataframes,stock_df],axis=1)\n",
    "        \n",
    "    return ticker_dataframes\n",
    "\n",
    "# Sample DataFrame with specified columns\n",
    "columns = list(df2.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Extract and wrap ticker data\n",
    "ticker_dataframes = extract_and_wrap_ticker_data(df2, columns)\n",
    "\n",
    "# Access information for a specific ticker (e.g., AAPL)\n",
    "# print(ticker_dataframes)\n",
    "\n",
    "ticker_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfe6dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            wti_spot  10y2y_spread  10y3m_spread  3m_rate  ltiit  ted_spread  \\\n",
      "Date                                                                           \n",
      "2007-01-02     60.77         -0.12         -0.39     4.94   2.33        0.42   \n",
      "2007-01-03     58.31         -0.09         -0.38     4.92   2.32        0.44   \n",
      "2007-01-04     55.65         -0.09         -0.42     4.91   2.30        0.45   \n",
      "2007-01-05     56.29         -0.11         -0.40     4.92   2.33        0.44   \n",
      "2007-01-08     56.08         -0.12         -0.42     4.95   2.34        0.41   \n",
      "...              ...           ...           ...      ...    ...         ...   \n",
      "2022-07-12       NaN         -0.07          0.74     2.16   1.12         NaN   \n",
      "2022-07-13       NaN         -0.22          0.52     2.33   1.06         NaN   \n",
      "2022-07-14       NaN         -0.19          0.56     2.33   1.07         NaN   \n",
      "2022-07-15       NaN         -0.20          0.56     2.29   1.03         NaN   \n",
      "2022-07-18       NaN         -0.19          0.46      NaN    NaN         NaN   \n",
      "\n",
      "             var_wti  \n",
      "Date                  \n",
      "2007-01-02       NaN  \n",
      "2007-01-03 -0.040481  \n",
      "2007-01-04 -0.045618  \n",
      "2007-01-05  0.011500  \n",
      "2007-01-08 -0.003731  \n",
      "...              ...  \n",
      "2022-07-12  0.000000  \n",
      "2022-07-13  0.000000  \n",
      "2022-07-14  0.000000  \n",
      "2022-07-15  0.000000  \n",
      "2022-07-18  0.000000  \n",
      "\n",
      "[3944 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the tickers\n",
    "new_tickers = ['EIA/PET_RWTC_D','FRED/T10Y2Y','FRED/T10Y3M','FRED/DTB3','FRED/DLTIIT','FRED/TEDRATE']\n",
    "\n",
    "# # Define the date range\n",
    "# start='2020-01-01'\n",
    "# end='2021-01-01'\n",
    "names = ['wti_spot','10y2y_spread','10y3m_spread','3m_rate','ltiit','ted_spread']\n",
    "# add_factors = quandl.get(tickers, start=start_date, end=end_date)\n",
    "\n",
    "# # Retrieve data\n",
    "data_quandl = quandl.get(new_tickers, start_date=start_date, end_date=end_date)\n",
    "data_quandl.columns = names\n",
    "data_quandl['var_wti'] = data_quandl['wti_spot'].pct_change()\n",
    "# print(data_quandl)\n",
    "\n",
    "# # Fill NaN values:\n",
    "# data_quandl.fillna(method='ffill',inplace=True)\n",
    "# data_quandl.fillna(method='bfill',inplace=True)\n",
    "\n",
    "\n",
    "# # Print the data\n",
    "print(data_quandl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1459bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci</th>\n",
       "      <th>BBWI_stochrsi</th>\n",
       "      <th>BBWI_mfi</th>\n",
       "      <th>BBWI_bop</th>\n",
       "      <th>...</th>\n",
       "      <th>SWK_momentum</th>\n",
       "      <th>SWK_simple_moving_average</th>\n",
       "      <th>SWK_bollinger_bands</th>\n",
       "      <th>wti_spot</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>10.077243</td>\n",
       "      <td>9288633</td>\n",
       "      <td>24.252222</td>\n",
       "      <td>24.276476</td>\n",
       "      <td>23.540825</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.268619</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>58.31</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.040481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>9.317530</td>\n",
       "      <td>17476831</td>\n",
       "      <td>22.546482</td>\n",
       "      <td>22.756668</td>\n",
       "      <td>21.891672</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-15.293658</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>55.65</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.91</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.045618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>9.310719</td>\n",
       "      <td>9970096</td>\n",
       "      <td>21.980598</td>\n",
       "      <td>22.433306</td>\n",
       "      <td>21.147940</td>\n",
       "      <td>-68.036202</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-9.857021</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>56.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-01-08</td>\n",
       "      <td>9.130158</td>\n",
       "      <td>9207238</td>\n",
       "      <td>22.029102</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.471302</td>\n",
       "      <td>-53.794519</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-12.972356</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>56.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.003731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-01-09</td>\n",
       "      <td>9.266429</td>\n",
       "      <td>10808659</td>\n",
       "      <td>21.843168</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.827002</td>\n",
       "      <td>-33.004998</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.692961</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>55.65</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.007668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>2022-07-12</td>\n",
       "      <td>26.133543</td>\n",
       "      <td>3131200</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>27.610001</td>\n",
       "      <td>26.680000</td>\n",
       "      <td>-60.147023</td>\n",
       "      <td>71.722638</td>\n",
       "      <td>0.398109</td>\n",
       "      <td>-0.716619</td>\n",
       "      <td>...</td>\n",
       "      <td>1.013797</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>26.678598</td>\n",
       "      <td>3244200</td>\n",
       "      <td>26.450001</td>\n",
       "      <td>27.540001</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>-57.069069</td>\n",
       "      <td>99.104567</td>\n",
       "      <td>0.413616</td>\n",
       "      <td>0.156574</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011708</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.141272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>25.267290</td>\n",
       "      <td>4278500</td>\n",
       "      <td>26.870001</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>25.809999</td>\n",
       "      <td>-93.255672</td>\n",
       "      <td>63.458163</td>\n",
       "      <td>0.423011</td>\n",
       "      <td>-1.324553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995777</td>\n",
       "      <td>-0.009406</td>\n",
       "      <td>-0.163278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>26.775930</td>\n",
       "      <td>4123700</td>\n",
       "      <td>26.459999</td>\n",
       "      <td>27.680000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>-34.397976</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.374416</td>\n",
       "      <td>0.220931</td>\n",
       "      <td>...</td>\n",
       "      <td>1.053392</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.002596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>27.817379</td>\n",
       "      <td>5343900</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>138.383628</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.393358</td>\n",
       "      <td>-0.162671</td>\n",
       "      <td>...</td>\n",
       "      <td>1.034479</td>\n",
       "      <td>-0.005507</td>\n",
       "      <td>-0.105374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3912 rows Ã— 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "0    2007-01-03   10.077243      9288633  24.252222  24.276476  23.540825   \n",
       "1    2007-01-04    9.317530     17476831  22.546482  22.756668  21.891672   \n",
       "2    2007-01-05    9.310719      9970096  21.980598  22.433306  21.147940   \n",
       "3    2007-01-08    9.130158      9207238  22.029102  22.465643  21.471302   \n",
       "4    2007-01-09    9.266429     10808659  21.843168  22.465643  21.827002   \n",
       "...         ...         ...          ...        ...        ...        ...   \n",
       "3907 2022-07-12   26.133543      3131200  26.799999  27.610001  26.680000   \n",
       "3908 2022-07-13   26.678598      3244200  26.450001  27.540001  26.080000   \n",
       "3909 2022-07-14   25.267290      4278500  26.870001  27.020000  25.809999   \n",
       "3910 2022-07-15   26.775930      4123700  26.459999  27.680000  26.250000   \n",
       "3911 2022-07-18   27.817379      5343900  28.049999  29.480000  28.049999   \n",
       "\n",
       "        BBWI_cci  BBWI_stochrsi  BBWI_mfi   BBWI_bop  ...  SWK_momentum  \\\n",
       "0     -66.666667     100.000000  0.500000 -19.268619  ...      1.127610   \n",
       "1     -66.666667     100.000000  0.500000 -15.293658  ...      1.127610   \n",
       "2     -68.036202     100.000000  0.500000  -9.857021  ...      1.127610   \n",
       "3     -53.794519     100.000000  0.500000 -12.972356  ...      1.127610   \n",
       "4     -33.004998     100.000000  0.500000 -19.692961  ...      1.127610   \n",
       "...          ...            ...       ...        ...  ...           ...   \n",
       "3907  -60.147023      71.722638  0.398109  -0.716619  ...      1.013797   \n",
       "3908  -57.069069      99.104567  0.413616   0.156574  ...      1.011708   \n",
       "3909  -93.255672      63.458163  0.423011  -1.324553  ...      0.995777   \n",
       "3910  -34.397976     100.000000  0.374416   0.220931  ...      1.053392   \n",
       "3911  138.383628     100.000000  0.393358  -0.162671  ...      1.034479   \n",
       "\n",
       "      SWK_simple_moving_average  SWK_bollinger_bands  wti_spot  10y2y_spread  \\\n",
       "0                      0.074882             1.098085     58.31         -0.09   \n",
       "1                      0.074882             1.098085     55.65         -0.09   \n",
       "2                      0.074882             1.098085     56.29         -0.11   \n",
       "3                      0.074882             1.098085     56.08         -0.12   \n",
       "4                      0.074882             1.098085     55.65         -0.13   \n",
       "...                         ...                  ...       ...           ...   \n",
       "3907                   0.002237             0.038829       NaN         -0.07   \n",
       "3908                   0.008115             0.141272       NaN         -0.22   \n",
       "3909                  -0.009406            -0.163278       NaN         -0.19   \n",
       "3910                  -0.000149            -0.002596       NaN         -0.20   \n",
       "3911                  -0.005507            -0.105374       NaN         -0.19   \n",
       "\n",
       "      10y3m_spread  3m_rate  ltiit  ted_spread   var_wti  \n",
       "0            -0.38     4.92   2.32        0.44 -0.040481  \n",
       "1            -0.42     4.91   2.30        0.45 -0.045618  \n",
       "2            -0.40     4.92   2.33        0.44  0.011500  \n",
       "3            -0.42     4.95   2.34        0.41 -0.003731  \n",
       "4            -0.42     4.95   2.35        0.41 -0.007668  \n",
       "...            ...      ...    ...         ...       ...  \n",
       "3907          0.74     2.16   1.12         NaN  0.000000  \n",
       "3908          0.52     2.33   1.06         NaN  0.000000  \n",
       "3909          0.56     2.33   1.07         NaN  0.000000  \n",
       "3910          0.56     2.29   1.03         NaN  0.000000  \n",
       "3911          0.46      NaN    NaN         NaN  0.000000  \n",
       "\n",
       "[3912 rows x 228 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df=pd.merge(ticker_dataframes.reset_index(),data_quandl.reset_index(),on=['Date'],how='inner')\n",
    "# final_df=ticker_dataframes.copy()\n",
    "\n",
    "# Fill NaN values:\n",
    "# final_df.fillna(method='ffill',inplace=True)\n",
    "# final_df.fillna(method='bfill',inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6053a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci</th>\n",
       "      <th>BBWI_stochrsi</th>\n",
       "      <th>BBWI_mfi</th>\n",
       "      <th>BBWI_bop</th>\n",
       "      <th>BBWI_supertrend_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>SWK_ker</th>\n",
       "      <th>SWK_momentum</th>\n",
       "      <th>SWK_simple_moving_average</th>\n",
       "      <th>SWK_bollinger_bands</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-18</th>\n",
       "      <td>0.022358</td>\n",
       "      <td>4008127</td>\n",
       "      <td>22.667746</td>\n",
       "      <td>23.435732</td>\n",
       "      <td>22.554567</td>\n",
       "      <td>64.492789</td>\n",
       "      <td>97.163927</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-14.764028</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807868</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>4.99</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.034226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-19</th>\n",
       "      <td>0.001411</td>\n",
       "      <td>4918559</td>\n",
       "      <td>22.441389</td>\n",
       "      <td>23.039612</td>\n",
       "      <td>22.384802</td>\n",
       "      <td>32.172596</td>\n",
       "      <td>98.183420</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.501164</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422496</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.029103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-22</th>\n",
       "      <td>-0.003522</td>\n",
       "      <td>4623906</td>\n",
       "      <td>23.282133</td>\n",
       "      <td>23.435732</td>\n",
       "      <td>22.772839</td>\n",
       "      <td>68.262730</td>\n",
       "      <td>95.793785</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-20.583048</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504272</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.016737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-23</th>\n",
       "      <td>-0.000707</td>\n",
       "      <td>4078018</td>\n",
       "      <td>22.691998</td>\n",
       "      <td>23.120453</td>\n",
       "      <td>22.675829</td>\n",
       "      <td>42.442588</td>\n",
       "      <td>95.294295</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-29.375478</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588112</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.048914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-24</th>\n",
       "      <td>0.001061</td>\n",
       "      <td>3377257</td>\n",
       "      <td>22.789005</td>\n",
       "      <td>23.088116</td>\n",
       "      <td>22.732416</td>\n",
       "      <td>64.123934</td>\n",
       "      <td>96.228779</td>\n",
       "      <td>0.466907</td>\n",
       "      <td>-36.963312</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536994</td>\n",
       "      <td>1.127610</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.011752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12</th>\n",
       "      <td>-0.002600</td>\n",
       "      <td>3131200</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>27.610001</td>\n",
       "      <td>26.680000</td>\n",
       "      <td>-60.147023</td>\n",
       "      <td>71.722638</td>\n",
       "      <td>0.398109</td>\n",
       "      <td>-0.716619</td>\n",
       "      <td>32.801145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121495</td>\n",
       "      <td>1.013797</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-13</th>\n",
       "      <td>0.020857</td>\n",
       "      <td>3244200</td>\n",
       "      <td>26.450001</td>\n",
       "      <td>27.540001</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>-57.069069</td>\n",
       "      <td>99.104567</td>\n",
       "      <td>0.413616</td>\n",
       "      <td>0.156574</td>\n",
       "      <td>32.374992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036593</td>\n",
       "      <td>1.011708</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.141272</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-14</th>\n",
       "      <td>-0.052900</td>\n",
       "      <td>4278500</td>\n",
       "      <td>26.870001</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>25.809999</td>\n",
       "      <td>-93.255672</td>\n",
       "      <td>63.458163</td>\n",
       "      <td>0.423011</td>\n",
       "      <td>-1.324553</td>\n",
       "      <td>31.841778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.995777</td>\n",
       "      <td>-0.009406</td>\n",
       "      <td>-0.163278</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>0.059707</td>\n",
       "      <td>4123700</td>\n",
       "      <td>26.459999</td>\n",
       "      <td>27.680000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>-34.397976</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.374416</td>\n",
       "      <td>0.220931</td>\n",
       "      <td>31.841778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147882</td>\n",
       "      <td>1.053392</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.002596</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>0.038895</td>\n",
       "      <td>5343900</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>138.383628</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.393358</td>\n",
       "      <td>-0.162671</td>\n",
       "      <td>31.841778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036254</td>\n",
       "      <td>1.034479</td>\n",
       "      <td>-0.005507</td>\n",
       "      <td>-0.105374</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3902 rows Ã— 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "Date                                                                   \n",
       "2007-01-18    0.022358      4008127  22.667746  23.435732  22.554567   \n",
       "2007-01-19    0.001411      4918559  22.441389  23.039612  22.384802   \n",
       "2007-01-22   -0.003522      4623906  23.282133  23.435732  22.772839   \n",
       "2007-01-23   -0.000707      4078018  22.691998  23.120453  22.675829   \n",
       "2007-01-24    0.001061      3377257  22.789005  23.088116  22.732416   \n",
       "...                ...          ...        ...        ...        ...   \n",
       "2022-07-12   -0.002600      3131200  26.799999  27.610001  26.680000   \n",
       "2022-07-13    0.020857      3244200  26.450001  27.540001  26.080000   \n",
       "2022-07-14   -0.052900      4278500  26.870001  27.020000  25.809999   \n",
       "2022-07-15    0.059707      4123700  26.459999  27.680000  26.250000   \n",
       "2022-07-18    0.038895      5343900  28.049999  29.480000  28.049999   \n",
       "\n",
       "              BBWI_cci  BBWI_stochrsi  BBWI_mfi   BBWI_bop  \\\n",
       "Date                                                         \n",
       "2007-01-18   64.492789      97.163927  0.500000 -14.764028   \n",
       "2007-01-19   32.172596      98.183420  0.500000 -19.501164   \n",
       "2007-01-22   68.262730      95.793785  0.500000 -20.583048   \n",
       "2007-01-23   42.442588      95.294295  0.500000 -29.375478   \n",
       "2007-01-24   64.123934      96.228779  0.466907 -36.963312   \n",
       "...                ...            ...       ...        ...   \n",
       "2022-07-12  -60.147023      71.722638  0.398109  -0.716619   \n",
       "2022-07-13  -57.069069      99.104567  0.413616   0.156574   \n",
       "2022-07-14  -93.255672      63.458163  0.423011  -1.324553   \n",
       "2022-07-15  -34.397976     100.000000  0.374416   0.220931   \n",
       "2022-07-18  138.383628     100.000000  0.393358  -0.162671   \n",
       "\n",
       "            BBWI_supertrend_ub  ...   SWK_ker  SWK_momentum  \\\n",
       "Date                            ...                           \n",
       "2007-01-18           61.163117  ...  0.807868      1.127610   \n",
       "2007-01-19           61.163117  ...  0.422496      1.127610   \n",
       "2007-01-22           61.163117  ...  0.504272      1.127610   \n",
       "2007-01-23           61.163117  ...  0.588112      1.127610   \n",
       "2007-01-24           61.163117  ...  0.536994      1.127610   \n",
       "...                        ...  ...       ...           ...   \n",
       "2022-07-12           32.801145  ...  0.121495      1.013797   \n",
       "2022-07-13           32.374992  ...  0.036593      1.011708   \n",
       "2022-07-14           31.841778  ...  0.001285      0.995777   \n",
       "2022-07-15           31.841778  ...  0.147882      1.053392   \n",
       "2022-07-18           31.841778  ...  0.036254      1.034479   \n",
       "\n",
       "            SWK_simple_moving_average  SWK_bollinger_bands  10y2y_spread  \\\n",
       "Date                                                                       \n",
       "2007-01-18                   0.074882             1.098085         -0.14   \n",
       "2007-01-19                   0.074882             1.098085         -0.15   \n",
       "2007-01-22                   0.074882             1.098085         -0.15   \n",
       "2007-01-23                   0.074882             1.098085         -0.13   \n",
       "2007-01-24                   0.074882             1.098085         -0.12   \n",
       "...                               ...                  ...           ...   \n",
       "2022-07-12                   0.002237             0.038829         -0.07   \n",
       "2022-07-13                   0.008115             0.141272         -0.22   \n",
       "2022-07-14                  -0.009406            -0.163278         -0.19   \n",
       "2022-07-15                  -0.000149            -0.002596         -0.20   \n",
       "2022-07-18                  -0.005507            -0.105374         -0.19   \n",
       "\n",
       "            10y3m_spread  3m_rate  ltiit  ted_spread   var_wti  \n",
       "Date                                                            \n",
       "2007-01-18         -0.37     4.99   2.41        0.37 -0.034226  \n",
       "2007-01-19         -0.36     5.01   2.42        0.35  0.029103  \n",
       "2007-01-22         -0.37     5.00   2.39        0.36 -0.016737  \n",
       "2007-01-23         -0.33     5.01   2.41        0.35  0.048914  \n",
       "2007-01-24         -0.32     5.00   2.39        0.36  0.011752  \n",
       "...                  ...      ...    ...         ...       ...  \n",
       "2022-07-12          0.74     2.16   1.12         NaN  0.000000  \n",
       "2022-07-13          0.52     2.33   1.06         NaN  0.000000  \n",
       "2022-07-14          0.56     2.33   1.07         NaN  0.000000  \n",
       "2022-07-15          0.56     2.29   1.03         NaN  0.000000  \n",
       "2022-07-18          0.46      NaN    NaN         NaN  0.000000  \n",
       "\n",
       "[3902 rows x 226 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2=final_df.copy()\n",
    "for ticker in tickers:\n",
    "    final_df2[ticker+\"_close\"] = final_df2[ticker+\"_close\"].pct_change()\n",
    "#     final_df.drop(columns=[ticker+\"_adj close\"],inplace=True)\n",
    "    final_df2=final_df2.dropna(subset=[ticker+\"_close\"])\n",
    "final_df2.set_index(['Date'],inplace=True)\n",
    "final_df2.drop(columns=['wti_spot'],inplace=True)\n",
    "final_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12af3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def check_var_importance(df,ticker):\n",
    "    # Separate features (X) and target (y)\n",
    "    # AAPL_df.drop(columns=[\"AAPL_cr\"],inplace=True)\n",
    "    X = df  # Use relevant columns\n",
    "    y = df[ticker+'_close']  # Target variable is 'Close'\n",
    "\n",
    "    # Standardize features (optional but often recommended for neural networks)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Create sequences of length 5 for X and corresponding y for training\n",
    "    X_train, y_train = [], []\n",
    "\n",
    "    for i in range(5, len(X)):\n",
    "        X_train.append(X[i-5:i])\n",
    "        y_train.append(y[i])\n",
    "\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "#     print(X_train)\n",
    "#     print(\"***************\")\n",
    "#     print(y_train)\n",
    "\n",
    "    # Step 2: Check Variable Importance\n",
    "    # Use RandomForestRegressor to determine importance\n",
    "    rf = RandomForestRegressor(n_estimators=10)\n",
    "    rf.fit(X_train.reshape(-1, 5 * X.shape[1]), y_train)\n",
    "\n",
    "    # Get feature importances along with their corresponding feature names\n",
    "    feature_importances = rf.feature_importances_\n",
    "    feature_names = [col.split(ticker+\"_\")[-1] for col in df.columns]\n",
    "#     print(feature_names)\n",
    "\n",
    "    # Combine feature names with their importances\n",
    "    feature_importance_info = list(zip(feature_names, feature_importances))\n",
    "\n",
    "    # Sort the list by importance (in descending order)\n",
    "    feature_importance_info.sort(key=lambda x: x[0])\n",
    "\n",
    "    return pd.DataFrame(feature_importance_info,columns=['Features',ticker+'_Relative_Importance'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55920c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Features DIS_Relative_Importance BIIB_Relative_Importance  \\\n",
      "0         bollinger_bands                0.007488                 0.004856   \n",
      "1                     bop                0.017155                  0.03182   \n",
      "2                     cci                0.008339                 0.007719   \n",
      "3                   close                 0.03147                 0.013267   \n",
      "4                     cti                0.008852                 0.012871   \n",
      "5                 eribear                0.007958                 0.006587   \n",
      "6                 eribull                 0.01113                 0.013581   \n",
      "7                    high                0.004509                  0.00211   \n",
      "8                     ker                 0.01682                 0.029807   \n",
      "9                     low                0.006655                 0.003702   \n",
      "10                    mfi                 0.01003                 0.010346   \n",
      "11               momentum                0.022341                 0.014543   \n",
      "12                   open                0.012499                 0.003611   \n",
      "13                    qqe                0.004816                 0.004326   \n",
      "14                   qqel                0.003082                 0.002711   \n",
      "15                   qqes                0.003263                 0.004015   \n",
      "16  simple_moving_average                0.009263                 0.006631   \n",
      "17               stochrsi                0.004384                 0.005879   \n",
      "18             supertrend                0.001331                 0.008534   \n",
      "19          supertrend_lb                0.002869                 0.004742   \n",
      "20          supertrend_ub                0.000525                 0.007367   \n",
      "21                 volume                0.012126                 0.015747   \n",
      "\n",
      "   GILD_Relative_Importance ETR_Relative_Importance PEP_Relative_Importance  \\\n",
      "0                  0.008581                0.013891                0.005018   \n",
      "1                  0.013984                0.012446                0.012384   \n",
      "2                  0.011249                0.007782                 0.00902   \n",
      "3                  0.038689                 0.04462                0.035998   \n",
      "4                  0.011369                0.011946                0.009832   \n",
      "5                  0.005911                0.005868                0.009095   \n",
      "6                  0.004845                  0.0088                0.011676   \n",
      "7                   0.00339                0.003306                0.002222   \n",
      "8                  0.012188                 0.01289                 0.01037   \n",
      "9                  0.003603                0.005904                0.002555   \n",
      "10                 0.015033                0.006466                0.009808   \n",
      "11                 0.012173                0.013753                0.014018   \n",
      "12                 0.004465                0.005016                0.004571   \n",
      "13                 0.006418                0.009626                0.007079   \n",
      "14                 0.004853                0.002764                0.005767   \n",
      "15                 0.012665                0.002054                0.003374   \n",
      "16                 0.014224                0.017999                0.011933   \n",
      "17                 0.006736                0.007004                0.005437   \n",
      "18                 0.000969                  0.0006                0.001471   \n",
      "19                 0.001937                0.000683                0.000928   \n",
      "20                 0.000776                0.000287                0.000266   \n",
      "21                 0.016717                0.013235                0.015496   \n",
      "\n",
      "   NVDA_Relative_Importance NI_Relative_Importance BBWI_Relative_Importance  \\\n",
      "0                  0.011538               0.011221                 0.007981   \n",
      "1                  0.025213               0.011963                 0.012386   \n",
      "2                  0.006552               0.008015                 0.012167   \n",
      "3                  0.022339               0.026808                 0.011321   \n",
      "4                  0.009601               0.011939                 0.014467   \n",
      "5                  0.008766               0.005407                 0.004496   \n",
      "6                  0.009778                0.00534                 0.008683   \n",
      "7                  0.003619               0.002804                 0.004674   \n",
      "8                  0.017015               0.014926                 0.013957   \n",
      "9                  0.006307                0.00168                 0.006508   \n",
      "10                   0.0116               0.012141                 0.012559   \n",
      "11                 0.014195                0.01873                 0.008214   \n",
      "12                 0.003055               0.005476                 0.003788   \n",
      "13                  0.00773               0.005895                 0.007351   \n",
      "14                 0.003255               0.001469                 0.001839   \n",
      "15                 0.002917               0.003034                  0.00286   \n",
      "16                 0.007936               0.008845                 0.048985   \n",
      "17                 0.006233               0.008149                 0.003321   \n",
      "18                 0.001586               0.003087                 0.001591   \n",
      "19                 0.007175               0.002596                 0.001872   \n",
      "20                 0.000817                0.00218                 0.003126   \n",
      "21                   0.0222               0.009061                 0.014402   \n",
      "\n",
      "   SWK_Relative_Importance CBRE_Relative_Importance  \\\n",
      "0                 0.005983                 0.003583   \n",
      "1                 0.009777                 0.009906   \n",
      "2                 0.009965                 0.007296   \n",
      "3                 0.015787                 0.028826   \n",
      "4                 0.007772                 0.003479   \n",
      "5                  0.02048                 0.006546   \n",
      "6                 0.013923                 0.009418   \n",
      "7                 0.005378                  0.00516   \n",
      "8                 0.010682                 0.006405   \n",
      "9                 0.003958                  0.00176   \n",
      "10                0.007565                 0.006632   \n",
      "11                 0.01195                 0.021457   \n",
      "12                0.006744                 0.006373   \n",
      "13                0.010601                 0.005002   \n",
      "14                0.001812                 0.001001   \n",
      "15                0.006446                 0.001476   \n",
      "16                0.013411                  0.00549   \n",
      "17                0.008713                 0.004949   \n",
      "18                0.004844                 0.005498   \n",
      "19                0.001214                  0.00524   \n",
      "20                0.001266                 0.002084   \n",
      "21                0.016185                 0.011431   \n",
      "\n",
      "    Average_relative_Importance  \n",
      "0                      0.008014  \n",
      "1                      0.015703  \n",
      "2                      0.008811  \n",
      "3                      0.026912  \n",
      "4                      0.010213  \n",
      "5                      0.008111  \n",
      "6                      0.009717  \n",
      "7                      0.003717  \n",
      "8                      0.014506  \n",
      "9                      0.004263  \n",
      "10                     0.010218  \n",
      "11                     0.015137  \n",
      "12                     0.005560  \n",
      "13                     0.006884  \n",
      "14                     0.002855  \n",
      "15                     0.004210  \n",
      "16                     0.014472  \n",
      "17                     0.006081  \n",
      "18                     0.002951  \n",
      "19                     0.002926  \n",
      "20                     0.001869  \n",
      "21                     0.014660  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>DIS_Relative_Importance</th>\n",
       "      <th>BIIB_Relative_Importance</th>\n",
       "      <th>GILD_Relative_Importance</th>\n",
       "      <th>ETR_Relative_Importance</th>\n",
       "      <th>PEP_Relative_Importance</th>\n",
       "      <th>NVDA_Relative_Importance</th>\n",
       "      <th>NI_Relative_Importance</th>\n",
       "      <th>BBWI_Relative_Importance</th>\n",
       "      <th>SWK_Relative_Importance</th>\n",
       "      <th>CBRE_Relative_Importance</th>\n",
       "      <th>Average_relative_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bop</td>\n",
       "      <td>0.017155</td>\n",
       "      <td>0.03182</td>\n",
       "      <td>0.013984</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.009906</td>\n",
       "      <td>0.015703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>close</td>\n",
       "      <td>0.03147</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>0.038689</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.035998</td>\n",
       "      <td>0.022339</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.026912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cti</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.012871</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.011946</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>0.011939</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.010213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ker</td>\n",
       "      <td>0.01682</td>\n",
       "      <td>0.029807</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>0.01289</td>\n",
       "      <td>0.01037</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.014926</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.010682</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.014506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mfi</td>\n",
       "      <td>0.01003</td>\n",
       "      <td>0.010346</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.012141</td>\n",
       "      <td>0.012559</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.010218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>momentum</td>\n",
       "      <td>0.022341</td>\n",
       "      <td>0.014543</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>0.013753</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.01873</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.01195</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.015137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>simple_moving_average</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.014224</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>0.048985</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.00549</td>\n",
       "      <td>0.014472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.012126</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.014402</td>\n",
       "      <td>0.016185</td>\n",
       "      <td>0.011431</td>\n",
       "      <td>0.014660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Features DIS_Relative_Importance BIIB_Relative_Importance  \\\n",
       "0                    bop                0.017155                  0.03182   \n",
       "1                  close                 0.03147                 0.013267   \n",
       "2                    cti                0.008852                 0.012871   \n",
       "3                    ker                 0.01682                 0.029807   \n",
       "4                    mfi                 0.01003                 0.010346   \n",
       "5               momentum                0.022341                 0.014543   \n",
       "6  simple_moving_average                0.009263                 0.006631   \n",
       "7                 volume                0.012126                 0.015747   \n",
       "\n",
       "  GILD_Relative_Importance ETR_Relative_Importance PEP_Relative_Importance  \\\n",
       "0                 0.013984                0.012446                0.012384   \n",
       "1                 0.038689                 0.04462                0.035998   \n",
       "2                 0.011369                0.011946                0.009832   \n",
       "3                 0.012188                 0.01289                 0.01037   \n",
       "4                 0.015033                0.006466                0.009808   \n",
       "5                 0.012173                0.013753                0.014018   \n",
       "6                 0.014224                0.017999                0.011933   \n",
       "7                 0.016717                0.013235                0.015496   \n",
       "\n",
       "  NVDA_Relative_Importance NI_Relative_Importance BBWI_Relative_Importance  \\\n",
       "0                 0.025213               0.011963                 0.012386   \n",
       "1                 0.022339               0.026808                 0.011321   \n",
       "2                 0.009601               0.011939                 0.014467   \n",
       "3                 0.017015               0.014926                 0.013957   \n",
       "4                   0.0116               0.012141                 0.012559   \n",
       "5                 0.014195                0.01873                 0.008214   \n",
       "6                 0.007936               0.008845                 0.048985   \n",
       "7                   0.0222               0.009061                 0.014402   \n",
       "\n",
       "  SWK_Relative_Importance CBRE_Relative_Importance  \\\n",
       "0                0.009777                 0.009906   \n",
       "1                0.015787                 0.028826   \n",
       "2                0.007772                 0.003479   \n",
       "3                0.010682                 0.006405   \n",
       "4                0.007565                 0.006632   \n",
       "5                 0.01195                 0.021457   \n",
       "6                0.013411                  0.00549   \n",
       "7                0.016185                 0.011431   \n",
       "\n",
       "   Average_relative_Importance  \n",
       "0                     0.015703  \n",
       "1                     0.026912  \n",
       "2                     0.010213  \n",
       "3                     0.014506  \n",
       "4                     0.010218  \n",
       "5                     0.015137  \n",
       "6                     0.014472  \n",
       "7                     0.014660  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2.head(2)\n",
    "feat_importance_all=pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "\n",
    "    matching_columns = [col for col in final_df2.columns if ticker in col]\n",
    "#     matching_columns.append([])\n",
    "    # # Print matching columns\n",
    "    # for col in matching_columns:\n",
    "    #     print(col)\n",
    "\n",
    "    temp_df=final_df2[matching_columns]\n",
    "    feat_importance_temp=check_var_importance(temp_df,ticker)\n",
    "    feat_importance_all=pd.concat([feat_importance_all,feat_importance_temp], axis=1)\n",
    "#     print(feat_importance_all)\n",
    "feat_importance_all=feat_importance_all.T.drop_duplicates(keep='first').T\n",
    "feat_importance_all[\"Average_relative_Importance\"]=feat_importance_all.iloc[:,1:].mean(axis=1)\n",
    "print(feat_importance_all)\n",
    "threshold = .01  # Example threshold value\n",
    "\n",
    "# Create a boolean mask based on the condition\n",
    "mask = feat_importance_all['Average_relative_Importance'] >= threshold\n",
    "\n",
    "# Apply the mask to the DataFrame\n",
    "filtered_df = feat_importance_all[mask]\n",
    "\n",
    "# df['Average_Columns'] = df[['Column1', 'Column2']].mean(axis=1)\n",
    "#     print(temp_df)\n",
    "\n",
    "filtered_df.reset_index(drop=True,inplace=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18289dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci</th>\n",
       "      <th>BBWI_stochrsi</th>\n",
       "      <th>BBWI_mfi</th>\n",
       "      <th>BBWI_bop</th>\n",
       "      <th>BBWI_supertrend_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>SWK_ker</th>\n",
       "      <th>SWK_momentum</th>\n",
       "      <th>SWK_simple_moving_average</th>\n",
       "      <th>SWK_bollinger_bands</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-18</th>\n",
       "      <td>0.022358</td>\n",
       "      <td>4008127</td>\n",
       "      <td>22.667746</td>\n",
       "      <td>23.435732</td>\n",
       "      <td>22.554567</td>\n",
       "      <td>64.492789</td>\n",
       "      <td>97.163927</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-14.764028</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807868</td>\n",
       "      <td>1.12761</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>4.99</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.034226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-19</th>\n",
       "      <td>0.001411</td>\n",
       "      <td>4918559</td>\n",
       "      <td>22.441389</td>\n",
       "      <td>23.039612</td>\n",
       "      <td>22.384802</td>\n",
       "      <td>32.172596</td>\n",
       "      <td>98.183420</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-19.501164</td>\n",
       "      <td>61.163117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422496</td>\n",
       "      <td>1.12761</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.029103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "Date                                                                   \n",
       "2007-01-18    0.022358      4008127  22.667746  23.435732  22.554567   \n",
       "2007-01-19    0.001411      4918559  22.441389  23.039612  22.384802   \n",
       "\n",
       "             BBWI_cci  BBWI_stochrsi  BBWI_mfi   BBWI_bop  BBWI_supertrend_ub  \\\n",
       "Date                                                                            \n",
       "2007-01-18  64.492789      97.163927       0.5 -14.764028           61.163117   \n",
       "2007-01-19  32.172596      98.183420       0.5 -19.501164           61.163117   \n",
       "\n",
       "            ...   SWK_ker  SWK_momentum  SWK_simple_moving_average  \\\n",
       "Date        ...                                                      \n",
       "2007-01-18  ...  0.807868       1.12761                   0.074882   \n",
       "2007-01-19  ...  0.422496       1.12761                   0.074882   \n",
       "\n",
       "            SWK_bollinger_bands  10y2y_spread  10y3m_spread  3m_rate  ltiit  \\\n",
       "Date                                                                          \n",
       "2007-01-18             1.098085         -0.14         -0.37     4.99   2.41   \n",
       "2007-01-19             1.098085         -0.15         -0.36     5.01   2.42   \n",
       "\n",
       "            ted_spread   var_wti  \n",
       "Date                              \n",
       "2007-01-18        0.37 -0.034226  \n",
       "2007-01-19        0.35  0.029103  \n",
       "\n",
       "[2 rows x 226 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e446bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIS_bop', 'BIIB_bop', 'GILD_bop', 'ETR_bop', 'PEP_bop', 'NVDA_bop', 'NI_bop', 'BBWI_bop', 'SWK_bop', 'CBRE_bop', 'DIS_close', 'BIIB_close', 'GILD_close', 'ETR_close', 'PEP_close', 'NVDA_close', 'NI_close', 'BBWI_close', 'SWK_close', 'CBRE_close', 'DIS_cti', 'BIIB_cti', 'GILD_cti', 'ETR_cti', 'PEP_cti', 'NVDA_cti', 'NI_cti', 'BBWI_cti', 'SWK_cti', 'CBRE_cti', 'DIS_ker', 'BIIB_ker', 'GILD_ker', 'ETR_ker', 'PEP_ker', 'NVDA_ker', 'NI_ker', 'BBWI_ker', 'SWK_ker', 'CBRE_ker', 'DIS_mfi', 'BIIB_mfi', 'GILD_mfi', 'ETR_mfi', 'PEP_mfi', 'NVDA_mfi', 'NI_mfi', 'BBWI_mfi', 'SWK_mfi', 'CBRE_mfi', 'DIS_momentum', 'BIIB_momentum', 'GILD_momentum', 'ETR_momentum', 'PEP_momentum', 'NVDA_momentum', 'NI_momentum', 'BBWI_momentum', 'SWK_momentum', 'CBRE_momentum', 'DIS_simple_moving_average', 'BIIB_simple_moving_average', 'GILD_simple_moving_average', 'ETR_simple_moving_average', 'PEP_simple_moving_average', 'NVDA_simple_moving_average', 'NI_simple_moving_average', 'BBWI_simple_moving_average', 'SWK_simple_moving_average', 'CBRE_simple_moving_average', 'DIS_volume', 'BIIB_volume', 'GILD_volume', 'ETR_volume', 'PEP_volume', 'NVDA_volume', 'NI_volume', 'BBWI_volume', 'SWK_volume', 'CBRE_volume']\n"
     ]
    }
   ],
   "source": [
    "imp_col_names=[filtered_df['Features'].values][0].tolist()\n",
    "\n",
    "# imp_col_names = [ticker +\"_\"+ element for element in imp_col_names]\n",
    "final_col_names = []\n",
    "for col in imp_col_names:\n",
    "    for ticker in tickers:\n",
    "        final_col_names.append(f'{ticker}_{col}')\n",
    "\n",
    "print(final_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7143b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_col_names=final_col_names+['10y2y_spread','10y3m_spread','3m_rate','ltiit','ted_spread','var_wti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38092fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIS_bop',\n",
       " 'BIIB_bop',\n",
       " 'GILD_bop',\n",
       " 'ETR_bop',\n",
       " 'PEP_bop',\n",
       " 'NVDA_bop',\n",
       " 'NI_bop',\n",
       " 'BBWI_bop',\n",
       " 'SWK_bop',\n",
       " 'CBRE_bop',\n",
       " 'DIS_close',\n",
       " 'BIIB_close',\n",
       " 'GILD_close',\n",
       " 'ETR_close',\n",
       " 'PEP_close',\n",
       " 'NVDA_close',\n",
       " 'NI_close',\n",
       " 'BBWI_close',\n",
       " 'SWK_close',\n",
       " 'CBRE_close',\n",
       " 'DIS_cti',\n",
       " 'BIIB_cti',\n",
       " 'GILD_cti',\n",
       " 'ETR_cti',\n",
       " 'PEP_cti',\n",
       " 'NVDA_cti',\n",
       " 'NI_cti',\n",
       " 'BBWI_cti',\n",
       " 'SWK_cti',\n",
       " 'CBRE_cti',\n",
       " 'DIS_ker',\n",
       " 'BIIB_ker',\n",
       " 'GILD_ker',\n",
       " 'ETR_ker',\n",
       " 'PEP_ker',\n",
       " 'NVDA_ker',\n",
       " 'NI_ker',\n",
       " 'BBWI_ker',\n",
       " 'SWK_ker',\n",
       " 'CBRE_ker',\n",
       " 'DIS_mfi',\n",
       " 'BIIB_mfi',\n",
       " 'GILD_mfi',\n",
       " 'ETR_mfi',\n",
       " 'PEP_mfi',\n",
       " 'NVDA_mfi',\n",
       " 'NI_mfi',\n",
       " 'BBWI_mfi',\n",
       " 'SWK_mfi',\n",
       " 'CBRE_mfi',\n",
       " 'DIS_momentum',\n",
       " 'BIIB_momentum',\n",
       " 'GILD_momentum',\n",
       " 'ETR_momentum',\n",
       " 'PEP_momentum',\n",
       " 'NVDA_momentum',\n",
       " 'NI_momentum',\n",
       " 'BBWI_momentum',\n",
       " 'SWK_momentum',\n",
       " 'CBRE_momentum',\n",
       " 'DIS_simple_moving_average',\n",
       " 'BIIB_simple_moving_average',\n",
       " 'GILD_simple_moving_average',\n",
       " 'ETR_simple_moving_average',\n",
       " 'PEP_simple_moving_average',\n",
       " 'NVDA_simple_moving_average',\n",
       " 'NI_simple_moving_average',\n",
       " 'BBWI_simple_moving_average',\n",
       " 'SWK_simple_moving_average',\n",
       " 'CBRE_simple_moving_average',\n",
       " 'DIS_volume',\n",
       " 'BIIB_volume',\n",
       " 'GILD_volume',\n",
       " 'ETR_volume',\n",
       " 'PEP_volume',\n",
       " 'NVDA_volume',\n",
       " 'NI_volume',\n",
       " 'BBWI_volume',\n",
       " 'SWK_volume',\n",
       " 'CBRE_volume',\n",
       " '10y2y_spread',\n",
       " '10y3m_spread',\n",
       " '3m_rate',\n",
       " 'ltiit',\n",
       " 'ted_spread',\n",
       " 'var_wti']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d5903d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIS_bop</th>\n",
       "      <th>BIIB_bop</th>\n",
       "      <th>GILD_bop</th>\n",
       "      <th>ETR_bop</th>\n",
       "      <th>PEP_bop</th>\n",
       "      <th>NVDA_bop</th>\n",
       "      <th>NI_bop</th>\n",
       "      <th>BBWI_bop</th>\n",
       "      <th>SWK_bop</th>\n",
       "      <th>CBRE_bop</th>\n",
       "      <th>...</th>\n",
       "      <th>NI_volume</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>SWK_volume</th>\n",
       "      <th>CBRE_volume</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-18</th>\n",
       "      <td>-7.662506</td>\n",
       "      <td>-0.367923</td>\n",
       "      <td>-9.698914</td>\n",
       "      <td>-68.917441</td>\n",
       "      <td>-18.312857</td>\n",
       "      <td>-1.731363</td>\n",
       "      <td>-38.002019</td>\n",
       "      <td>-14.764028</td>\n",
       "      <td>-7.766762</td>\n",
       "      <td>-0.503762</td>\n",
       "      <td>...</td>\n",
       "      <td>3706284</td>\n",
       "      <td>4008127</td>\n",
       "      <td>918600</td>\n",
       "      <td>1363900</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>4.99</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.034226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-19</th>\n",
       "      <td>-12.518908</td>\n",
       "      <td>0.476192</td>\n",
       "      <td>-12.176348</td>\n",
       "      <td>-42.831266</td>\n",
       "      <td>-73.043117</td>\n",
       "      <td>-1.491105</td>\n",
       "      <td>-39.868729</td>\n",
       "      <td>-19.501164</td>\n",
       "      <td>-20.102638</td>\n",
       "      <td>-0.661287</td>\n",
       "      <td>...</td>\n",
       "      <td>3921845</td>\n",
       "      <td>4918559</td>\n",
       "      <td>1044400</td>\n",
       "      <td>1318400</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.029103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-22</th>\n",
       "      <td>-13.832666</td>\n",
       "      <td>-0.926830</td>\n",
       "      <td>-10.789761</td>\n",
       "      <td>-28.542541</td>\n",
       "      <td>-41.459803</td>\n",
       "      <td>-3.119636</td>\n",
       "      <td>-41.165056</td>\n",
       "      <td>-20.583048</td>\n",
       "      <td>-20.188575</td>\n",
       "      <td>0.216669</td>\n",
       "      <td>...</td>\n",
       "      <td>3585905</td>\n",
       "      <td>4623906</td>\n",
       "      <td>571700</td>\n",
       "      <td>1243300</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.016737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-23</th>\n",
       "      <td>-8.058034</td>\n",
       "      <td>-0.612716</td>\n",
       "      <td>-13.363528</td>\n",
       "      <td>-68.481285</td>\n",
       "      <td>-53.310011</td>\n",
       "      <td>-1.962929</td>\n",
       "      <td>-25.558618</td>\n",
       "      <td>-29.375478</td>\n",
       "      <td>-25.374396</td>\n",
       "      <td>0.593408</td>\n",
       "      <td>...</td>\n",
       "      <td>2448290</td>\n",
       "      <td>4078018</td>\n",
       "      <td>481400</td>\n",
       "      <td>1668600</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.048914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-24</th>\n",
       "      <td>-12.312823</td>\n",
       "      <td>-0.619717</td>\n",
       "      <td>-17.002254</td>\n",
       "      <td>-69.956131</td>\n",
       "      <td>-34.756480</td>\n",
       "      <td>-1.840533</td>\n",
       "      <td>-34.495847</td>\n",
       "      <td>-36.963312</td>\n",
       "      <td>-20.766541</td>\n",
       "      <td>0.661537</td>\n",
       "      <td>...</td>\n",
       "      <td>2247744</td>\n",
       "      <td>3377257</td>\n",
       "      <td>436800</td>\n",
       "      <td>2947300</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.011752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12</th>\n",
       "      <td>0.010308</td>\n",
       "      <td>-0.238938</td>\n",
       "      <td>-3.179339</td>\n",
       "      <td>-2.074620</td>\n",
       "      <td>-2.045910</td>\n",
       "      <td>-0.360463</td>\n",
       "      <td>-2.009898</td>\n",
       "      <td>-0.716619</td>\n",
       "      <td>-1.373918</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>...</td>\n",
       "      <td>2643700</td>\n",
       "      <td>3131200</td>\n",
       "      <td>1738700</td>\n",
       "      <td>2029500</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-13</th>\n",
       "      <td>0.366072</td>\n",
       "      <td>0.589164</td>\n",
       "      <td>-1.616124</td>\n",
       "      <td>-3.021859</td>\n",
       "      <td>-1.174616</td>\n",
       "      <td>0.615635</td>\n",
       "      <td>-1.829128</td>\n",
       "      <td>0.156574</td>\n",
       "      <td>-0.471402</td>\n",
       "      <td>-0.047060</td>\n",
       "      <td>...</td>\n",
       "      <td>2505100</td>\n",
       "      <td>3244200</td>\n",
       "      <td>1380200</td>\n",
       "      <td>2285100</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-14</th>\n",
       "      <td>0.198891</td>\n",
       "      <td>0.269312</td>\n",
       "      <td>-3.690008</td>\n",
       "      <td>-1.284978</td>\n",
       "      <td>-0.856431</td>\n",
       "      <td>0.335624</td>\n",
       "      <td>-0.927322</td>\n",
       "      <td>-1.324553</td>\n",
       "      <td>-1.868440</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>...</td>\n",
       "      <td>4172800</td>\n",
       "      <td>4278500</td>\n",
       "      <td>1071300</td>\n",
       "      <td>1995200</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>0.874042</td>\n",
       "      <td>0.454293</td>\n",
       "      <td>-2.022904</td>\n",
       "      <td>-2.724536</td>\n",
       "      <td>-2.026871</td>\n",
       "      <td>0.263161</td>\n",
       "      <td>-2.482120</td>\n",
       "      <td>0.220931</td>\n",
       "      <td>-2.093828</td>\n",
       "      <td>0.832001</td>\n",
       "      <td>...</td>\n",
       "      <td>3497500</td>\n",
       "      <td>4123700</td>\n",
       "      <td>848200</td>\n",
       "      <td>1831500</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>-0.336737</td>\n",
       "      <td>-0.562721</td>\n",
       "      <td>-2.003491</td>\n",
       "      <td>-4.383414</td>\n",
       "      <td>-2.493611</td>\n",
       "      <td>-0.240918</td>\n",
       "      <td>-3.189894</td>\n",
       "      <td>-0.162671</td>\n",
       "      <td>-2.032164</td>\n",
       "      <td>-0.459118</td>\n",
       "      <td>...</td>\n",
       "      <td>6041000</td>\n",
       "      <td>5343900</td>\n",
       "      <td>937500</td>\n",
       "      <td>1907700</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3902 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DIS_bop  BIIB_bop   GILD_bop    ETR_bop    PEP_bop  NVDA_bop  \\\n",
       "Date                                                                         \n",
       "2007-01-18  -7.662506 -0.367923  -9.698914 -68.917441 -18.312857 -1.731363   \n",
       "2007-01-19 -12.518908  0.476192 -12.176348 -42.831266 -73.043117 -1.491105   \n",
       "2007-01-22 -13.832666 -0.926830 -10.789761 -28.542541 -41.459803 -3.119636   \n",
       "2007-01-23  -8.058034 -0.612716 -13.363528 -68.481285 -53.310011 -1.962929   \n",
       "2007-01-24 -12.312823 -0.619717 -17.002254 -69.956131 -34.756480 -1.840533   \n",
       "...               ...       ...        ...        ...        ...       ...   \n",
       "2022-07-12   0.010308 -0.238938  -3.179339  -2.074620  -2.045910 -0.360463   \n",
       "2022-07-13   0.366072  0.589164  -1.616124  -3.021859  -1.174616  0.615635   \n",
       "2022-07-14   0.198891  0.269312  -3.690008  -1.284978  -0.856431  0.335624   \n",
       "2022-07-15   0.874042  0.454293  -2.022904  -2.724536  -2.026871  0.263161   \n",
       "2022-07-18  -0.336737 -0.562721  -2.003491  -4.383414  -2.493611 -0.240918   \n",
       "\n",
       "               NI_bop   BBWI_bop    SWK_bop  CBRE_bop  ...  NI_volume  \\\n",
       "Date                                                   ...              \n",
       "2007-01-18 -38.002019 -14.764028  -7.766762 -0.503762  ...    3706284   \n",
       "2007-01-19 -39.868729 -19.501164 -20.102638 -0.661287  ...    3921845   \n",
       "2007-01-22 -41.165056 -20.583048 -20.188575  0.216669  ...    3585905   \n",
       "2007-01-23 -25.558618 -29.375478 -25.374396  0.593408  ...    2448290   \n",
       "2007-01-24 -34.495847 -36.963312 -20.766541  0.661537  ...    2247744   \n",
       "...               ...        ...        ...       ...  ...        ...   \n",
       "2022-07-12  -2.009898  -0.716619  -1.373918  0.641791  ...    2643700   \n",
       "2022-07-13  -1.829128   0.156574  -0.471402 -0.047060  ...    2505100   \n",
       "2022-07-14  -0.927322  -1.324553  -1.868440  0.836601  ...    4172800   \n",
       "2022-07-15  -2.482120   0.220931  -2.093828  0.832001  ...    3497500   \n",
       "2022-07-18  -3.189894  -0.162671  -2.032164 -0.459118  ...    6041000   \n",
       "\n",
       "            BBWI_volume  SWK_volume  CBRE_volume  10y2y_spread  10y3m_spread  \\\n",
       "Date                                                                           \n",
       "2007-01-18      4008127      918600      1363900         -0.14         -0.37   \n",
       "2007-01-19      4918559     1044400      1318400         -0.15         -0.36   \n",
       "2007-01-22      4623906      571700      1243300         -0.15         -0.37   \n",
       "2007-01-23      4078018      481400      1668600         -0.13         -0.33   \n",
       "2007-01-24      3377257      436800      2947300         -0.12         -0.32   \n",
       "...                 ...         ...          ...           ...           ...   \n",
       "2022-07-12      3131200     1738700      2029500         -0.07          0.74   \n",
       "2022-07-13      3244200     1380200      2285100         -0.22          0.52   \n",
       "2022-07-14      4278500     1071300      1995200         -0.19          0.56   \n",
       "2022-07-15      4123700      848200      1831500         -0.20          0.56   \n",
       "2022-07-18      5343900      937500      1907700         -0.19          0.46   \n",
       "\n",
       "            3m_rate  ltiit  ted_spread   var_wti  \n",
       "Date                                              \n",
       "2007-01-18     4.99   2.41        0.37 -0.034226  \n",
       "2007-01-19     5.01   2.42        0.35  0.029103  \n",
       "2007-01-22     5.00   2.39        0.36 -0.016737  \n",
       "2007-01-23     5.01   2.41        0.35  0.048914  \n",
       "2007-01-24     5.00   2.39        0.36  0.011752  \n",
       "...             ...    ...         ...       ...  \n",
       "2022-07-12     2.16   1.12         NaN  0.000000  \n",
       "2022-07-13     2.33   1.06         NaN  0.000000  \n",
       "2022-07-14     2.33   1.07         NaN  0.000000  \n",
       "2022-07-15     2.29   1.03         NaN  0.000000  \n",
       "2022-07-18      NaN    NaN         NaN  0.000000  \n",
       "\n",
       "[3902 rows x 86 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data=final_df2[final_col_names]\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df01faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    \"\"\"This class takes time series data that is in a sequential format, transforming\n",
    "    it into pairs of inputs and labels, so that the inputs are windows of consecutive\n",
    "    samples from the data.\n",
    "    \"\"\"\n",
    "    def __init__(self,input_width=5,label_width=1,shift=1, train_df=None, val_df=None,\n",
    "                 test_df=None, label_columns=None,batch_size=None,shuffle=False):\n",
    "        \"\"\"This method initiates the WindowGenerator class.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "        input_width (int, default=5): the width of the window, which represents the \n",
    "            amount of time steps from the earliest input observation to the last.\n",
    "        label_width (int, default=1): the width of the label. This determines the amount\n",
    "             of time steps that will be predicted.\n",
    "        shift (int, default=1): jump between the last input in the window and the first \n",
    "            label.\n",
    "        train_df (pandas Dataframe, default=None): array-like object containing the train \n",
    "            data which comes in a time series format.\n",
    "        val_df (pandas Dataframe, default=None): array-like object containing the \n",
    "            validation data.\n",
    "        test_df (pandas Dataframe, default=None): array-like object containing the test \n",
    "            data.\n",
    "        label_columns (list|string, default=None): name of the column(s) that are used \n",
    "            as labels.\n",
    "        batch_size (int, deafault=None): the size of the batches of the tf.data.Dataset\n",
    "            object (whose dimensions are (batch,input_width,features) for the input and\n",
    "            (batch,label_width,label_columns) for the labels).\n",
    "        shuffle (boolean, default=False): determines if the data inside the tf.data.Dataset\n",
    "            is shuffled.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Define attributes of the class:\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.label_columns = label_columns\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Define information about columns:\n",
    "        if isinstance(label_columns,type(None)):\n",
    "            self.label_columns_indices = {name:i for i,name in enumerate(label_columns)}\n",
    "        self.column_indices = {name:i for i,name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Define window information:\n",
    "        self.total_window_size = input_width+shift\n",
    "        self.input_slice = slice(0,input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size-self.label_width\n",
    "        self.labels_slice = slice(self.label_start,None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"This method determines what is returned when an instance of the object\n",
    "        is called\n",
    "        \"\"\"\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'\n",
    "        ])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        \"\"\"This method converts a list of consecutive inputs to a window of\n",
    "        inputs and a window of labels.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "        features (pandas Dataset): features in the dataframe\n",
    "\n",
    "        Outputs:\n",
    "        --------\n",
    "        inputs ()\n",
    "        \"\"\"\n",
    "        inputs = features[:, self.input_slice,:]\n",
    "        labels = features[:,self.labels_slice,:]\n",
    "        if not isinstance(self.label_columns,type(None)):\n",
    "            labels = tf.stack(\n",
    "                [labels[:,:,self.column_indices[name]] for name in self.label_columns],\n",
    "                axis = -1\n",
    "            )\n",
    "        \n",
    "        # Set the shapes of the informaiton:\n",
    "        inputs.set_shape([None,self.input_width,None])\n",
    "        labels.set_shape([None,self.label_width,None])\n",
    "\n",
    "        return inputs,labels\n",
    "    \n",
    "    def make_dataset(self,data):\n",
    "        \"\"\"This method takes a time series DataFrame and convert it to a \n",
    "        tf.data.Dataset of (input_window,label_window) pairs, using the\n",
    "        tf.keras.preprocessing.timeseries_dataset_from_array function.\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "        data (pandas DataFrame): dataframe containing the time series information\n",
    "            of the inputs and labels, which will transformed into windows and then \n",
    "            a tf.Dataset object.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data = data,\n",
    "            targets = None,\n",
    "            sequence_length = self.total_window_size,\n",
    "            sequence_stride = 1,\n",
    "            shuffle = self.shuffle,\n",
    "            batch_size = self.batch_size\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        print(\"************************************************\")\n",
    "        print(ds)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    # Adding properties for accessing the train, val and test as tf.data.Dataset objects\n",
    "    @property\n",
    "    def train(self):\n",
    "        if isinstance(self.train_df,type(None)):\n",
    "            return None\n",
    "        else:\n",
    "            return self.make_dataset(self.train_df)\n",
    "        print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")    \n",
    "        print(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        if isinstance(self.val_df,type(None)):\n",
    "            return None\n",
    "        else:\n",
    "            return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        if isinstance(self.test_df,type(None)):\n",
    "            return None\n",
    "        else:\n",
    "            return self.make_dataset(self.test_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "210b0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIS_close', 'BIIB_close', 'GILD_close', 'ETR_close', 'PEP_close', 'NVDA_close', 'NI_close', 'BBWI_close', 'SWK_close', 'CBRE_close']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/mxqr91nj7135zkb7ptp3mq5c0000gn/T/ipykernel_12380/3367714174.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_data.fillna(method='ffill',inplace=True)\n",
      "/var/folders/dk/mxqr91nj7135zkb7ptp3mq5c0000gn/T/ipykernel_12380/3367714174.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_data.fillna(method='bfill',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Define the label columns:\n",
    "\n",
    "# total_df=final_df2.copy()\n",
    "# total_df.set_index('Date',inplace=True)\n",
    "# label_cols = list(total_df.columns)\n",
    "# label_cols=label_cols[:len(tickers)]\n",
    "\n",
    "\n",
    "\n",
    "# matching_columns = [col for col in df.columns if \"close\" in col]\n",
    "model_data.fillna(method='ffill',inplace=True)\n",
    "model_data.fillna(method='bfill',inplace=True)\n",
    "\n",
    "model_data\n",
    "    \n",
    "label_cols=[col for col in model_data.columns if \"close\" in col]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ['AAPL_returns', 'GOOGL_returns', 'MSFT_returns', 'SPY_returns']\n",
    "print(label_cols)\n",
    "\n",
    "# Define train (70%), val (20%) and test (10%) dataframes: \n",
    "train_p, val_p, test_p = 0.7,0.2,0.1\n",
    "window_size = 5\n",
    "num_features = model_data.shape[1]\n",
    "total_size = len(model_data)\n",
    "train_size = int(total_size*train_p)\n",
    "val_size = int(total_size*val_p)\n",
    "test_size = int(total_size*test_p)\n",
    "train_df = model_data.iloc[:train_size,:]\n",
    "val_df = model_data.iloc[train_size-window_size:train_size+val_size,:]\n",
    "test_df = model_data.iloc[train_size+val_size-window_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d6d6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DIS_bop', 'BIIB_bop', 'GILD_bop', 'ETR_bop', 'PEP_bop', 'NVDA_bop',\n",
       "       'NI_bop', 'BBWI_bop', 'SWK_bop', 'CBRE_bop', 'DIS_close', 'BIIB_close',\n",
       "       'GILD_close', 'ETR_close', 'PEP_close', 'NVDA_close', 'NI_close',\n",
       "       'BBWI_close', 'SWK_close', 'CBRE_close', 'DIS_cti', 'BIIB_cti',\n",
       "       'GILD_cti', 'ETR_cti', 'PEP_cti', 'NVDA_cti', 'NI_cti', 'BBWI_cti',\n",
       "       'SWK_cti', 'CBRE_cti', 'DIS_ker', 'BIIB_ker', 'GILD_ker', 'ETR_ker',\n",
       "       'PEP_ker', 'NVDA_ker', 'NI_ker', 'BBWI_ker', 'SWK_ker', 'CBRE_ker',\n",
       "       'DIS_mfi', 'BIIB_mfi', 'GILD_mfi', 'ETR_mfi', 'PEP_mfi', 'NVDA_mfi',\n",
       "       'NI_mfi', 'BBWI_mfi', 'SWK_mfi', 'CBRE_mfi', 'DIS_momentum',\n",
       "       'BIIB_momentum', 'GILD_momentum', 'ETR_momentum', 'PEP_momentum',\n",
       "       'NVDA_momentum', 'NI_momentum', 'BBWI_momentum', 'SWK_momentum',\n",
       "       'CBRE_momentum', 'DIS_simple_moving_average',\n",
       "       'BIIB_simple_moving_average', 'GILD_simple_moving_average',\n",
       "       'ETR_simple_moving_average', 'PEP_simple_moving_average',\n",
       "       'NVDA_simple_moving_average', 'NI_simple_moving_average',\n",
       "       'BBWI_simple_moving_average', 'SWK_simple_moving_average',\n",
       "       'CBRE_simple_moving_average', 'DIS_volume', 'BIIB_volume',\n",
       "       'GILD_volume', 'ETR_volume', 'PEP_volume', 'NVDA_volume', 'NI_volume',\n",
       "       'BBWI_volume', 'SWK_volume', 'CBRE_volume', '10y2y_spread',\n",
       "       '10y3m_spread', '3m_rate', 'ltiit', 'ted_spread', 'var_wti'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2af088d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total window size: 6\n",
      "Input indices: [0 1 2 3 4]\n",
      "Label indices: [5]\n",
      "Label column name(s): ['DIS_close', 'BIIB_close', 'GILD_close', 'ETR_close', 'PEP_close', 'NVDA_close', 'NI_close', 'BBWI_close', 'SWK_close', 'CBRE_close']\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Train input shape: (512, 5, 86)\n",
      "Train target shape: (512, 1, 10)\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Validation input shape: (512, 5, 86)\n",
      "Validation target shape: (512, 1, 10)\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Test input shape: (391, 5, 86)\n",
      "Test target shape: (391, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size:\n",
    "batch_size = 512\n",
    "\n",
    "# Create an instance of the WindowGenerator object:\n",
    "my_window = WindowGenerator(input_width=window_size,label_width=1,shift=1,train_df=train_df,val_df=val_df,\n",
    "                            test_df=test_df,label_columns=label_cols,\n",
    "                            batch_size=batch_size,shuffle=True)\n",
    "print(my_window)\n",
    "\n",
    "# Print the shapes for one batch of each sub dataset:\n",
    "for example_inputs, example_labels in my_window.train.take(1):\n",
    "    print(\"Train input shape:\",example_inputs.shape)\n",
    "    print(\"Train target shape:\",example_labels.shape)\n",
    "for example_inputs, example_labels in my_window.val.take(1):\n",
    "    print(\"Validation input shape:\",example_inputs.shape)\n",
    "    print(\"Validation target shape:\",example_labels.shape)\n",
    "for example_inputs, example_labels in my_window.test.take(1):\n",
    "    print(\"Test input shape:\",example_inputs.shape)\n",
    "    print(\"Test target shape:\",example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5edca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def initial_eda(dataset):\n",
    "#     for batch in dataset.take(1):  # Assuming you want to analyze the first batch\n",
    "#         print(\"Dimensions:\", batch[0].shape[0], \"samples,\", batch[0].shape[1], \"features\")\n",
    "#         print(\"Labels shape:\", batch[1].shape)\n",
    "#         print(\"Labels sample:\", batch[1].numpy()[:5])  # Printing first 5 labels as an example\n",
    "#         print(\"Features sample:\", batch[0].numpy()[:5])\n",
    "#         print(\"dataframe of the features\")\n",
    "#         print(pd.DataFrame(batch[0].numpy()[:5]))\n",
    "# # Assuming my_window.train is your TensorFlow dataset\n",
    "# initial_eda(my_window.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac977e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c14a5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0016 - root_mean_squared_error: 0.0574 - mae: 0.0424\n",
      "Epoch 1: val_loss improved from inf to 0.00285, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 603ms/step - loss: 0.0016 - root_mean_squared_error: 0.0574 - mae: 0.0424 - val_loss: 0.0028 - val_root_mean_squared_error: 0.0756 - val_mae: 0.0604\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.2323e-04 - root_mean_squared_error: 0.0352 - mae: 0.0263\n",
      "Epoch 2: val_loss improved from 0.00285 to 0.00118, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 511ms/step - loss: 6.2323e-04 - root_mean_squared_error: 0.0352 - mae: 0.0263 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0483 - val_mae: 0.0384\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.4441e-04 - root_mean_squared_error: 0.0299 - mae: 0.0216\n",
      "Epoch 3: val_loss improved from 0.00118 to 0.00084, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 546ms/step - loss: 4.4441e-04 - root_mean_squared_error: 0.0299 - mae: 0.0216 - val_loss: 8.4375e-04 - val_root_mean_squared_error: 0.0413 - val_mae: 0.0313\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.6122e-04 - root_mean_squared_error: 0.0268 - mae: 0.0188\n",
      "Epoch 4: val_loss improved from 0.00084 to 0.00062, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 547ms/step - loss: 3.6122e-04 - root_mean_squared_error: 0.0268 - mae: 0.0188 - val_loss: 6.2064e-04 - val_root_mean_squared_error: 0.0349 - val_mae: 0.0256\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2224e-04 - root_mean_squared_error: 0.0254 - mae: 0.0172\n",
      "Epoch 5: val_loss improved from 0.00062 to 0.00051, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 536ms/step - loss: 3.2224e-04 - root_mean_squared_error: 0.0254 - mae: 0.0172 - val_loss: 5.0940e-04 - val_root_mean_squared_error: 0.0321 - val_mae: 0.0221\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.9938e-04 - root_mean_squared_error: 0.0245 - mae: 0.0162\n",
      "Epoch 6: val_loss improved from 0.00051 to 0.00047, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 569ms/step - loss: 2.9938e-04 - root_mean_squared_error: 0.0245 - mae: 0.0162 - val_loss: 4.7029e-04 - val_root_mean_squared_error: 0.0307 - val_mae: 0.0207\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.8659e-04 - root_mean_squared_error: 0.0240 - mae: 0.0155\n",
      "Epoch 7: val_loss improved from 0.00047 to 0.00043, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 540ms/step - loss: 2.8659e-04 - root_mean_squared_error: 0.0240 - mae: 0.0155 - val_loss: 4.3468e-04 - val_root_mean_squared_error: 0.0293 - val_mae: 0.0196\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.7926e-04 - root_mean_squared_error: 0.0237 - mae: 0.0152\n",
      "Epoch 8: val_loss improved from 0.00043 to 0.00043, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 2.7926e-04 - root_mean_squared_error: 0.0237 - mae: 0.0152 - val_loss: 4.2673e-04 - val_root_mean_squared_error: 0.0289 - val_mae: 0.0191\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.7444e-04 - root_mean_squared_error: 0.0235 - mae: 0.0149\n",
      "Epoch 9: val_loss improved from 0.00043 to 0.00041, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 517ms/step - loss: 2.7444e-04 - root_mean_squared_error: 0.0235 - mae: 0.0149 - val_loss: 4.1494e-04 - val_root_mean_squared_error: 0.0290 - val_mae: 0.0189\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.7102e-04 - root_mean_squared_error: 0.0234 - mae: 0.0147\n",
      "Epoch 10: val_loss improved from 0.00041 to 0.00041, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 583ms/step - loss: 2.7102e-04 - root_mean_squared_error: 0.0234 - mae: 0.0147 - val_loss: 4.1055e-04 - val_root_mean_squared_error: 0.0277 - val_mae: 0.0181\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6860e-04 - root_mean_squared_error: 0.0232 - mae: 0.0146\n",
      "Epoch 11: val_loss improved from 0.00041 to 0.00040, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 571ms/step - loss: 2.6860e-04 - root_mean_squared_error: 0.0232 - mae: 0.0146 - val_loss: 3.9912e-04 - val_root_mean_squared_error: 0.0281 - val_mae: 0.0180\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6721e-04 - root_mean_squared_error: 0.0231 - mae: 0.0144\n",
      "Epoch 12: val_loss improved from 0.00040 to 0.00040, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 534ms/step - loss: 2.6721e-04 - root_mean_squared_error: 0.0231 - mae: 0.0144 - val_loss: 3.9858e-04 - val_root_mean_squared_error: 0.0282 - val_mae: 0.0180\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6608e-04 - root_mean_squared_error: 0.0231 - mae: 0.0144\n",
      "Epoch 13: val_loss improved from 0.00040 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 570ms/step - loss: 2.6608e-04 - root_mean_squared_error: 0.0231 - mae: 0.0144 - val_loss: 3.9298e-04 - val_root_mean_squared_error: 0.0279 - val_mae: 0.0178\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6540e-04 - root_mean_squared_error: 0.0231 - mae: 0.0144\n",
      "Epoch 14: val_loss improved from 0.00039 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 545ms/step - loss: 2.6540e-04 - root_mean_squared_error: 0.0231 - mae: 0.0144 - val_loss: 3.9277e-04 - val_root_mean_squared_error: 0.0277 - val_mae: 0.0177\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6466e-04 - root_mean_squared_error: 0.0229 - mae: 0.0142\n",
      "Epoch 15: val_loss improved from 0.00039 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 545ms/step - loss: 2.6466e-04 - root_mean_squared_error: 0.0229 - mae: 0.0142 - val_loss: 3.8969e-04 - val_root_mean_squared_error: 0.0281 - val_mae: 0.0177\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6422e-04 - root_mean_squared_error: 0.0231 - mae: 0.0143\n",
      "Epoch 16: val_loss improved from 0.00039 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 561ms/step - loss: 2.6422e-04 - root_mean_squared_error: 0.0231 - mae: 0.0143 - val_loss: 3.8859e-04 - val_root_mean_squared_error: 0.0278 - val_mae: 0.0176\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6379e-04 - root_mean_squared_error: 0.0231 - mae: 0.0143\n",
      "Epoch 17: val_loss improved from 0.00039 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 518ms/step - loss: 2.6379e-04 - root_mean_squared_error: 0.0231 - mae: 0.0143 - val_loss: 3.8708e-04 - val_root_mean_squared_error: 0.0281 - val_mae: 0.0177\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6346e-04 - root_mean_squared_error: 0.0230 - mae: 0.0142\n",
      "Epoch 18: val_loss did not improve from 0.00039\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 2.6346e-04 - root_mean_squared_error: 0.0230 - mae: 0.0142 - val_loss: 3.8836e-04 - val_root_mean_squared_error: 0.0282 - val_mae: 0.0177\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6309e-04 - root_mean_squared_error: 0.0230 - mae: 0.0142\n",
      "Epoch 19: val_loss improved from 0.00039 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 556ms/step - loss: 2.6309e-04 - root_mean_squared_error: 0.0230 - mae: 0.0142 - val_loss: 3.8584e-04 - val_root_mean_squared_error: 0.0278 - val_mae: 0.0174\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6300e-04 - root_mean_squared_error: 0.0230 - mae: 0.0142\n",
      "Epoch 20: val_loss did not improve from 0.00039\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 2.6300e-04 - root_mean_squared_error: 0.0230 - mae: 0.0142 - val_loss: 3.8667e-04 - val_root_mean_squared_error: 0.0276 - val_mae: 0.0175\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6292e-04 - root_mean_squared_error: 0.0231 - mae: 0.0143\n",
      "Epoch 21: val_loss improved from 0.00039 to 0.00039, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 2.6292e-04 - root_mean_squared_error: 0.0231 - mae: 0.0143 - val_loss: 3.8503e-04 - val_root_mean_squared_error: 0.0281 - val_mae: 0.0175\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Batch_Norm_1 (BatchNormali  (None, 5, 86)             344       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " LSTM_1 (LSTM)               (None, 5, 512)            1226752   \n",
      "                                                                 \n",
      " LSTM_2 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " Returns (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3460194 (13.20 MB)\n",
      "Trainable params: 3460022 (13.20 MB)\n",
      "Non-trainable params: 172 (688.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, GRU, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Model architecture creation:\n",
    "model_1 = Sequential([\n",
    "    BatchNormalization(\n",
    "        input_shape = (window_size,num_features),\n",
    "        name = 'Batch_Norm_1'),\n",
    "    LSTM(512,return_sequences=True,name='LSTM_1'),\n",
    "#    BatchNormalization(),\n",
    "    LSTM(512,name='LSTM_2'),\n",
    "#    BatchNormalization(momentum=0.8),\n",
    "    Dense(256,activation='relu',name='Dense_1'),\n",
    "    Dense(len(tickers),name='Returns')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define an early stopping callback chatgpt\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,          # Stop training after no improvement for 10 epochs\n",
    "    restore_best_weights=True,  # Restore the best weights when stopping\n",
    "min_delta=0.0001)\n",
    "\n",
    "# Checkpoint callback to save the model:\n",
    "checkpont_rnn = ModelCheckpoint(\n",
    "    filepath='model_inder_rnn2',\n",
    "    save_weights_only=False,\n",
    "    save_freq = 'epoch',\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only = True,\n",
    "    verbose = 1)\n",
    "\n",
    "\n",
    "# # Learning Rate Schedule: used to decide the learning rate:\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "#                 lambda epoch: 1e-8*10**(epoch/20))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=lr_schedule)\n",
    "\n",
    "# Compile Model\n",
    "model_1.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    metrics=[tf.metrics.RootMeanSquaredError(),'mae'],\n",
    "    optimizer=optimizer)\n",
    "\n",
    "# # Train model\n",
    "# history = model_1.fit(\n",
    "#     my_window.train,\n",
    "#     validation_data=my_window.val,\n",
    "#     epochs=100)\n",
    "\n",
    "# During model training, include both callbacks *********CHATGPT\n",
    "history = model_1.fit(\n",
    "    my_window.train,\n",
    "    epochs=100,\n",
    "    validation_data=my_window.val,\n",
    "    callbacks=[early_stopping, checkpont_rnn]\n",
    ")\n",
    "model_1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5131ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1105 - root_mean_squared_error: 0.5239 - mae: 0.2884[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>]\n",
      "Epoch 1: Learning Rate = 0.009999999776482582, Loss = 0.1104806512594223\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00158, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 609ms/step - loss: 0.1105 - root_mean_squared_error: 0.5239 - mae: 0.2884 - val_loss: 0.0016 - val_root_mean_squared_error: 0.0561 - val_mae: 0.0442 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0023 - root_mean_squared_error: 0.0690 - mae: 0.0451[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.009>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.009>]\n",
      "Epoch 2: Learning Rate = 0.008999999612569809, Loss = 0.0023073710035532713\n",
      "\n",
      "Epoch 2: val_loss improved from 0.00158 to 0.00044, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 515ms/step - loss: 0.0023 - root_mean_squared_error: 0.0690 - mae: 0.0451 - val_loss: 4.4304e-04 - val_root_mean_squared_error: 0.0301 - val_mae: 0.0203 - lr: 0.0090\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.5997e-04 - root_mean_squared_error: 0.0269 - mae: 0.0190[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0081>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0081>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0081>]\n",
      "Epoch 3: Learning Rate = 0.008100000210106373, Loss = 0.00035997419035993516\n",
      "\n",
      "Epoch 3: val_loss improved from 0.00044 to 0.00041, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 547ms/step - loss: 3.5997e-04 - root_mean_squared_error: 0.0269 - mae: 0.0190 - val_loss: 4.0911e-04 - val_root_mean_squared_error: 0.0282 - val_mae: 0.0188 - lr: 0.0081\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.9925e-04 - root_mean_squared_error: 0.0245 - mae: 0.0164[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00729>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00729>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00729>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00729>]\n",
      "Epoch 4: Learning Rate = 0.007290000095963478, Loss = 0.00029925076523795724\n",
      "\n",
      "Epoch 4: val_loss improved from 0.00041 to 0.00036, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 543ms/step - loss: 2.9925e-04 - root_mean_squared_error: 0.0245 - mae: 0.0164 - val_loss: 3.6012e-04 - val_root_mean_squared_error: 0.0271 - val_mae: 0.0165 - lr: 0.0073\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6752e-04 - root_mean_squared_error: 0.0233 - mae: 0.0146[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.006561>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.006561>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.006561>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.006561>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.006561>]\n",
      "Epoch 5: Learning Rate = 0.006560999900102615, Loss = 0.00026751827681437135\n",
      "\n",
      "Epoch 5: val_loss improved from 0.00036 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 523ms/step - loss: 2.6752e-04 - root_mean_squared_error: 0.0233 - mae: 0.0146 - val_loss: 3.5098e-04 - val_root_mean_squared_error: 0.0268 - val_mae: 0.0157 - lr: 0.0066\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6504e-04 - root_mean_squared_error: 0.0230 - mae: 0.0143[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0059049>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0059049>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0059049>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0059049>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0059049>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0059049>]\n",
      "Epoch 6: Learning Rate = 0.005904899910092354, Loss = 0.00026504058041609824\n",
      "\n",
      "Epoch 6: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 556ms/step - loss: 2.6504e-04 - root_mean_squared_error: 0.0230 - mae: 0.0143 - val_loss: 3.5090e-04 - val_root_mean_squared_error: 0.0265 - val_mae: 0.0158 - lr: 0.0059\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6375e-04 - root_mean_squared_error: 0.0230 - mae: 0.0143[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00531441>]\n",
      "Epoch 7: Learning Rate = 0.005314410198479891, Loss = 0.0002637517172843218\n",
      "\n",
      "Epoch 7: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 554ms/step - loss: 2.6375e-04 - root_mean_squared_error: 0.0230 - mae: 0.0143 - val_loss: 3.4785e-04 - val_root_mean_squared_error: 0.0267 - val_mae: 0.0158 - lr: 0.0053\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6068e-04 - root_mean_squared_error: 0.0229 - mae: 0.0141[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004782969>]\n",
      "Epoch 8: Learning Rate = 0.004782969132065773, Loss = 0.00026068391161970794\n",
      "\n",
      "Epoch 8: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 2.6068e-04 - root_mean_squared_error: 0.0229 - mae: 0.0141 - val_loss: 3.4616e-04 - val_root_mean_squared_error: 0.0270 - val_mae: 0.0158 - lr: 0.0048\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5939e-04 - root_mean_squared_error: 0.0228 - mae: 0.0140[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.004304672>]\n",
      "Epoch 9: Learning Rate = 0.004304672125726938, Loss = 0.0002593906538095325\n",
      "\n",
      "Epoch 9: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 522ms/step - loss: 2.5939e-04 - root_mean_squared_error: 0.0228 - mae: 0.0140 - val_loss: 3.4595e-04 - val_root_mean_squared_error: 0.0263 - val_mae: 0.0154 - lr: 0.0043\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5911e-04 - root_mean_squared_error: 0.0228 - mae: 0.0140[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0038742048>]\n",
      "Epoch 10: Learning Rate = 0.003874204820021987, Loss = 0.0002591066004242748\n",
      "\n",
      "Epoch 10: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 577ms/step - loss: 2.5911e-04 - root_mean_squared_error: 0.0228 - mae: 0.0140 - val_loss: 3.4578e-04 - val_root_mean_squared_error: 0.0262 - val_mae: 0.0154 - lr: 0.0039\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5887e-04 - root_mean_squared_error: 0.0229 - mae: 0.0140[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0034867844>]\n",
      "Epoch 11: Learning Rate = 0.0034867844078689814, Loss = 0.0002588716452009976\n",
      "\n",
      "Epoch 11: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 520ms/step - loss: 2.5887e-04 - root_mean_squared_error: 0.0229 - mae: 0.0140 - val_loss: 3.4554e-04 - val_root_mean_squared_error: 0.0263 - val_mae: 0.0154 - lr: 0.0035\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5869e-04 - root_mean_squared_error: 0.0226 - mae: 0.0140[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0031381059>]\n",
      "Epoch 12: Learning Rate = 0.0031381058506667614, Loss = 0.00025868858210742474\n",
      "\n",
      "Epoch 12: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 608ms/step - loss: 2.5869e-04 - root_mean_squared_error: 0.0226 - mae: 0.0140 - val_loss: 3.4538e-04 - val_root_mean_squared_error: 0.0268 - val_mae: 0.0156 - lr: 0.0031\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5853e-04 - root_mean_squared_error: 0.0228 - mae: 0.0140[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0028242953>]\n",
      "Epoch 13: Learning Rate = 0.002824295312166214, Loss = 0.0002585330803412944\n",
      "\n",
      "Epoch 13: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 2.5853e-04 - root_mean_squared_error: 0.0228 - mae: 0.0140 - val_loss: 3.4526e-04 - val_root_mean_squared_error: 0.0266 - val_mae: 0.0154 - lr: 0.0028\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5848e-04 - root_mean_squared_error: 0.0226 - mae: 0.0139[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.002541866>]\n",
      "Epoch 14: Learning Rate = 0.0025418659206479788, Loss = 0.0002584763860795647\n",
      "\n",
      "Epoch 14: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 552ms/step - loss: 2.5848e-04 - root_mean_squared_error: 0.0226 - mae: 0.0139 - val_loss: 3.4524e-04 - val_root_mean_squared_error: 0.0263 - val_mae: 0.0156 - lr: 0.0025\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5846e-04 - root_mean_squared_error: 0.0228 - mae: 0.0139[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0022876794>]\n",
      "Epoch 15: Learning Rate = 0.0022876793518662453, Loss = 0.00025845979689620435\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.00035\n",
      "6/6 [==============================] - 1s 206ms/step - loss: 2.5846e-04 - root_mean_squared_error: 0.0228 - mae: 0.0139 - val_loss: 3.4525e-04 - val_root_mean_squared_error: 0.0263 - val_mae: 0.0156 - lr: 0.0023\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5847e-04 - root_mean_squared_error: 0.0226 - mae: 0.0139[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0020589114>]\n",
      "Epoch 16: Learning Rate = 0.002058911370113492, Loss = 0.00025847216602414846\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00035\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 2.5847e-04 - root_mean_squared_error: 0.0226 - mae: 0.0139 - val_loss: 3.4526e-04 - val_root_mean_squared_error: 0.0259 - val_mae: 0.0151 - lr: 0.0021\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5847e-04 - root_mean_squared_error: 0.0228 - mae: 0.0139[<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>, <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0018530202>]\n",
      "Epoch 17: Learning Rate = 0.0018530201632529497, Loss = 0.0002584676258265972\n",
      "\n",
      "Epoch 17: val_loss improved from 0.00035 to 0.00035, saving model to model_inder_rnn2\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "6/6 [==============================] - 3s 533ms/step - loss: 2.5847e-04 - root_mean_squared_error: 0.0228 - mae: 0.0139 - val_loss: 3.4524e-04 - val_root_mean_squared_error: 0.0257 - val_mae: 0.0153 - lr: 0.0019\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, GRU, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "# Define your learning rate scheduler function\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.01\n",
    "    decay_factor = 0.9\n",
    "    return initial_lr * (decay_factor ** epoch)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,          # Stop training after no improvement for 10 epochs\n",
    "    restore_best_weights=True,  # Restore the best weights when stopping\n",
    "min_delta=0.00001)\n",
    "\n",
    "# Create a model (you need to replace this with your actual model)\n",
    "model_1 = Sequential([\n",
    "    BatchNormalization(\n",
    "        input_shape = (window_size,num_features),\n",
    "        name = 'Batch_Norm_1'),\n",
    "    LSTM(512,return_sequences=True,name='LSTM_1'),\n",
    "#    BatchNormalization(),\n",
    "    LSTM(512,name='LSTM_2'),\n",
    "#    BatchNormalization(momentum=0.8),\n",
    "    Dense(256,activation='relu',name='Dense_1'),\n",
    "    Dense(len(tickers),name='Returns')\n",
    "])\n",
    "\n",
    "\n",
    "checkpont_rnn = ModelCheckpoint(\n",
    "    filepath='model_inder_rnn2',\n",
    "    save_weights_only=False,\n",
    "    save_freq = 'epoch',\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only = True,\n",
    "    verbose = 1)\n",
    "\n",
    "# Compile the model with an optimizer (Adam in this example)\n",
    "optimizer = tf.keras.optimizers.Adam()  # Set initial learning rate to 0.0\n",
    "model_1.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    metrics=[tf.metrics.RootMeanSquaredError(),'mae'],\n",
    "    optimizer=optimizer)\n",
    "\n",
    "# Set up the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize lists to store loss and learning rates\n",
    "losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Custom callback to log loss and learning rate\n",
    "class LossLearningRateCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        loss = logs['loss']\n",
    "        learning_rates.append(lr)\n",
    "        print(learning_rates)\n",
    "        losses.append(loss)\n",
    "        print(f\"Epoch {epoch+1}: Learning Rate = {lr.numpy()}, Loss = {loss}\")\n",
    "\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "loss_lr_callback = LossLearningRateCallback()\n",
    "\n",
    "# Train your model\n",
    "# history = model.fit(x_train, y_train, epochs=num_epochs, callbacks=[lr_scheduler, loss_lr_callback])\n",
    "history = model_1.fit(\n",
    "    my_window.train,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=my_window.val,\n",
    "    callbacks=[lr_scheduler, loss_lr_callback, early_stopping, checkpont_rnn]\n",
    ")\n",
    "\n",
    "# Access the losses from the training history\n",
    "losses = history.history['loss']\n",
    "learning_rates = [lr_schedule(epoch) for epoch in range(len(losses))]\n",
    "\n",
    "# Plot the loss and learning rate\n",
    "plt.plot(learning_rates, losses, marker='o')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Learning Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70712664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Batch_Norm_1 (BatchNormali  (None, 5, 86)             344       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " LSTM_1 (LSTM)               (None, 5, 512)            1226752   \n",
      "                                                                 \n",
      " LSTM_2 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " Returns (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3460194 (13.20 MB)\n",
      "Trainable params: 3460022 (13.20 MB)\n",
      "Non-trainable params: 172 (688.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d468359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_1, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e376269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62da6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d563c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'RMSE Loss: Training and Validation')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.title('Huber Loss: Training and Validation')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_root_mean_squared_error'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.title('RMSE Loss: Training and Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdd49756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 86), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.2507e-04 - root_mean_squared_error: 0.0212 - mae: 0.0144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00022507143148686737, 0.02121657133102417, 0.014398113824427128]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(my_window.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15aec0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Create an example LSTM model\n",
    "\n",
    "\n",
    "# Plot the model architecture graph and save it to a file (e.g., model.png)\n",
    "plot_model(model_1, to_file='model_lstm.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "386c55c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final learning rate used for the final model is: 2.9512665430652826e-07\n"
     ]
    }
   ],
   "source": [
    "final_learning_rate = lr_schedule(num_epochs - 1)\n",
    "print(f\"The final learning rate used for the final model is: {final_learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "121ba916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.01,\n",
       "  0.009000000000000001,\n",
       "  0.008100000000000001,\n",
       "  0.007290000000000001,\n",
       "  0.006561,\n",
       "  0.005904900000000001,\n",
       "  0.00531441,\n",
       "  0.004782969000000001,\n",
       "  0.004304672100000001,\n",
       "  0.003874204890000001,\n",
       "  0.003486784401000001,\n",
       "  0.0031381059609000006,\n",
       "  0.0028242953648100013,\n",
       "  0.002541865828329001,\n",
       "  0.002287679245496101,\n",
       "  0.002058911320946491,\n",
       "  0.0018530201888518416],\n",
       " [0.1104806512594223,\n",
       "  0.0023073710035532713,\n",
       "  0.00035997419035993516,\n",
       "  0.00029925076523795724,\n",
       "  0.00026751827681437135,\n",
       "  0.00026504058041609824,\n",
       "  0.0002637517172843218,\n",
       "  0.00026068391161970794,\n",
       "  0.0002593906538095325,\n",
       "  0.0002591066004242748,\n",
       "  0.0002588716452009976,\n",
       "  0.00025868858210742474,\n",
       "  0.0002585330803412944,\n",
       "  0.0002584763860795647,\n",
       "  0.00025845979689620435,\n",
       "  0.00025847216602414846,\n",
       "  0.0002584676258265972])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f85e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

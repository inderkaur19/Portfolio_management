{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d71177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/mxqr91nj7135zkb7ptp3mq5c0000gn/T/ipykernel_13390/4242266090.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  mlp.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "# 1. Import libraries:\n",
    "%matplotlib inline\n",
    "import os\n",
    "import quandl\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as spo\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "# from financial_data import *\n",
    "import tensorflow as tf\n",
    "mlp.style.use('seaborn')\n",
    "quandl.save_key('HtwBLPt3k37yZHTvy15K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4404b7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol     Security             GICS Sector               GICS Sub-Industry  \\\n",
       "0    MMM           3M             Industrials        Industrial Conglomerates   \n",
       "1    AOS  A. O. Smith             Industrials               Building Products   \n",
       "2    ABT       Abbott             Health Care           Health Care Equipment   \n",
       "3   ABBV       AbbVie             Health Care                 Pharmaceuticals   \n",
       "4    ACN    Accenture  Information Technology  IT Consulting & Other Services   \n",
       "\n",
       "     Headquarters Location  Date added      CIK      Founded  \n",
       "0    Saint Paul, Minnesota  1957-03-04    66740         1902  \n",
       "1     Milwaukee, Wisconsin  2017-07-26    91142         1916  \n",
       "2  North Chicago, Illinois  1957-03-04     1800         1888  \n",
       "3  North Chicago, Illinois  2012-12-31  1551152  2013 (1888)  \n",
       "4          Dublin, Ireland  2011-07-06  1467373         1989  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "sp500 = pd.read_html(sp_url, header=0)[0]\n",
    "sp500.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e367bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stocks in the universe is: 241\n"
     ]
    }
   ],
   "source": [
    "# Correct invalid dates:\n",
    "sp500.loc[sp500[sp500['Date added']=='1983-11-30 (1957-03-04)'].index,'Date added'] = '1983-11-30'\n",
    "sp500.loc[sp500[sp500['Date added']=='2001?'].index,'Date added'] = '2001-01-01'\n",
    "# Filter firms that entered the index after December 2015:\n",
    "sp500['Date added'] = pd.to_datetime(sp500['Date added'],format='%Y-%m-%d')\n",
    "sp500 = sp500[sp500['Date added']<'2007-01-01']\n",
    "print(\"The number of stocks in the universe is:\", sp500.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b917005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIS', 'BIIB', 'PGR', 'SWK', 'PEP', 'ZION', 'LUV', 'BBWI', 'TFC', 'CBRE']\n"
     ]
    }
   ],
   "source": [
    "n_stocks = 10\n",
    "np.random.seed(1792)\n",
    "universe_tickers = sp500['Symbol'].unique()\n",
    "tickers = list(np.random.choice(universe_tickers,replace=False,size=n_stocks))\n",
    "print(tickers)\n",
    "# sp500[sp500['Symbol'].isin(portfolio_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1573d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  10 of 10 completed\n",
      "            Adj Close                                               \\\n",
      "                 BBWI        BIIB       CBRE        DIS        LUV   \n",
      "Date                                                                 \n",
      "2007-01-03  10.077239   49.330002  33.650002  28.317097  14.001652   \n",
      "2007-01-04   9.317532   49.750000  33.340000  28.540649  14.218172   \n",
      "2007-01-05   9.310717   49.759998  33.099998  28.308817  13.983607   \n",
      "2007-01-08   9.130157   50.020000  33.220001  28.565493  14.236216   \n",
      "2007-01-09   9.266430   49.500000  33.970001  28.524090  14.263284   \n",
      "...               ...         ...        ...        ...        ...   \n",
      "2022-12-23  41.287586  279.160004  76.669998  88.010002  35.322170   \n",
      "2022-12-27  41.297436  274.769989  76.489998  86.370003  33.217911   \n",
      "2022-12-28  40.116104  274.040009  75.410004  84.169998  31.505142   \n",
      "2022-12-29  41.002106  276.000000  77.550003  87.180000  32.669830   \n",
      "2022-12-30  41.484478  276.920013  76.959999  86.879997  32.953655   \n",
      "\n",
      "                                                                     ...  \\\n",
      "                   PEP         PGR        SWK        TFC       ZION  ...   \n",
      "Date                                                                 ...   \n",
      "2007-01-03   38.873489   13.782968  34.064186  23.917282  62.279121  ...   \n",
      "2007-01-04   39.139996   13.794351  34.278854  23.732466  62.557014  ...   \n",
      "2007-01-05   39.016041   13.521085  34.144691  23.417194  62.369225  ...   \n",
      "2007-01-08   39.102814   13.612178  34.017223  23.324783  62.474396  ...   \n",
      "2007-01-09   39.263954   13.378761  34.634384  23.281296  62.819939  ...   \n",
      "...                ...         ...        ...        ...        ...  ...   \n",
      "2022-12-23  178.524826  129.153381  71.870193  40.671665  46.701611  ...   \n",
      "2022-12-27  179.318237  129.881760  72.327049  40.910400  47.077538  ...   \n",
      "2022-12-28  178.025284  129.432770  70.159462  40.633465  46.238934  ...   \n",
      "2022-12-29  178.250565  130.270905  73.785049  41.254185  47.308880  ...   \n",
      "2022-12-30  176.957626  129.422791  73.017166  41.091843  47.385986  ...   \n",
      "\n",
      "                  Low                                               \\\n",
      "                 BBWI        BIIB       CBRE        DIS        LUV   \n",
      "Date                                                                 \n",
      "2007-01-03  23.540825   48.200001  33.139999  33.531136  15.350000   \n",
      "2007-01-04  21.891672   48.880001  33.099998  33.708706  15.420000   \n",
      "2007-01-05  21.147940   49.509998  32.799999  33.531136  15.420000   \n",
      "2007-01-08  21.471302   48.910000  32.500000  33.610054  15.340000   \n",
      "2007-01-09  21.827002   49.299999  33.060001  33.491676  15.700000   \n",
      "...               ...         ...        ...        ...        ...   \n",
      "2022-12-23  40.150002  276.070007  75.150002  85.769997  35.180000   \n",
      "2022-12-27  41.810001  273.380005  76.099998  85.959999  33.650002   \n",
      "2022-12-28  40.630001  272.640015  75.379997  84.070000  32.189999   \n",
      "2022-12-29  41.180000  274.299988  75.769997  84.970001  32.209999   \n",
      "2022-12-30  40.880001  272.200012  76.089996  85.230003  33.110001   \n",
      "\n",
      "                                                                     \n",
      "                   PEP         PGR        SWK        TFC       ZION  \n",
      "Date                                                                 \n",
      "2007-01-03   62.450001   24.070000  50.049999  43.630001  81.180000  \n",
      "2007-01-04   62.500000   24.049999  50.259998  43.299999  82.550003  \n",
      "2007-01-05   62.700001   23.600000  50.590000  42.990002  82.599998  \n",
      "2007-01-08   62.860001   23.540001  49.950001  42.490002  82.150002  \n",
      "2007-01-09   63.009998   23.440001  50.680000  42.580002  83.190002  \n",
      "...                ...         ...        ...        ...        ...  \n",
      "2022-12-23  180.449997  127.660004  72.599998  42.380001  47.990002  \n",
      "2022-12-27  182.270004  129.190002  73.250000  42.340000  47.910000  \n",
      "2022-12-28  181.639999  129.690002  72.160004  42.490002  47.939999  \n",
      "2022-12-29  181.889999  129.740005  72.769997  42.599998  47.930000  \n",
      "2022-12-30  179.289993  128.429993  74.330002  42.740002  48.500000  \n",
      "\n",
      "[4028 rows x 50 columns]\n",
      "the dataframe before the columns were renamed\n",
      "MultiIndex([('Adj Close', 'BBWI'),\n",
      "            ('Adj Close', 'BIIB'),\n",
      "            ('Adj Close', 'CBRE'),\n",
      "            ('Adj Close',  'DIS'),\n",
      "            ('Adj Close',  'LUV'),\n",
      "            ('Adj Close',  'PEP'),\n",
      "            ('Adj Close',  'PGR'),\n",
      "            ('Adj Close',  'SWK'),\n",
      "            ('Adj Close',  'TFC'),\n",
      "            ('Adj Close', 'ZION'),\n",
      "            (   'Volume', 'BBWI'),\n",
      "            (   'Volume', 'BIIB'),\n",
      "            (   'Volume', 'CBRE'),\n",
      "            (   'Volume',  'DIS'),\n",
      "            (   'Volume',  'LUV'),\n",
      "            (   'Volume',  'PEP'),\n",
      "            (   'Volume',  'PGR'),\n",
      "            (   'Volume',  'SWK'),\n",
      "            (   'Volume',  'TFC'),\n",
      "            (   'Volume', 'ZION'),\n",
      "            (     'Open', 'BBWI'),\n",
      "            (     'Open', 'BIIB'),\n",
      "            (     'Open', 'CBRE'),\n",
      "            (     'Open',  'DIS'),\n",
      "            (     'Open',  'LUV'),\n",
      "            (     'Open',  'PEP'),\n",
      "            (     'Open',  'PGR'),\n",
      "            (     'Open',  'SWK'),\n",
      "            (     'Open',  'TFC'),\n",
      "            (     'Open', 'ZION'),\n",
      "            (     'High', 'BBWI'),\n",
      "            (     'High', 'BIIB'),\n",
      "            (     'High', 'CBRE'),\n",
      "            (     'High',  'DIS'),\n",
      "            (     'High',  'LUV'),\n",
      "            (     'High',  'PEP'),\n",
      "            (     'High',  'PGR'),\n",
      "            (     'High',  'SWK'),\n",
      "            (     'High',  'TFC'),\n",
      "            (     'High', 'ZION'),\n",
      "            (      'Low', 'BBWI'),\n",
      "            (      'Low', 'BIIB'),\n",
      "            (      'Low', 'CBRE'),\n",
      "            (      'Low',  'DIS'),\n",
      "            (      'Low',  'LUV'),\n",
      "            (      'Low',  'PEP'),\n",
      "            (      'Low',  'PGR'),\n",
      "            (      'Low',  'SWK'),\n",
      "            (      'Low',  'TFC'),\n",
      "            (      'Low', 'ZION')],\n",
      "           )\n",
      "Index(['Adj Close_BBWI', 'Adj Close_BIIB', 'Adj Close_CBRE', 'Adj Close_DIS',\n",
      "       'Adj Close_LUV', 'Adj Close_PEP', 'Adj Close_PGR', 'Adj Close_SWK',\n",
      "       'Adj Close_TFC', 'Adj Close_ZION', 'Volume_BBWI', 'Volume_BIIB',\n",
      "       'Volume_CBRE', 'Volume_DIS', 'Volume_LUV', 'Volume_PEP', 'Volume_PGR',\n",
      "       'Volume_SWK', 'Volume_TFC', 'Volume_ZION', 'Open_BBWI', 'Open_BIIB',\n",
      "       'Open_CBRE', 'Open_DIS', 'Open_LUV', 'Open_PEP', 'Open_PGR', 'Open_SWK',\n",
      "       'Open_TFC', 'Open_ZION', 'High_BBWI', 'High_BIIB', 'High_CBRE',\n",
      "       'High_DIS', 'High_LUV', 'High_PEP', 'High_PGR', 'High_SWK', 'High_TFC',\n",
      "       'High_ZION', 'Low_BBWI', 'Low_BIIB', 'Low_CBRE', 'Low_DIS', 'Low_LUV',\n",
      "       'Low_PEP', 'Low_PGR', 'Low_SWK', 'Low_TFC', 'Low_ZION'],\n",
      "      dtype='object')\n",
      "the dataframe after the columns have been renamed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close_BBWI</th>\n",
       "      <th>Close_BIIB</th>\n",
       "      <th>Close_CBRE</th>\n",
       "      <th>Close_DIS</th>\n",
       "      <th>Close_LUV</th>\n",
       "      <th>Close_PEP</th>\n",
       "      <th>Close_PGR</th>\n",
       "      <th>Close_SWK</th>\n",
       "      <th>Close_TFC</th>\n",
       "      <th>Close_ZION</th>\n",
       "      <th>...</th>\n",
       "      <th>Low_BBWI</th>\n",
       "      <th>Low_BIIB</th>\n",
       "      <th>Low_CBRE</th>\n",
       "      <th>Low_DIS</th>\n",
       "      <th>Low_LUV</th>\n",
       "      <th>Low_PEP</th>\n",
       "      <th>Low_PGR</th>\n",
       "      <th>Low_SWK</th>\n",
       "      <th>Low_TFC</th>\n",
       "      <th>Low_ZION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>10.077239</td>\n",
       "      <td>49.330002</td>\n",
       "      <td>33.650002</td>\n",
       "      <td>28.317097</td>\n",
       "      <td>14.001652</td>\n",
       "      <td>38.873489</td>\n",
       "      <td>13.782968</td>\n",
       "      <td>34.064186</td>\n",
       "      <td>23.917282</td>\n",
       "      <td>62.279121</td>\n",
       "      <td>...</td>\n",
       "      <td>23.540825</td>\n",
       "      <td>48.200001</td>\n",
       "      <td>33.139999</td>\n",
       "      <td>33.531136</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>62.450001</td>\n",
       "      <td>24.070000</td>\n",
       "      <td>50.049999</td>\n",
       "      <td>43.630001</td>\n",
       "      <td>81.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>9.317532</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>33.340000</td>\n",
       "      <td>28.540649</td>\n",
       "      <td>14.218172</td>\n",
       "      <td>39.139996</td>\n",
       "      <td>13.794351</td>\n",
       "      <td>34.278854</td>\n",
       "      <td>23.732466</td>\n",
       "      <td>62.557014</td>\n",
       "      <td>...</td>\n",
       "      <td>21.891672</td>\n",
       "      <td>48.880001</td>\n",
       "      <td>33.099998</td>\n",
       "      <td>33.708706</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>50.259998</td>\n",
       "      <td>43.299999</td>\n",
       "      <td>82.550003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>9.310717</td>\n",
       "      <td>49.759998</td>\n",
       "      <td>33.099998</td>\n",
       "      <td>28.308817</td>\n",
       "      <td>13.983607</td>\n",
       "      <td>39.016041</td>\n",
       "      <td>13.521085</td>\n",
       "      <td>34.144691</td>\n",
       "      <td>23.417194</td>\n",
       "      <td>62.369225</td>\n",
       "      <td>...</td>\n",
       "      <td>21.147940</td>\n",
       "      <td>49.509998</td>\n",
       "      <td>32.799999</td>\n",
       "      <td>33.531136</td>\n",
       "      <td>15.420000</td>\n",
       "      <td>62.700001</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>50.590000</td>\n",
       "      <td>42.990002</td>\n",
       "      <td>82.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>9.130157</td>\n",
       "      <td>50.020000</td>\n",
       "      <td>33.220001</td>\n",
       "      <td>28.565493</td>\n",
       "      <td>14.236216</td>\n",
       "      <td>39.102814</td>\n",
       "      <td>13.612178</td>\n",
       "      <td>34.017223</td>\n",
       "      <td>23.324783</td>\n",
       "      <td>62.474396</td>\n",
       "      <td>...</td>\n",
       "      <td>21.471302</td>\n",
       "      <td>48.910000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>33.610054</td>\n",
       "      <td>15.340000</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>23.540001</td>\n",
       "      <td>49.950001</td>\n",
       "      <td>42.490002</td>\n",
       "      <td>82.150002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-09</th>\n",
       "      <td>9.266430</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>33.970001</td>\n",
       "      <td>28.524090</td>\n",
       "      <td>14.263284</td>\n",
       "      <td>39.263954</td>\n",
       "      <td>13.378761</td>\n",
       "      <td>34.634384</td>\n",
       "      <td>23.281296</td>\n",
       "      <td>62.819939</td>\n",
       "      <td>...</td>\n",
       "      <td>21.827002</td>\n",
       "      <td>49.299999</td>\n",
       "      <td>33.060001</td>\n",
       "      <td>33.491676</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>63.009998</td>\n",
       "      <td>23.440001</td>\n",
       "      <td>50.680000</td>\n",
       "      <td>42.580002</td>\n",
       "      <td>83.190002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>41.287586</td>\n",
       "      <td>279.160004</td>\n",
       "      <td>76.669998</td>\n",
       "      <td>88.010002</td>\n",
       "      <td>35.322170</td>\n",
       "      <td>178.524826</td>\n",
       "      <td>129.153381</td>\n",
       "      <td>71.870193</td>\n",
       "      <td>40.671665</td>\n",
       "      <td>46.701611</td>\n",
       "      <td>...</td>\n",
       "      <td>40.150002</td>\n",
       "      <td>276.070007</td>\n",
       "      <td>75.150002</td>\n",
       "      <td>85.769997</td>\n",
       "      <td>35.180000</td>\n",
       "      <td>180.449997</td>\n",
       "      <td>127.660004</td>\n",
       "      <td>72.599998</td>\n",
       "      <td>42.380001</td>\n",
       "      <td>47.990002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>41.297436</td>\n",
       "      <td>274.769989</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>86.370003</td>\n",
       "      <td>33.217911</td>\n",
       "      <td>179.318237</td>\n",
       "      <td>129.881760</td>\n",
       "      <td>72.327049</td>\n",
       "      <td>40.910400</td>\n",
       "      <td>47.077538</td>\n",
       "      <td>...</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>273.380005</td>\n",
       "      <td>76.099998</td>\n",
       "      <td>85.959999</td>\n",
       "      <td>33.650002</td>\n",
       "      <td>182.270004</td>\n",
       "      <td>129.190002</td>\n",
       "      <td>73.250000</td>\n",
       "      <td>42.340000</td>\n",
       "      <td>47.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>40.116104</td>\n",
       "      <td>274.040009</td>\n",
       "      <td>75.410004</td>\n",
       "      <td>84.169998</td>\n",
       "      <td>31.505142</td>\n",
       "      <td>178.025284</td>\n",
       "      <td>129.432770</td>\n",
       "      <td>70.159462</td>\n",
       "      <td>40.633465</td>\n",
       "      <td>46.238934</td>\n",
       "      <td>...</td>\n",
       "      <td>40.630001</td>\n",
       "      <td>272.640015</td>\n",
       "      <td>75.379997</td>\n",
       "      <td>84.070000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>181.639999</td>\n",
       "      <td>129.690002</td>\n",
       "      <td>72.160004</td>\n",
       "      <td>42.490002</td>\n",
       "      <td>47.939999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>41.002106</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>77.550003</td>\n",
       "      <td>87.180000</td>\n",
       "      <td>32.669830</td>\n",
       "      <td>178.250565</td>\n",
       "      <td>130.270905</td>\n",
       "      <td>73.785049</td>\n",
       "      <td>41.254185</td>\n",
       "      <td>47.308880</td>\n",
       "      <td>...</td>\n",
       "      <td>41.180000</td>\n",
       "      <td>274.299988</td>\n",
       "      <td>75.769997</td>\n",
       "      <td>84.970001</td>\n",
       "      <td>32.209999</td>\n",
       "      <td>181.889999</td>\n",
       "      <td>129.740005</td>\n",
       "      <td>72.769997</td>\n",
       "      <td>42.599998</td>\n",
       "      <td>47.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>41.484478</td>\n",
       "      <td>276.920013</td>\n",
       "      <td>76.959999</td>\n",
       "      <td>86.879997</td>\n",
       "      <td>32.953655</td>\n",
       "      <td>176.957626</td>\n",
       "      <td>129.422791</td>\n",
       "      <td>73.017166</td>\n",
       "      <td>41.091843</td>\n",
       "      <td>47.385986</td>\n",
       "      <td>...</td>\n",
       "      <td>40.880001</td>\n",
       "      <td>272.200012</td>\n",
       "      <td>76.089996</td>\n",
       "      <td>85.230003</td>\n",
       "      <td>33.110001</td>\n",
       "      <td>179.289993</td>\n",
       "      <td>128.429993</td>\n",
       "      <td>74.330002</td>\n",
       "      <td>42.740002</td>\n",
       "      <td>48.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4028 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close_BBWI  Close_BIIB  Close_CBRE  Close_DIS  Close_LUV  \\\n",
       "Date                                                                   \n",
       "2007-01-03   10.077239   49.330002   33.650002  28.317097  14.001652   \n",
       "2007-01-04    9.317532   49.750000   33.340000  28.540649  14.218172   \n",
       "2007-01-05    9.310717   49.759998   33.099998  28.308817  13.983607   \n",
       "2007-01-08    9.130157   50.020000   33.220001  28.565493  14.236216   \n",
       "2007-01-09    9.266430   49.500000   33.970001  28.524090  14.263284   \n",
       "...                ...         ...         ...        ...        ...   \n",
       "2022-12-23   41.287586  279.160004   76.669998  88.010002  35.322170   \n",
       "2022-12-27   41.297436  274.769989   76.489998  86.370003  33.217911   \n",
       "2022-12-28   40.116104  274.040009   75.410004  84.169998  31.505142   \n",
       "2022-12-29   41.002106  276.000000   77.550003  87.180000  32.669830   \n",
       "2022-12-30   41.484478  276.920013   76.959999  86.879997  32.953655   \n",
       "\n",
       "             Close_PEP   Close_PGR  Close_SWK  Close_TFC  Close_ZION  ...  \\\n",
       "Date                                                                  ...   \n",
       "2007-01-03   38.873489   13.782968  34.064186  23.917282   62.279121  ...   \n",
       "2007-01-04   39.139996   13.794351  34.278854  23.732466   62.557014  ...   \n",
       "2007-01-05   39.016041   13.521085  34.144691  23.417194   62.369225  ...   \n",
       "2007-01-08   39.102814   13.612178  34.017223  23.324783   62.474396  ...   \n",
       "2007-01-09   39.263954   13.378761  34.634384  23.281296   62.819939  ...   \n",
       "...                ...         ...        ...        ...         ...  ...   \n",
       "2022-12-23  178.524826  129.153381  71.870193  40.671665   46.701611  ...   \n",
       "2022-12-27  179.318237  129.881760  72.327049  40.910400   47.077538  ...   \n",
       "2022-12-28  178.025284  129.432770  70.159462  40.633465   46.238934  ...   \n",
       "2022-12-29  178.250565  130.270905  73.785049  41.254185   47.308880  ...   \n",
       "2022-12-30  176.957626  129.422791  73.017166  41.091843   47.385986  ...   \n",
       "\n",
       "             Low_BBWI    Low_BIIB   Low_CBRE    Low_DIS    Low_LUV  \\\n",
       "Date                                                                 \n",
       "2007-01-03  23.540825   48.200001  33.139999  33.531136  15.350000   \n",
       "2007-01-04  21.891672   48.880001  33.099998  33.708706  15.420000   \n",
       "2007-01-05  21.147940   49.509998  32.799999  33.531136  15.420000   \n",
       "2007-01-08  21.471302   48.910000  32.500000  33.610054  15.340000   \n",
       "2007-01-09  21.827002   49.299999  33.060001  33.491676  15.700000   \n",
       "...               ...         ...        ...        ...        ...   \n",
       "2022-12-23  40.150002  276.070007  75.150002  85.769997  35.180000   \n",
       "2022-12-27  41.810001  273.380005  76.099998  85.959999  33.650002   \n",
       "2022-12-28  40.630001  272.640015  75.379997  84.070000  32.189999   \n",
       "2022-12-29  41.180000  274.299988  75.769997  84.970001  32.209999   \n",
       "2022-12-30  40.880001  272.200012  76.089996  85.230003  33.110001   \n",
       "\n",
       "               Low_PEP     Low_PGR    Low_SWK    Low_TFC   Low_ZION  \n",
       "Date                                                                 \n",
       "2007-01-03   62.450001   24.070000  50.049999  43.630001  81.180000  \n",
       "2007-01-04   62.500000   24.049999  50.259998  43.299999  82.550003  \n",
       "2007-01-05   62.700001   23.600000  50.590000  42.990002  82.599998  \n",
       "2007-01-08   62.860001   23.540001  49.950001  42.490002  82.150002  \n",
       "2007-01-09   63.009998   23.440001  50.680000  42.580002  83.190002  \n",
       "...                ...         ...        ...        ...        ...  \n",
       "2022-12-23  180.449997  127.660004  72.599998  42.380001  47.990002  \n",
       "2022-12-27  182.270004  129.190002  73.250000  42.340000  47.910000  \n",
       "2022-12-28  181.639999  129.690002  72.160004  42.490002  47.939999  \n",
       "2022-12-29  181.889999  129.740005  72.769997  42.599998  47.930000  \n",
       "2022-12-30  179.289993  128.429993  74.330002  42.740002  48.500000  \n",
       "\n",
       "[4028 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tickers = ['AAPL', 'GOOGL', 'MSFT','SPY']\n",
    "# start_date='2007-01-01'\n",
    "# end_date='2022-12-31'\n",
    "# columns = ['Adj Close', 'Volume']\n",
    "# # Download data\n",
    "# data = yf.download(tickers, start=start_date, end=end_date)[columns]\n",
    "\n",
    "# # 'data' will be a Pandas DataFrame containing the historical data for the specified tickers\n",
    "# print(data)\n",
    "\n",
    "\n",
    "def download_data(tickers, start_date, end_date, columns):\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)\n",
    "    data=data[columns]\n",
    "    return data\n",
    "\n",
    "# Define the tickers, start date, end date, and columns you want to download\n",
    "# tickers = ['AAPL', 'GOOGL', 'MSFT']\n",
    "start_date = '2007-01-01'\n",
    "end_date = '2022-12-31'\n",
    "columns = [ 'Adj Close', 'Volume','Open','High','Low']\n",
    "\n",
    "# Download the data\n",
    "data = download_data(tickers, start_date, end_date, columns)\n",
    "print(data)\n",
    "\n",
    "\n",
    "def one_lvl_colnames(df,cols,tickers):\n",
    "    \"\"\"This function changes a multi-level column indexation into a one level\n",
    "    column indexation\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    df (pandas Dataframe): dataframe with the columns whose indexation will be \n",
    "        flattened.\n",
    "    tickers (list|string): list/string with the tickers (s) in the data frame df.\n",
    "    cols (list|string): list/string with the name of the columns (e.g. 'Adj Close',\n",
    "        'High', 'Close', etc.) that are in the dataframe df.\n",
    "    \n",
    "    Ouputs:\n",
    "    -------\n",
    "    df (pandas Dataframe): dataframe with the same information as df, but \n",
    "        with one level of indexation.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"the dataframe before the columns were renamed\")\n",
    "#     print(df.columns)\n",
    "    df_not_renamed=df.copy()\n",
    "#     df_not_renamed.drop(\"Open\",inplace=True)\n",
    "    print(df_not_renamed.columns)\n",
    "    # Define important variables:\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    if isinstance(cols, str):\n",
    "        \n",
    "        cols = [cols]\n",
    "        print(cols)\n",
    "\n",
    "    # For multi-level column indexing:\n",
    "    if isinstance(df.columns.values[0], tuple):\n",
    "\n",
    "        # Define important varibles\n",
    "        columns = df.columns.values\n",
    "        new_cols = []\n",
    "\n",
    "        # Itarate through the multi-level column names and flatten them:\n",
    "        for col in columns:\n",
    "            temp = []\n",
    "            for name in col:\n",
    "                if name != '':\n",
    "                    temp.append(name)\n",
    "            new_temp = '_'.join(temp)\n",
    "            new_cols.append(new_temp)\n",
    "        \n",
    "        # Change the column names:\n",
    "        df.columns = new_cols\n",
    "        print(df.columns)\n",
    "    \n",
    "    # For uni-level colum indexing:\n",
    "    elif isinstance(df.columns.values[0], str):\n",
    "        \n",
    "        # Define new names:\n",
    "        col_names = [column+'_'+ticker for column in cols\\\n",
    "                     for ticker in tickers]\n",
    "        df.columns = col_names\n",
    "    \n",
    "    print(\"the dataframe after the columns have been renamed\") \n",
    "#     print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "df2=pd.DataFrame(one_lvl_colnames(data,cols=columns,tickers=tickers))\n",
    "df2_columns=list(df2.columns)\n",
    "df2.columns = [col.replace('Adj Close', 'Close') for col in df2_columns]\n",
    "df2.fillna(method='ffill',inplace=True)\n",
    "df2.fillna(method='bfill',inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851f2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(prices):\n",
    "    \"\"\"This function finds the momentum metric for a group of assets.\n",
    "    Inputs:\n",
    "    -------\n",
    "    prices (pandas dataframe|series): dataframe with the information of the\n",
    "        prices of a group of assets or an indivitual asset.\n",
    "    \n",
    "    Outputs:\n",
    "    --------\n",
    "    momentum_df (pandas Dataframe): dataframe with the information of the\n",
    "        momentum.\n",
    "    \"\"\"\n",
    "    # Compute components:\n",
    "    first = prices.iloc[0]\n",
    "    last = prices.iloc[-1]\n",
    "\n",
    "    # Compute momentum:\n",
    "    momentum_df = last/first\n",
    "\n",
    "    return momentum_df\n",
    "\n",
    "def simple_moving_average(prices):\n",
    "    \"\"\"This function computes the simple moving average for a set of prices\n",
    "    given a time window.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    prices (pandas Dataframe|Series): dataframe with the information of the\n",
    "        prices of the analyzed assets.\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    sma (pandas Dataframe): dataframe of the SMA of the assets' prices in\n",
    "        the prices dataframe.\n",
    "    \"\"\"\n",
    "    # Compute the SMA of the assets:\n",
    "    mean = prices.mean()\n",
    "    sma = prices[-1]/mean-1\n",
    "\n",
    "    return sma\n",
    "    \n",
    "def bollinger_bands(prices):\n",
    "    \"\"\"This function computes the bollinger bands values for a set of assets\n",
    "    returned in a pandas dataframe.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    prices (pandas Dataframe|Series): dataframe with the price infromation\n",
    "        of the assets analyzed.\n",
    "    window (numeric value, default=None): determines if compute the BBs in\n",
    "        a rolling manner, or if compute them to the whole input of prices.\n",
    "    \n",
    "    Outputs:\n",
    "    --------\n",
    "    bb (pandas dataframe): dataframe with the information of the bollinger\n",
    "        bands of the assets in the prices dataframe.\n",
    "    \"\"\"\n",
    "    # Compute components:\n",
    "    ma = prices.mean()\n",
    "    std = prices.std()\n",
    "\n",
    "    # Compute bollinger bands:\n",
    "    bb = (prices[-1]-ma)/(2*std)\n",
    "\n",
    "    return bb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abafca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_statistics(df,cols='Adj Close',tickers=None,functions=None,\n",
    "                      window=20,bollinger=False,roll_linewidth=1.5,**kwargs):\n",
    "        '''This method extracts the rolling statistics from a time series, and\n",
    "        can plot the rolling window with the data, adding the Bollinger bands.\n",
    "        \n",
    "        Inputs:\n",
    "        -------\n",
    "        column (string, default=None): the column from which you want compute \n",
    "            the rolling function.\n",
    "        tickers (str|list, default=None): the ticker(s) from which you want to know the \n",
    "            information.\n",
    "        functions (function|string|list): function(s) that will be rolled through the \n",
    "            time series.\n",
    "        window (int): the window of the rolling data\n",
    "            \n",
    "        OUTPUTS:\n",
    "            rolled (pandas series): a series with the rolling statistics specified\n",
    "\n",
    "        '''\n",
    "        # Define important varibles:\n",
    "#         df = self.df\n",
    "#         if isinstance(tickers,type(None)):\n",
    "#             tickers = self.get_tickers()\n",
    "        col_names = cols\n",
    "        if isinstance(functions,type(None)):\n",
    "            functions = [momentum, simple_moving_average, bollinger_bands]\n",
    "        elif not isinstance(functions,list):\n",
    "            functions = [functions]\n",
    "\n",
    "        # Define the actual dataframe analized\n",
    "        df = df[col_names]\n",
    "#         print(df)\n",
    "\n",
    "        # Compute the rolling statistics:\n",
    "        rolling_stats = df.rolling(window).agg(functions)\n",
    "        \n",
    "        return rolling_stats\n",
    "        \n",
    "#         print(rolling_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2180b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "BBWI\n",
      "                close    volume       open       high        low      cci_14  \\\n",
      "Date                                                                           \n",
      "2007-01-03  10.077239   9288633  24.252222  24.276476  23.540825         NaN   \n",
      "2007-01-04   9.317532  17476831  22.546482  22.756668  21.891672  -66.666667   \n",
      "2007-01-05   9.310717   9970096  21.980598  22.433306  21.147940  -68.036282   \n",
      "2007-01-08   9.130157   9207238  22.029102  22.465643  21.471302  -53.794551   \n",
      "2007-01-09   9.266430  10808659  21.843168  22.465643  21.827002  -33.004947   \n",
      "2007-01-10   9.177852   5218037  21.891672  21.891672  21.535975  -71.427884   \n",
      "2007-01-11   9.545781   8770577  21.883589  22.789005  21.859337    9.747085   \n",
      "2007-01-12   9.750189   5945764  22.635408  23.241714  22.530315   69.965750   \n",
      "2007-01-16   9.590069   5575406  22.991108  23.185125  22.716249   58.354482   \n",
      "2007-01-17   9.446983   6431163  22.716249  22.716249  22.320129    6.521029   \n",
      "2007-01-18   9.658209   4008127  22.667746  23.435732  22.554567   64.492961   \n",
      "2007-01-19   9.671836   4918559  22.441389  23.039612  22.384802   32.172834   \n",
      "2007-01-22   9.637767   4623906  23.282133  23.435732  22.772839   68.262702   \n",
      "2007-01-23   9.630957   4078018  22.691998  23.120453  22.675829   42.442659   \n",
      "2007-01-24   9.641175   3377257  22.789005  23.088116  22.732416   64.124148   \n",
      "2007-01-25   9.467430   3347569  22.942602  22.942602  22.376719   14.674043   \n",
      "2007-01-26   9.409515   4071214  22.457560  22.562653  22.093775  -46.356057   \n",
      "2007-01-29   9.389075   2294759  22.231205  22.400970  22.126112  -68.274649   \n",
      "2007-01-30   9.470837   4566138  22.247374  22.506063  22.247374  -49.613538   \n",
      "2007-01-31   9.518534   2649159  22.368633  22.764753  22.368633  -30.100530   \n",
      "2007-02-01   9.702496   4721011  22.716249  23.047695  22.457560   27.015062   \n",
      "2007-02-02   9.729747   4073194  23.209377  23.249798  22.926435  104.883873   \n",
      "2007-02-05   9.746786   3568003  23.055780  23.233629  22.926435   97.129259   \n",
      "2007-02-06   9.937562   6720374  23.120453  23.678253  23.104284  157.611932   \n",
      "2007-02-07   9.978444   5023086  23.492319  24.155214  23.338722  184.858791   \n",
      "2007-02-08   9.896679   3266546  23.637833  23.759094  23.257881  113.131323   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -19.268624         66.506361   \n",
      "2007-01-04          NaN  0.500000 -15.293656         62.557735   \n",
      "2007-01-05          NaN  0.500000  -9.857022         61.706631   \n",
      "2007-01-08          NaN  0.500000 -12.972357         61.706631   \n",
      "2007-01-09   100.000000  0.500000 -19.692959         61.706631   \n",
      "2007-01-10    90.587563  0.500000 -35.743337         61.163120   \n",
      "2007-01-11   100.000000  0.500000 -13.271191         61.163120   \n",
      "2007-01-12   100.000000  0.500000 -18.112505         61.163120   \n",
      "2007-01-16    88.729109  0.500000 -28.581208         61.163120   \n",
      "2007-01-17    80.046771  0.500000 -33.498091         61.163120   \n",
      "2007-01-18    97.164158  0.500000 -14.764026         61.163120   \n",
      "2007-01-19    98.183784  0.500000 -19.501159         61.163120   \n",
      "2007-01-22    95.793827  0.500000 -20.583051         61.163120   \n",
      "2007-01-23    95.294481  0.500000 -29.375478         61.163120   \n",
      "2007-01-24    96.229215  0.466907 -36.963303         61.163120   \n",
      "2007-01-25    83.463610  0.542647 -23.812620         61.163120   \n",
      "2007-01-26    79.669686  0.580741 -27.828241         61.163120   \n",
      "2007-01-29    69.942468  0.511763 -46.722699         61.163120   \n",
      "2007-01-30    82.122159  0.467810 -49.389396         61.163120   \n",
      "2007-01-31    63.268723  0.525680 -32.439910         61.163120   \n",
      "2007-02-01   100.000000  0.495154 -22.052143         61.163120   \n",
      "2007-02-02   100.000000  0.479165 -41.685838         61.163120   \n",
      "2007-02-05   100.000000  0.559890 -43.324430         61.163120   \n",
      "2007-02-06   100.000000  0.675272 -22.967954         61.163120   \n",
      "2007-02-07   100.000000  0.681892 -16.551140         61.163120   \n",
      "2007-02-08    82.672656  0.700400 -27.415793         61.163120   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03        -18.689060      66.506361   14.199237  0.000000  \n",
      "2007-01-04        -17.909395      62.557735   12.780723  1.000000  \n",
      "2007-01-05        -17.909395      61.706631   12.546058  1.000000  \n",
      "2007-01-08        -17.821897      61.706631   12.679341  1.000000  \n",
      "2007-01-09        -17.693895      61.706631   12.748657  0.748425  \n",
      "2007-01-10        -17.693895      61.163120   12.246571  0.767440  \n",
      "2007-01-11        -17.369404      61.163120   13.157147  0.345134  \n",
      "2007-01-12        -17.030222      61.163120   13.594078  0.187500  \n",
      "2007-01-16        -17.022572      61.163120   13.545165  0.255814  \n",
      "2007-01-17        -17.022572      61.163120   13.102019  0.307821  \n",
      "2007-01-18        -17.022572      61.163120   13.815638  0.185518  \n",
      "2007-01-19        -17.022572      61.163120   13.412619  0.178409  \n",
      "2007-01-22        -17.022572      61.163120   13.807302  0.190545  \n",
      "2007-01-23        -17.022572      61.163120   13.491687  0.192928  \n",
      "2007-01-24        -17.022572      61.163120   13.457695  0.187681  \n",
      "2007-01-25        -17.022572      61.163120   13.333913  0.086274  \n",
      "2007-01-26        -17.022572      61.163120   12.980520  0.055239  \n",
      "2007-01-29        -17.022572      61.163120   12.844579  0.158997  \n",
      "2007-01-30        -17.022572      61.163120   12.961079  0.129870  \n",
      "2007-01-31        -17.022572      61.163120   13.223296  0.222225  \n",
      "2007-02-01        -17.022572      61.163120   13.484766  0.116164  \n",
      "2007-02-02        -17.000223      61.163120   13.664626  0.017443  \n",
      "2007-02-05        -17.000223      61.163120   13.626909  0.152323  \n",
      "2007-02-06        -16.877475      61.163120   14.027421  0.455701  \n",
      "2007-02-07        -16.723748      61.163120   14.460700  0.353385  \n",
      "2007-02-08        -16.723748      61.163120   14.037625  0.230766  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "BIIB\n",
      "                close   volume       open       high        low      cci_14  \\\n",
      "Date                                                                          \n",
      "2007-01-03  49.330002  3833500  49.279999  50.250000  48.200001         NaN   \n",
      "2007-01-04  49.750000  2884300  49.270000  50.099998  48.880001   66.666667   \n",
      "2007-01-05  49.759998  2230400  49.990002  50.330002  49.509998   97.111977   \n",
      "2007-01-08  50.020000  3160200  50.000000  50.160000  48.910000   35.474059   \n",
      "2007-01-09  49.500000  2816500  49.880001  50.139999  49.299999   16.288392   \n",
      "2007-01-10  49.279999  3161600  49.230000  49.549999  48.820000 -107.091180   \n",
      "2007-01-11  50.439999  2847900  49.340000  50.630001  49.099998  128.500386   \n",
      "2007-01-12  50.970001  3147800  50.360001  51.369999  50.360001  200.782685   \n",
      "2007-01-16  51.799999  2710000  51.200001  51.849998  50.500000  173.213448   \n",
      "2007-01-17  51.900002  3963000  51.480000  52.450001  51.410000  157.279760   \n",
      "2007-01-18  51.520000  2043900  51.910000  52.430000  51.369999  113.090620   \n",
      "2007-01-19  51.840000  2367200  51.439999  52.020000  51.180000   90.651291   \n",
      "2007-01-22  50.180000  3699500  51.700001  51.750000  50.110001   18.531811   \n",
      "2007-01-23  49.130001  4706700  50.189999  50.259998  48.529999  -78.750021   \n",
      "2007-01-24  48.790001  3545200  49.230000  49.299999  48.590000 -105.183199   \n",
      "2007-01-25  47.790001  4167600  48.500000  48.560001  47.439999 -150.491456   \n",
      "2007-01-26  47.520000  2816400  47.740002  47.939999  47.040001 -146.403328   \n",
      "2007-01-29  47.680000  2753400  47.430000  48.009998  47.230000 -115.248885   \n",
      "2007-01-30  48.049999  2927700  47.849998  48.209999  47.580002  -85.415261   \n",
      "2007-01-31  48.340000  2184800  47.860001  48.599998  47.320000  -71.071087   \n",
      "2007-02-01  49.389999  4495600  48.320000  49.599998  48.270000  -23.634285   \n",
      "2007-02-02  49.299999  2725200  49.330002  49.709999  49.189999   -5.501642   \n",
      "2007-02-05  49.029999  2073200  49.220001  49.320000  48.500000  -20.947521   \n",
      "2007-02-06  48.939999  3008600  49.040001  49.080002  47.860001  -30.697318   \n",
      "2007-02-07  49.320000  2026900  48.849998  49.480000  48.709999   19.933214   \n",
      "2007-02-08  49.639999  1993700  49.160000  49.889999  49.040001   72.419326   \n",
      "\n",
      "            stochrsi_14    mfi_14    bop_14  supertrend_14_ub  \\\n",
      "Date                                                            \n",
      "2007-01-03          NaN  0.500000  0.024392         55.374998   \n",
      "2007-01-04          NaN  0.500000  0.393443         54.348884   \n",
      "2007-01-05          NaN  0.500000 -0.280491         53.919324   \n",
      "2007-01-08          NaN  0.500000  0.016000         53.464903   \n",
      "2007-01-09     0.000000  0.500000 -0.452382         53.324661   \n",
      "2007-01-10     0.000000  0.500000  0.068492         52.508153   \n",
      "2007-01-11    50.038586  0.500000  0.718952         52.508153   \n",
      "2007-01-12    59.909388  0.500000  0.603962         52.508153   \n",
      "2007-01-16    69.928973  0.500000  0.444444         52.508153   \n",
      "2007-01-17    70.873497  0.500000  0.403848         52.508153   \n",
      "2007-01-18    53.978713  0.500000 -0.367923         52.508153   \n",
      "2007-01-19    58.287226  0.500000  0.476192         52.508153   \n",
      "2007-01-22    11.670228  0.500000 -0.926830         52.508153   \n",
      "2007-01-23     0.000000  0.500000 -0.612716         52.508153   \n",
      "2007-01-24     0.000000  0.414611 -0.619717         52.507887   \n",
      "2007-01-25     0.000000  0.340012 -0.633926         51.612989   \n",
      "2007-01-26     0.000000  0.287285 -0.244446         51.011946   \n",
      "2007-01-29     3.698197  0.349469  0.320513         51.011946   \n",
      "2007-01-30    12.059571  0.412345  0.317463         51.011946   \n",
      "2007-01-31    18.306656  0.470125  0.375000         51.011946   \n",
      "2007-02-01    37.458842  0.488464  0.804512         51.011946   \n",
      "2007-02-02    35.993426  0.482596 -0.057697         51.011946   \n",
      "2007-02-05    31.553939  0.427428 -0.231710         51.011946   \n",
      "2007-02-06    34.692882  0.342929 -0.081969         51.011946   \n",
      "2007-02-07    42.946648  0.390753  0.610391         51.011946   \n",
      "2007-02-08   100.000000  0.442051  0.564706         51.011946   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03         43.075003      55.374998    0.919998  0.000000  \n",
      "2007-01-04         44.631116      54.348884    0.713997  1.000000  \n",
      "2007-01-05         45.920676      53.919324    0.894134  1.000000  \n",
      "2007-01-08         45.920676      53.464903    0.646248  1.000000  \n",
      "2007-01-09         46.115338      53.324661    0.628081  0.140494  \n",
      "2007-01-10         46.115338      52.508153    0.069003  0.034967  \n",
      "2007-01-11         46.318272      52.508153    1.021138  0.428570  \n",
      "2007-01-12         47.400796      52.508153    1.579651  0.525640  \n",
      "2007-01-16         47.624831      52.508153    1.791697  0.625316  \n",
      "2007-01-17         48.438537      52.508153    2.146139  0.634567  \n",
      "2007-01-18         48.448445      52.508153    1.963987  0.494356  \n",
      "2007-01-19         48.448445      52.508153    1.370789  0.528420  \n",
      "2007-01-22         48.448445      52.508153    1.163350  0.132605  \n",
      "2007-01-23         48.448445      52.508153   -0.132432  0.026810  \n",
      "2007-01-24         48.448445      52.507887   -0.878774  0.069231  \n",
      "2007-01-25         48.448445      51.612989   -1.300269  0.233890  \n",
      "2007-01-26         43.968054      51.011946   -1.608235  0.259259  \n",
      "2007-01-29         44.212673      51.011946   -1.289138  0.274005  \n",
      "2007-01-30         44.631153      51.011946   -0.922586  0.172825  \n",
      "2007-01-31         44.642903      51.011946   -0.426908  0.111111  \n",
      "2007-02-01         45.556992      51.011946    0.524679  0.125748  \n",
      "2007-02-02         46.233477      51.011946    0.604723  0.211125  \n",
      "2007-02-05         46.233477      51.011946    0.224760  0.376871  \n",
      "2007-02-06         46.233477      51.011946    0.005461  0.403270  \n",
      "2007-02-07         46.233477      51.011946    0.372731  0.299728  \n",
      "2007-02-08         46.393245      51.011946    0.711700  0.299728  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "CBRE\n",
      "                close   volume       open       high        low      cci_14  \\\n",
      "Date                                                                          \n",
      "2007-01-03  33.650002  2079600  33.400002  33.830002  33.139999         NaN   \n",
      "2007-01-04  33.340000  1354900  33.810001  34.000000  33.099998  -66.666667   \n",
      "2007-01-05  33.099998  1374100  33.340000  33.599998  32.799999 -100.000000   \n",
      "2007-01-08  33.220001   960400  33.410000  33.410000  32.500000  -86.968148   \n",
      "2007-01-09  33.970001  1457300  33.380001  34.200001  33.060001  100.306946   \n",
      "2007-01-10  34.250000  2001800  33.970001  34.599998  33.529999  141.860057   \n",
      "2007-01-11  34.939999  1569300  34.200001  34.939999  33.770000  144.811390   \n",
      "2007-01-12  34.160000  2116800  34.840000  34.840000  33.400002   65.864343   \n",
      "2007-01-16  35.910000  2708600  34.349998  36.250000  34.160000  176.437912   \n",
      "2007-01-17  35.700001  1874400  35.750000  36.349998  35.599998  165.052766   \n",
      "2007-01-18  35.279999  1363900  35.950001  36.349998  35.020000  107.639306   \n",
      "2007-01-19  35.029999  1318400  35.439999  35.490002  34.869999   65.475679   \n",
      "2007-01-22  35.400002  1243300  35.270000  35.599998  35.000000   73.704116   \n",
      "2007-01-23  36.040001  1668600  35.500000  36.060001  35.150002   94.759587   \n",
      "2007-01-24  37.110001  2947300  36.250000  37.310001  36.009998  142.578131   \n",
      "2007-01-25  35.380001  2154700  37.509998  37.520000  35.230000   79.346662   \n",
      "2007-01-26  35.630001  2255000  35.200001  35.680000  34.900002   27.337166   \n",
      "2007-01-29  37.009998  2421000  35.820000  37.160000  35.820000  126.226855   \n",
      "2007-01-30  36.750000  1213700  37.009998  37.220001  36.259998  120.419342   \n",
      "2007-01-31  37.610001  2028200  36.840000  37.840000  36.840000  156.964003   \n",
      "2007-02-01  37.730000  2003400  37.950001  38.000000  37.020000  139.598825   \n",
      "2007-02-02  38.689999  1525100  37.860001  38.799999  37.790001  169.388466   \n",
      "2007-02-05  38.419998  1558600  38.310001  38.730000  37.939999  133.952158   \n",
      "2007-02-06  38.480000  2146900  38.639999  39.150002  37.529999  114.379575   \n",
      "2007-02-07  38.369999  3448700  37.799999  38.880001  37.099998   82.095559   \n",
      "2007-02-08  38.430000  1572800  38.369999  38.770000  38.040001   87.404635   \n",
      "\n",
      "            stochrsi_14    mfi_14    bop_14  supertrend_14_ub  \\\n",
      "Date                                                            \n",
      "2007-01-03          NaN  0.500000  0.362318         35.555008   \n",
      "2007-01-04          NaN  0.500000 -0.522223         35.555008   \n",
      "2007-01-05          NaN  0.500000 -0.300002         35.555008   \n",
      "2007-01-08   100.000000  0.500000 -0.208790         35.445343   \n",
      "2007-01-09   100.000000  0.500000  0.517544         35.445343   \n",
      "2007-01-10   100.000000  0.500000  0.261681         35.445343   \n",
      "2007-01-11   100.000000  0.500000  0.632478         35.445343   \n",
      "2007-01-12    71.284128  0.500000 -0.472223         35.445343   \n",
      "2007-01-16    92.543300  0.500000  0.746412         35.445343   \n",
      "2007-01-17    87.890769  0.500000 -0.066666         39.434121   \n",
      "2007-01-18    79.303497  0.500000 -0.503762         39.212146   \n",
      "2007-01-19    74.629425  0.500000 -0.661287         38.504990   \n",
      "2007-01-22    78.798956  0.500000  0.216669         38.448846   \n",
      "2007-01-23    84.919764  0.500000  0.593408         38.448846   \n",
      "2007-01-24    92.530687  0.653752  0.661537         38.448846   \n",
      "2007-01-25    68.727936  0.629454 -0.930129         38.448846   \n",
      "2007-01-26    61.495534  0.605580  0.551283         38.448846   \n",
      "2007-01-29    37.880844  0.662047  0.888059         38.448846   \n",
      "2007-01-30    28.500544  0.660428 -0.270831         38.448846   \n",
      "2007-01-31    45.219140  0.663107  0.770000         38.448846   \n",
      "2007-02-01    62.248091  0.670319 -0.224491         38.448846   \n",
      "2007-02-02    83.461248  0.740111  0.821782         38.448846   \n",
      "2007-02-05    69.667275  0.666067  0.139236         41.648565   \n",
      "2007-02-06    71.070376  0.671395 -0.098765         41.648565   \n",
      "2007-02-07    64.913210  0.617456  0.320224         41.596877   \n",
      "2007-02-08    66.544025  0.666854  0.082194         41.596877   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03         31.414993      35.555008    0.180000  0.000000  \n",
      "2007-01-04         31.414993      35.555008    0.391332  1.000000  \n",
      "2007-01-05         31.414993      35.555008    0.059153  1.000000  \n",
      "2007-01-08         31.414993      35.445343   -0.088066  0.641786  \n",
      "2007-01-09         31.414993      35.445343    0.639010  0.225351  \n",
      "2007-01-10         31.414993      35.445343    0.947140  0.352939  \n",
      "2007-01-11         31.425310      35.445343    1.115521  0.539747  \n",
      "2007-01-12         31.425310      35.445343    0.970786  0.160883  \n",
      "2007-01-16         31.554790      31.554790    2.108681  0.459349  \n",
      "2007-01-17         32.515875      32.515875    2.000856  0.399610  \n",
      "2007-01-18         32.515875      32.515875    1.876742  0.293693  \n",
      "2007-01-19         32.515875      32.515875    0.942512  0.237930  \n",
      "2007-01-22         32.515875      32.515875    0.938841  0.283630  \n",
      "2007-01-23         32.515875      32.515875    1.214998  0.350954  \n",
      "2007-01-24         33.472591      33.472591    2.162998  0.439086  \n",
      "2007-01-25         33.472591      33.472591    2.341931  0.219355  \n",
      "2007-01-26         33.472591      33.472591    0.441673  0.271751  \n",
      "2007-01-29         33.472591      33.472591    1.685450  0.358562  \n",
      "2007-01-30         33.472591      33.472591    1.575391  0.275794  \n",
      "2007-01-31         33.869233      33.869233    1.933338  0.315197  \n",
      "2007-02-01         34.087279      34.087279    1.850226  0.276512  \n",
      "2007-02-02         34.891174      34.891174    2.311529  0.441091  \n",
      "2007-02-05         35.021433      35.021433    1.983992  0.285552  \n",
      "2007-02-06         35.021433      35.021433    2.172795  0.321759  \n",
      "2007-02-07         35.021433      35.021433    1.717089  0.370948  \n",
      "2007-02-08         35.021433      35.021433    1.438143  0.417691  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "DIS\n",
      "                close    volume       open       high        low      cci_14  \\\n",
      "Date                                                                           \n",
      "2007-01-03  28.317097  13562595  33.748165  34.073711  33.531136         NaN   \n",
      "2007-01-04  28.540649   9806285  33.738300  34.083576  33.708706   66.666667   \n",
      "2007-01-05  28.308817  10551445  33.807354  33.975060  33.531136  -67.267333   \n",
      "2007-01-08  28.565493   9479676  33.728436  34.162495  33.610054   67.396102   \n",
      "2007-01-09  28.524090  11588444  34.034248  34.221684  33.491676   34.655583   \n",
      "2007-01-10  28.557213   7305525  33.688976  34.083576  33.521271   10.329688   \n",
      "2007-01-11  28.971203  12361379  34.123035  34.557095  33.984924  231.778489   \n",
      "2007-01-12  29.153364  12022301  34.517635  34.813583  34.379524  174.589810   \n",
      "2007-01-16  29.004318  11147897  34.537365  34.675476  34.418983  109.123593   \n",
      "2007-01-17  29.186478   9946173  34.527500  34.823448  34.428848  107.330221   \n",
      "2007-01-18  29.683273  16723872  34.823448  35.444946  34.774124  156.730061   \n",
      "2007-01-19  29.393473  12621186  35.444946  35.484406  35.001019  128.210026   \n",
      "2007-01-22  29.103683   8841662  34.971424  35.001019  34.576824   61.236083   \n",
      "2007-01-23  29.459713  11278865  34.547230  35.158859  34.527500   77.889991   \n",
      "2007-01-24  29.269278  10761683  35.099670  35.267376  34.793854   76.947902   \n",
      "2007-01-25  28.929808   9584693  34.872776  34.941830  34.409119   15.679473   \n",
      "2007-01-26  28.606892  10533604  34.527500  34.596554  34.004654  -59.285898   \n",
      "2007-01-29  28.714525  11190370  34.034248  34.596554  33.975060  -63.441524   \n",
      "2007-01-30  29.012604   6639736  34.172359  34.616283  34.152630  -41.623722   \n",
      "2007-01-31  29.120239   9058895  34.665611  34.783989  34.537365   -1.086116   \n",
      "2007-02-01  28.987761   8158236  34.616283  34.675476  34.418983  -42.862061   \n",
      "2007-02-02  29.128519   7773239  34.547230  34.902370  34.527500    5.259395   \n",
      "2007-02-05  29.194765   8992296  34.922100  35.010883  34.675476   33.349512   \n",
      "2007-02-06  29.136805   7906944  34.931965  34.981289  34.606419   14.667366   \n",
      "2007-02-07  29.376919  12159250  34.872776  35.109535  34.724800   76.377185   \n",
      "2007-02-08  29.219595  20736442  35.553459  35.602783  34.478176   96.949889   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -10.009787         51.072268   \n",
      "2007-01-04          NaN  0.500000 -13.865198         51.072268   \n",
      "2007-01-05     0.000000  0.500000 -12.386214         50.686433   \n",
      "2007-01-08    38.614928  0.500000  -9.345697         50.686433   \n",
      "2007-01-09    30.558797  0.500000  -7.548088         50.686433   \n",
      "2007-01-10    34.126463  0.500000  -9.126292         50.686433   \n",
      "2007-01-11    61.056854  0.500000  -9.004020         50.686433   \n",
      "2007-01-12    67.376750  0.500000 -12.358388         50.686433   \n",
      "2007-01-16    47.745233  0.500000 -21.571953         50.686433   \n",
      "2007-01-17    55.133181  0.500000 -13.535285         50.686433   \n",
      "2007-01-18    68.297582  0.500000  -7.662500         50.686433   \n",
      "2007-01-19    43.732167  0.500000 -12.518900         50.686433   \n",
      "2007-01-22    24.601670  0.500000 -13.832671         50.686433   \n",
      "2007-01-23    36.635846  0.500000  -8.058040         50.686433   \n",
      "2007-01-24    26.012007  0.599989 -12.312819         50.686433   \n",
      "2007-01-25    14.544161  0.537681 -11.156083         50.686433   \n",
      "2007-01-26     0.000000  0.537221 -10.002720         50.686433   \n",
      "2007-01-29     6.206362  0.542518  -8.559569         50.686433   \n",
      "2007-01-30    21.914774  0.603230 -11.128472         50.686433   \n",
      "2007-01-31    27.102329  0.654909 -22.485128         50.686433   \n",
      "2007-02-01    19.147193  0.590708 -21.944191         50.686433   \n",
      "2007-02-02    26.330286  0.578733 -14.454895         50.686433   \n",
      "2007-02-05    29.666164  0.650586 -17.075763         50.686433   \n",
      "2007-02-06    25.719790  0.589096 -15.459108         50.686433   \n",
      "2007-02-07    58.248777  0.574811 -14.284782         50.686433   \n",
      "2007-02-08    49.002874  0.687839  -5.632069         50.686433   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03         16.532579      51.072268    5.756615  0.000000  \n",
      "2007-01-04         16.610952      51.072268    5.736672  1.000000  \n",
      "2007-01-05         16.819762      50.686433    5.633234  0.018182  \n",
      "2007-01-08         16.819762      50.686433    5.790847  0.348841  \n",
      "2007-01-09         16.819762      50.686433    5.829710  0.274722  \n",
      "2007-01-10         16.819762      50.686433    5.669571  0.305263  \n",
      "2007-01-11         17.097203      50.686433    6.068796  0.544827  \n",
      "2007-01-12         17.366319      50.686433    6.236610  0.604791  \n",
      "2007-01-16         17.414421      50.686433    6.041523  0.448641  \n",
      "2007-01-17         17.449044      50.686433    6.115825  0.507240  \n",
      "2007-01-18         17.727631      50.686433    6.607237  0.617973  \n",
      "2007-01-19         17.858202      50.686433    6.572594  0.430457  \n",
      "2007-01-22         17.858202      50.686433    6.063624  0.281897  \n",
      "2007-01-23         17.858202      50.686433    6.151822  0.363155  \n",
      "2007-01-24         17.858202      50.686433    6.225374  0.285358  \n",
      "2007-01-25         17.858202      50.686433    5.914787  0.112711  \n",
      "2007-01-26         17.858202      50.686433    5.625531  0.084112  \n",
      "2007-01-29         17.858202      50.686433    5.659731  0.043901  \n",
      "2007-01-30         17.858202      50.686433    5.669356  0.133787  \n",
      "2007-01-31         17.858202      50.686433    5.813953  0.151110  \n",
      "2007-02-01         17.858202      50.686433    5.703077  0.004807  \n",
      "2007-02-02         17.858202      50.686433    5.909156  0.007301  \n",
      "2007-02-05         17.858202      50.686433    5.990795  0.057360  \n",
      "2007-02-06         17.858202      50.686433    5.945639  0.015542  \n",
      "2007-02-07         17.858202      50.686433    6.028382  0.104225  \n",
      "2007-02-08         17.858202      50.686433    6.503171  0.061947  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "LUV\n",
      "                close    volume       open       high        low      cci_14  \\\n",
      "Date                                                                           \n",
      "2007-01-03  14.001652   7007400  15.460000  15.780000  15.350000         NaN   \n",
      "2007-01-04  14.218172   7568700  15.500000  15.850000  15.420000   66.666667   \n",
      "2007-01-05  13.983607   5059700  15.800000  15.850000  15.420000  -19.051207   \n",
      "2007-01-08  14.236216   6256800  15.480000  15.800000  15.340000   35.511067   \n",
      "2007-01-09  14.263284   7097100  15.890000  15.980000  15.700000  151.756629   \n",
      "2007-01-10  14.479798   9463100  16.000000  16.049999  15.800000  135.174859   \n",
      "2007-01-11  14.570018  11504200  16.010000  16.150000  16.000000  129.755060   \n",
      "2007-01-12  14.633170  10524100  16.129999  16.280001  16.010000  117.421872   \n",
      "2007-01-16  14.948925  11919300  16.280001  16.580000  16.219999  149.060550   \n",
      "2007-01-17  14.344476  17090700  16.139999  16.299999  15.850000   33.558382   \n",
      "2007-01-18  14.290342   8469900  15.960000  16.180000  15.820000   13.912998   \n",
      "2007-01-19  14.353494   5599000  15.850000  15.940000  15.680000  -18.084902   \n",
      "2007-01-22  13.992632   9120700  15.900000  15.920000  15.360000  -85.184528   \n",
      "2007-01-23  13.866324  13407900  15.340000  15.420000  15.250000 -140.485766   \n",
      "2007-01-24  13.794150   9347200  15.330000  15.370000  15.250000 -136.249376   \n",
      "2007-01-25  13.649807   6466100  15.260000  15.340000  15.100000 -135.553681   \n",
      "2007-01-26  13.604692  12677000  15.070000  15.120000  14.950000 -138.304240   \n",
      "2007-01-29  13.658822   9112600  15.000000  15.300000  15.000000 -100.106290   \n",
      "2007-01-30  13.658822   8063500  15.130000  15.200000  15.010000  -88.536857   \n",
      "2007-01-31  13.622744   7734900  15.030000  15.170000  14.960000  -82.439613   \n",
      "2007-02-01  13.848280   9809500  15.120000  15.380000  15.000000  -49.041181   \n",
      "2007-02-02  13.767090   5894400  15.390000  15.470000  15.230000  -28.773131   \n",
      "2007-02-05  13.559590   7286100  15.270000  15.320000  15.020000  -62.793263   \n",
      "2007-02-06  13.721979   7018700  15.030000  15.250000  15.020000  -51.254549   \n",
      "2007-02-07  13.649807   6945300  15.190000  15.200000  15.030000  -58.690494   \n",
      "2007-02-08  13.676869   6835400  15.140000  15.300000  15.090000  -17.919543   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000  -3.391513         20.900044   \n",
      "2007-01-04          NaN  0.500000  -2.980993         20.900044   \n",
      "2007-01-05     0.000000  0.500000  -4.224167         20.882415   \n",
      "2007-01-08    38.442199  0.500000  -2.703878         20.873593   \n",
      "2007-01-09    41.056985  0.500000  -5.809708         20.873593   \n",
      "2007-01-10    56.846862  0.500000  -6.080830         20.873593   \n",
      "2007-01-11    61.477672  0.500000  -9.599907         20.873593   \n",
      "2007-01-12    64.360689  0.500000  -5.543802         20.873593   \n",
      "2007-01-16    74.597367  0.500000  -3.697426         20.873593   \n",
      "2007-01-17    14.975302  0.500000  -3.990063         20.873593   \n",
      "2007-01-18    11.488633  0.500000  -4.637930         20.873593   \n",
      "2007-01-19    15.178705  0.500000  -5.755811         20.873593   \n",
      "2007-01-22     0.000000  0.500000  -3.406011         20.750209   \n",
      "2007-01-23     0.000000  0.500000  -8.668677         20.353598   \n",
      "2007-01-24     0.000000  0.491368 -12.798759         20.274564   \n",
      "2007-01-25     0.000000  0.439115  -6.709145         20.150930   \n",
      "2007-01-26     0.000000  0.416687  -8.619450         19.914041   \n",
      "2007-01-29     4.293111  0.427199  -4.470590         19.914041   \n",
      "2007-01-30     4.293111  0.375227  -7.743059         19.914041   \n",
      "2007-01-31     2.495344  0.312598  -6.701218         19.906667   \n",
      "2007-02-01    20.696977  0.301288  -3.346630         19.906667   \n",
      "2007-02-02    16.026060  0.274597  -6.762107         19.906667   \n",
      "2007-02-05    14.116913  0.188519  -5.701382         19.906667   \n",
      "2007-02-06    44.948655  0.263623  -5.687057         19.906667   \n",
      "2007-02-07    35.242745  0.268013  -9.059952         19.906667   \n",
      "2007-02-08    75.261390  0.322762  -6.967289         19.906667   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03         10.229956      20.900044    1.778348  0.000000  \n",
      "2007-01-04         10.229956      20.900044    1.819479  1.000000  \n",
      "2007-01-05         10.387586      20.882415    1.825734  0.040002  \n",
      "2007-01-08         10.387586      20.873593    1.747474  0.333332  \n",
      "2007-01-09         10.553071      20.873593    1.899373  0.358026  \n",
      "2007-01-10         10.623501      20.873593    1.916149  0.504759  \n",
      "2007-01-11         10.824839      20.873593    1.957994  0.547825  \n",
      "2007-01-12         10.914038      20.873593    2.029173  0.573770  \n",
      "2007-01-16         11.079589      20.873593    2.236093  0.668788  \n",
      "2007-01-17         11.079589      20.873593    1.956016  0.169643  \n",
      "2007-01-18         11.079589      20.873593    1.843169  0.139129  \n",
      "2007-01-19         11.079589      20.873593    1.600947  0.164555  \n",
      "2007-01-22         11.079589      20.750209    1.627137  0.003609  \n",
      "2007-01-23         11.079589      20.353598    1.184009  0.051547  \n",
      "2007-01-24         11.079589      20.274564    1.192921  0.076924  \n",
      "2007-01-25         11.079589      20.150930    1.233224  0.216495  \n",
      "2007-01-26         11.079589      19.914041    1.080168  0.155557  \n",
      "2007-01-29         11.079589      19.914041    1.310970  0.258067  \n",
      "2007-01-30         11.079589      19.914041    1.254997  0.273474  \n",
      "2007-01-31         11.079589      19.906667    1.267965  0.422221  \n",
      "2007-02-01         11.079589      19.906667    1.485133  0.333337  \n",
      "2007-02-02         11.079589      19.906667    1.592170  0.396698  \n",
      "2007-02-05         11.079589      19.906667    1.484601  0.669569  \n",
      "2007-02-06         11.079589      19.906667    1.429724  0.381219  \n",
      "2007-02-07         11.079589      19.906667    1.402453  0.387980  \n",
      "2007-02-08         11.079589      19.906667    1.518544  0.418999  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "PEP\n",
      "                close    volume       open       high        low      cci_14  \\\n",
      "Date                                                                           \n",
      "2007-01-03  38.873489   6161600  62.700001  63.349998  62.450001         NaN   \n",
      "2007-01-04  39.139996   5414300  62.700001  63.250000  62.500000   66.666667   \n",
      "2007-01-05  39.016041   4542400  62.700001  63.220001  62.700001   64.417955   \n",
      "2007-01-08  39.102814   6122600  63.000000  63.270000  62.860001  132.030625   \n",
      "2007-01-09  39.263954   4916500  63.049999  63.470001  63.009998  137.508078   \n",
      "2007-01-10  39.759792   6427100  63.000000  64.290001  62.290001  140.303479   \n",
      "2007-01-11  40.255638   6909300  64.199997  65.059998  64.199997  208.233949   \n",
      "2007-01-12  40.100681   4982100  64.779999  65.250000  64.589996  141.870675   \n",
      "2007-01-16  40.193642   3517700  64.500000  64.989998  64.500000   98.990721   \n",
      "2007-01-17  40.286613   4494400  64.669998  65.000000  64.559998   86.457763   \n",
      "2007-01-18  40.237019   4144300  64.410004  65.540001  64.220001   79.325680   \n",
      "2007-01-19  40.175049   5333700  65.010002  65.010002  64.669998   68.105782   \n",
      "2007-01-22  40.168854   5546700  64.629997  65.129997  64.540001   62.731321   \n",
      "2007-01-23  40.187443   3860200  64.709999  65.110001  64.650002   61.922381   \n",
      "2007-01-24  40.410576   3586600  64.739998  65.199997  64.500000   60.078485   \n",
      "2007-01-25  39.927135   3750500  65.070000  65.070000  64.330002   28.223279   \n",
      "2007-01-26  39.945736   8566800  64.449997  64.809998  64.400002   12.968879   \n",
      "2007-01-29  40.038708   4451200  64.449997  64.849998  64.400002    7.658595   \n",
      "2007-01-30  40.342400   5131600  64.680000  65.209999  64.400002   56.850760   \n",
      "2007-01-31  40.435371   4277000  65.099998  65.360001  64.879997  204.736734   \n",
      "2007-02-01  40.522137   4382800  65.000000  65.389999  64.680000  159.223023   \n",
      "2007-02-02  40.311413   3097800  64.650002  65.300003  64.639999   73.539804   \n",
      "2007-02-05  40.181255   3646800  65.000000  65.000000  64.440002  -54.606170   \n",
      "2007-02-06  39.908531   4129800  64.839996  64.940002  64.360001 -112.991982   \n",
      "2007-02-07  39.976723   4323100  64.400002  64.699997  64.220001 -130.967245   \n",
      "2007-02-08  39.239170  13901100  63.250000  64.110001  62.799999 -314.064599   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -26.473969        136.329527   \n",
      "2007-01-04          NaN  0.500000 -31.413340        136.148974   \n",
      "2007-01-05     0.000000  0.500000 -45.546037        135.863490   \n",
      "2007-01-08    20.101801  0.500000 -58.285842        135.863490   \n",
      "2007-01-09    43.011795  0.500000 -51.708468        135.863490   \n",
      "2007-01-10    70.778048  0.500000 -11.620104        135.863490   \n",
      "2007-01-11    80.834486  0.500000 -27.842258        135.863490   \n",
      "2007-01-12    51.721196  0.500000 -37.392698        135.863490   \n",
      "2007-01-16    54.755290  0.500000 -49.605029        135.863490   \n",
      "2007-01-17    57.623654  0.500000 -55.416476        135.863490   \n",
      "2007-01-18    48.583234  0.500000 -18.312872        135.863490   \n",
      "2007-01-19    37.375560  0.500000 -73.043128        135.863490   \n",
      "2007-01-22    36.229003  0.500000 -41.459823        135.863490   \n",
      "2007-01-23    37.210504  0.500000 -53.310011        135.863490   \n",
      "2007-01-24    47.629910  0.791647 -34.756469        135.863490   \n",
      "2007-01-25     0.000000  0.731634 -33.976942        135.863490   \n",
      "2007-01-26     1.357643  0.628051 -59.767069        135.863490   \n",
      "2007-01-29     8.163248  0.619996 -54.247677        135.863490   \n",
      "2007-01-30    26.895596  0.621812 -30.046511        135.863490   \n",
      "2007-01-31    31.753960  0.610792 -51.384280        135.863490   \n",
      "2007-02-01    46.763135  0.529411 -34.475908        135.863490   \n",
      "2007-02-02    14.998836  0.467207 -36.876445        135.863490   \n",
      "2007-02-05     0.000000  0.466279 -44.319380        135.863490   \n",
      "2007-02-06     0.000000  0.398704 -42.985149        135.863490   \n",
      "2007-02-07     5.615039  0.332990 -50.882284        135.863490   \n",
      "2007-02-08     0.000000  0.294927 -18.328859        135.863490   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03        -10.529528     136.329527   24.476509  0.000000  \n",
      "2007-01-04        -10.398974     136.148974   24.340976  1.000000  \n",
      "2007-01-05         -9.943488     135.863490   24.296709  0.365085  \n",
      "2007-01-08         -9.799059     135.863490   24.322772  0.480528  \n",
      "2007-01-09         -9.678848     135.863490   24.480542  0.611655  \n",
      "2007-01-10         -9.678848     135.863490   25.197831  0.781426  \n",
      "2007-01-11         -9.168927     135.863490   25.812699  0.847914  \n",
      "2007-01-12         -9.068041     135.863490   25.888917  0.687497  \n",
      "2007-01-16         -9.068041     135.863490   25.517907  0.702966  \n",
      "2007-01-17         -9.068041     135.863490   25.419306  0.716977  \n",
      "2007-01-18         -9.068041     135.863490   25.871797  0.674833  \n",
      "2007-01-19         -9.068041     135.863490   25.274219  0.624995  \n",
      "2007-01-22         -9.068041     135.863490   25.336471  0.620175  \n",
      "2007-01-23         -9.068041     135.863490   25.263952  0.623526  \n",
      "2007-01-24         -9.068041     135.863490   25.278678  0.659572  \n",
      "2007-01-25         -9.068041     135.863490   25.147906  0.309002  \n",
      "2007-01-26         -9.068041     135.863490   24.884751  0.380709  \n",
      "2007-01-29         -9.068041     135.863490   24.909624  0.382277  \n",
      "2007-01-30         -9.068041     135.863490   25.216021  0.416266  \n",
      "2007-01-31         -9.068041     135.863490   25.307170  0.308781  \n",
      "2007-02-01         -9.068041     135.863490   25.274595  0.149818  \n",
      "2007-02-02         -9.068041     135.863490   25.158464  0.114866  \n",
      "2007-02-05         -9.068041     135.863490   24.853165  0.006617  \n",
      "2007-02-06         -9.068041     135.863490   24.824942  0.184293  \n",
      "2007-02-07         -9.068041     135.863490   24.603381  0.125739  \n",
      "2007-02-08         -9.068041     135.863490   24.127711  0.340851  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "PGR\n",
      "                close   volume       open       high        low      cci_14  \\\n",
      "Date                                                                          \n",
      "2007-01-03  13.782968  3855600  24.500000  24.750000  24.070000         NaN   \n",
      "2007-01-04  13.794351  3071000  24.270000  24.360001  24.049999  -66.666667   \n",
      "2007-01-05  13.521085  2676800  24.150000  24.160000  23.600000 -100.000000   \n",
      "2007-01-08  13.612178  2389200  23.780001  23.910000  23.540001  -78.511991   \n",
      "2007-01-09  13.378761  2614000  24.000000  24.000000  23.440001  -79.764156   \n",
      "2007-01-10  13.344604  2046700  23.480000  23.600000  23.400000 -102.264490   \n",
      "2007-01-11  13.207967  4182600  23.379999  23.420000  23.129999 -123.825409   \n",
      "2007-01-12  13.321828  3325700  23.080000  23.490000  23.049999  -94.639299   \n",
      "2007-01-16  13.310443  2658200  23.500000  23.700001  23.350000  -48.592996   \n",
      "2007-01-17  13.407223  3529900  23.750000  23.850000  23.459999  -17.288730   \n",
      "2007-01-18  13.373065  4049200  23.480000  23.600000  23.090000  -72.546050   \n",
      "2007-01-19  13.253509  3281700  23.600000  23.660000  23.170000  -65.453586   \n",
      "2007-01-22  13.122573  2918600  23.170000  23.299999  22.900000 -120.742097   \n",
      "2007-01-23  13.253509  2556700  22.990000  23.370001  22.950001  -89.305263   \n",
      "2007-01-24  13.293361  2443500  23.240000  23.350000  23.150000  -64.420342   \n",
      "2007-01-25  13.333214  4381500  23.350000  23.480000  23.299999  -16.170773   \n",
      "2007-01-26  13.338905  3472900  23.410000  23.540001  23.270000    0.819230   \n",
      "2007-01-29  13.390147  3571900  23.420000  23.900000  23.420000  115.636639   \n",
      "2007-01-30  13.236431  3089900  23.559999  23.590000  23.219999   -4.052391   \n",
      "2007-01-31  13.202273  4103200  23.160000  23.270000  22.920000 -123.865113   \n",
      "2007-02-01  13.475539  5030800  23.340000  23.760000  23.240000   87.128538   \n",
      "2007-02-02  13.418609  3319400  23.620001  23.690001  23.530001  102.286530   \n",
      "2007-02-05  13.452768  3331400  23.490000  23.719999  23.450001   90.521845   \n",
      "2007-02-06  13.475539  3300100  23.600000  23.760000  23.600000  124.171042   \n",
      "2007-02-07  13.412917  2673600  23.660000  23.750000  23.540001   83.932691   \n",
      "2007-02-08  13.373065  2591800  23.590000  23.680000  23.459999   46.865566   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -15.760335         57.311097   \n",
      "2007-01-04          NaN  0.500000 -33.792269         56.499432   \n",
      "2007-01-05     0.000000  0.500000 -18.980223         55.745352   \n",
      "2007-01-08    25.684987  0.500000 -27.480682         55.395834   \n",
      "2007-01-09    13.426796  0.500000 -18.966516         55.273792   \n",
      "2007-01-10    12.365658  0.500000 -50.676782         54.876673   \n",
      "2007-01-11     8.971805  0.500000 -35.075863         54.448635   \n",
      "2007-01-12    23.352045  0.500000 -22.177637         54.391326   \n",
      "2007-01-16    22.896748  0.500000 -29.112988         54.391326   \n",
      "2007-01-17    33.135336  0.500000 -26.519853         54.391326   \n",
      "2007-01-18    31.357413  0.500000 -19.817510         54.391326   \n",
      "2007-01-19    25.956012  0.500000 -21.115299         54.391326   \n",
      "2007-01-22    21.388378  0.500000 -25.118592         54.073025   \n",
      "2007-01-23    32.518376  0.500000 -23.182118         54.073025   \n",
      "2007-01-24    35.511133  0.424523 -49.733006         54.073025   \n",
      "2007-01-25   100.000000  0.514303 -55.648720         54.073025   \n",
      "2007-01-26   100.000000  0.584786 -37.300287         54.073025   \n",
      "2007-01-29   100.000000  0.649556 -20.895547         54.073025   \n",
      "2007-01-30    76.588075  0.643246 -27.901473         54.073025   \n",
      "2007-01-31    72.117297  0.616158 -28.450616         53.900187   \n",
      "2007-02-01   100.000000  0.709262 -18.970101         53.900187   \n",
      "2007-02-02    89.958308  0.709482 -63.758762         53.900187   \n",
      "2007-02-05    96.093079  0.645155 -37.175131         53.900187   \n",
      "2007-02-06   100.000000  0.643518 -63.277943         53.900187   \n",
      "2007-02-07    87.834724  0.661738 -48.795845         53.900187   \n",
      "2007-02-08    80.369071  0.601226 -46.440357         53.900187   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03         -8.491097      57.311097   10.967032  0.000000  \n",
      "2007-01-04         -8.089432      56.499432   10.575515  1.000000  \n",
      "2007-01-05         -7.985351      55.745352   10.410635  0.920020  \n",
      "2007-01-08         -7.945833      55.395834   10.178926  0.454540  \n",
      "2007-01-09         -7.833792      55.273792   10.315901  0.663549  \n",
      "2007-01-10         -7.833792      54.876673    9.961168  0.681412  \n",
      "2007-01-11         -7.833792      54.448635    9.838616  0.737225  \n",
      "2007-01-12         -7.833792      54.391326    9.943223  0.515923  \n",
      "2007-01-16         -7.598262      54.391326   10.184735  0.522012  \n",
      "2007-01-17         -7.535871      54.391326   10.349141  0.375003  \n",
      "2007-01-18         -7.535871      54.391326   10.116180  0.395607  \n",
      "2007-01-19         -7.535871      54.391326   10.206888  0.458131  \n",
      "2007-01-22         -7.535871      54.073025    9.890959  0.513275  \n",
      "2007-01-23         -7.535871      54.073025    9.981698  0.373499  \n",
      "2007-01-24         -7.535871      54.073025    9.974357  0.335942  \n",
      "2007-01-25         -7.454790      54.073025   10.110013  0.310345  \n",
      "2007-01-26         -7.417409      54.073025   10.174159  0.149534  \n",
      "2007-01-29         -7.245894      54.073025   10.530917  0.188407  \n",
      "2007-01-30         -7.245894      54.073025   10.238604  0.129537  \n",
      "2007-01-31         -7.245894      53.900187    9.938487  0.129538  \n",
      "2007-02-01         -7.245894      53.900187   10.409284  0.216589  \n",
      "2007-02-02         -7.245894      53.900187   10.330232  0.082125  \n",
      "2007-02-05         -7.245894      53.900187   10.347831  0.118483  \n",
      "2007-02-06         -7.190836      53.900187   10.374049  0.060606  \n",
      "2007-02-07         -7.190836      53.900187   10.360453  0.034483  \n",
      "2007-02-08         -7.190836      53.900187   10.292651  0.111113  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "SWK\n",
      "                close   volume       open       high        low      cci_14  \\\n",
      "Date                                                                          \n",
      "2007-01-03  34.064186   666400  50.450001  50.799999  50.049999         NaN   \n",
      "2007-01-04  34.278854   753200  50.750000  51.290001  50.259998   66.666667   \n",
      "2007-01-05  34.144691  1146400  51.000000  51.189999  50.590000   57.467112   \n",
      "2007-01-08  34.017223   750800  50.700001  50.860001  49.950001  -72.429027   \n",
      "2007-01-09  34.634384  1000100  50.959999  51.730000  50.680000  133.040292   \n",
      "2007-01-10  35.036865   630600  51.549999  52.270000  51.389999  149.636254   \n",
      "2007-01-11  35.533268   461800  52.480000  52.990002  52.240002  153.615162   \n",
      "2007-01-12  35.700985   490800  52.720001  53.279999  52.549999  125.828465   \n",
      "2007-01-16  35.727810   434800  53.130001  53.500000  52.950001  111.789854   \n",
      "2007-01-17  35.902233   789800  53.099998  53.970001  53.099998  108.789490   \n",
      "2007-01-18  36.264465   918600  52.730000  54.849998  52.730000  108.583568   \n",
      "2007-01-19  35.613800  1044400  52.500000  53.279999  52.439999   49.259680   \n",
      "2007-01-22  35.727810   571700  53.090000  53.380001  52.520000   52.518763   \n",
      "2007-01-23  35.875401   481400  53.130001  53.700001  53.020000   69.630699   \n",
      "2007-01-24  36.143723   436800  53.380001  53.990002  53.160000   78.407791   \n",
      "2007-01-25  35.452785   785000  54.000000  54.099998  52.680000   46.537379   \n",
      "2007-01-26  37.277409  4553300  54.250000  56.250000  54.220001  198.725471   \n",
      "2007-01-29  37.257286  1443000  55.299999  56.000000  55.090000  194.848322   \n",
      "2007-01-30  37.639652  1305900  55.250000  56.500000  55.250000  176.571471   \n",
      "2007-01-31  38.411106  1189200  56.110001  57.540001  55.779999  172.768269   \n",
      "2007-02-01  39.001415  1650700  57.500000  58.509998  57.490002  175.617516   \n",
      "2007-02-02  39.544781  1043400  58.299999  58.990002  58.070000  155.771385   \n",
      "2007-02-05  38.706257   870500  58.619999  58.830002  57.549999  110.491651   \n",
      "2007-02-06  39.021538   698100  57.750000  58.169998  57.330002   88.171873   \n",
      "2007-02-07  38.967873   482000  58.349998  58.410000  57.660000   81.658331   \n",
      "2007-02-08  38.806885   539000  58.090000  58.090000  57.389999   62.168243   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -21.847753        100.632439   \n",
      "2007-01-04          NaN  0.500000 -15.991363        100.632439   \n",
      "2007-01-05     0.000000  0.500000 -28.092252        100.632439   \n",
      "2007-01-08     0.000000  0.500000 -18.332725        100.632439   \n",
      "2007-01-09    60.323762  0.500000 -15.548216        100.632439   \n",
      "2007-01-10    72.130872  0.500000 -18.764902        100.632439   \n",
      "2007-01-11    80.025901  0.500000 -22.595642        100.632439   \n",
      "2007-01-12    81.892365  0.500000 -23.313736        100.632439   \n",
      "2007-01-16    82.179196  0.500000 -31.640391        100.632439   \n",
      "2007-01-17    83.958535  0.500000 -19.767484        100.632439   \n",
      "2007-01-18    86.886784  0.500000  -7.766765        100.632439   \n",
      "2007-01-19    45.047729  0.500000 -20.102615        100.632439   \n",
      "2007-01-22    47.626794  0.500000 -20.188579        100.632439   \n",
      "2007-01-23    50.843134  0.500000 -25.374401        100.632439   \n",
      "2007-01-24    56.119192  0.820339 -20.766555        100.632439   \n",
      "2007-01-25    30.387579  0.741469 -13.061436        100.632439   \n",
      "2007-01-26    63.828024  0.811390  -8.360887        100.632439   \n",
      "2007-01-29    47.043501  0.871915 -19.827161        100.632439   \n",
      "2007-01-30    53.550036  0.875484 -14.088278        100.632439   \n",
      "2007-01-31    64.399772  0.880807 -10.056178        100.632439   \n",
      "2007-02-01    71.044693  0.890422 -18.135927        100.632439   \n",
      "2007-02-02    76.267010  0.894606 -20.386063        100.632439   \n",
      "2007-02-05    40.954473  0.844356 -15.557579        100.632439   \n",
      "2007-02-06    45.469817  0.801376 -22.295886        100.632439   \n",
      "2007-02-07    56.817554  0.796761 -25.842834        100.632439   \n",
      "2007-02-08    48.142652  0.818101 -27.547278        100.632439   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03          0.217560     100.632439   16.735813  0.000000  \n",
      "2007-01-04          0.217560     100.632439   17.197192  1.000000  \n",
      "2007-01-05          0.217560     100.632439   17.090272  0.230786  \n",
      "2007-01-08          0.217560     100.632439   16.771275  0.098599  \n",
      "2007-01-09          0.217560     100.632439   17.568519  0.521462  \n",
      "2007-01-10          0.253100     100.632439   17.991802  0.650212  \n",
      "2007-01-11          0.635283     100.632439   18.544461  0.737364  \n",
      "2007-01-12          0.733979     100.632439   18.667065  0.757756  \n",
      "2007-01-16          0.865530     100.632439   18.738416  0.760727  \n",
      "2007-01-17          0.865530     100.632439   19.056331  0.778402  \n",
      "2007-01-18          0.865530     100.632439   19.756222  0.807874  \n",
      "2007-01-19          0.865530     100.632439   18.116886  0.459253  \n",
      "2007-01-22          0.865530     100.632439   18.141595  0.476927  \n",
      "2007-01-23          0.865530     100.632439   18.376662  0.498160  \n",
      "2007-01-24          0.865530     100.632439   18.557279  0.532651  \n",
      "2007-01-25          0.865530     100.632439   18.664601  0.267996  \n",
      "2007-01-26          1.034055     100.632439   20.569001  0.516025  \n",
      "2007-01-29          1.153321     100.632439   20.108829  0.543314  \n",
      "2007-01-30          1.167826     100.632439   20.375698  0.524597  \n",
      "2007-01-31          1.491282     100.632439   21.110792  0.553364  \n",
      "2007-02-01          2.367092     100.632439   21.737828  0.560138  \n",
      "2007-02-02          2.512224     100.632439   21.848150  0.585298  \n",
      "2007-02-05          2.512224     100.632439   21.479563  0.403641  \n",
      "2007-02-06          2.512224     100.632439   20.596746  0.414812  \n",
      "2007-02-07          2.512224     100.632439   20.650799  0.374888  \n",
      "2007-02-08          2.512224     100.632439   20.191108  0.475051  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "TFC\n",
      "                close   volume       open       high        low      cci_14  \\\n",
      "Date                                                                          \n",
      "2007-01-03  23.917282  1810200  43.680000  44.299999  43.630001         NaN   \n",
      "2007-01-04  23.732466  1855100  43.860001  43.970001  43.299999  -66.666667   \n",
      "2007-01-05  23.417194  1529100  43.660000  43.660000  42.990002 -100.000000   \n",
      "2007-01-08  23.324783  1156900  43.000000  43.060001  42.490002 -107.351785   \n",
      "2007-01-09  23.281296  1536000  42.900002  43.160000  42.580002  -75.347683   \n",
      "2007-01-10  23.402067  1385800  42.509998  42.700001  42.400002  -84.910231   \n",
      "2007-01-11  23.594200  1471000  42.779999  43.090000  42.610001  -33.683779   \n",
      "2007-01-12  23.396574  1338300  42.950001  42.950001  42.500000  -57.550066   \n",
      "2007-01-16  23.506361  1395500  42.639999  42.849998  42.500000  -53.100810   \n",
      "2007-01-17  23.462446  1659300  42.939999  42.980000  42.509998  -44.112826   \n",
      "2007-01-18  23.703991  1695000  43.000000  43.590000  42.750000   36.964310   \n",
      "2007-01-19  23.478912  1553200  43.380001  43.400002  42.619999   -5.830295   \n",
      "2007-01-22  23.467943  1102400  42.639999  42.910000  42.509998  -55.448950   \n",
      "2007-01-23  23.484413   854300  42.820000  42.900002  42.549999  -49.411630   \n",
      "2007-01-24  23.605186   992200  42.770000  43.000000  42.709999    3.027335   \n",
      "2007-01-25  23.171497  1585300  43.110001  43.110001  42.180000 -113.835553   \n",
      "2007-01-26  23.160528  1230400  42.130001  42.330002  41.919998 -270.115225   \n",
      "2007-01-29  23.083679  1419300  42.250000  42.340000  41.970001 -189.876763   \n",
      "2007-01-30  23.144053  1123400  42.119999  42.209999  41.939999 -148.311791   \n",
      "2007-01-31  23.198952  1639600  42.240002  42.330002  41.970001 -103.767507   \n",
      "2007-02-01  23.402067  2221600  42.720001  42.810001  42.369999    1.199301   \n",
      "2007-02-02  23.336191  1829900  42.750000  42.849998  42.470001    9.885852   \n",
      "2007-02-05  23.407551  1937500  42.480000  42.779999  42.389999    4.170016   \n",
      "2007-02-06  23.511848  1164400  42.720001  43.000000  42.680000   63.289838   \n",
      "2007-02-07  23.539307  1034700  42.759998  42.980000  42.750000   81.225800   \n",
      "2007-02-08  23.522837  1070300  42.779999  42.980000  42.599998   68.872492   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -29.496675        105.113152   \n",
      "2007-01-04          NaN  0.500000 -30.041008        104.269821   \n",
      "2007-01-05          NaN  0.500000 -30.213225        103.654456   \n",
      "2007-01-08          NaN  0.500000 -34.517942        102.714357   \n",
      "2007-01-09          NaN  0.500000 -33.825470        102.709307   \n",
      "2007-01-10   100.000000  0.500000 -63.693266        102.074263   \n",
      "2007-01-11   100.000000  0.500000 -39.970452        102.074263   \n",
      "2007-01-12    78.514712  0.500000 -43.451986        101.948312   \n",
      "2007-01-16    98.684890  0.500000 -54.667777        101.771661   \n",
      "2007-01-17    94.069487  0.500000 -41.441493        101.749442   \n",
      "2007-01-18   100.000000  0.500000 -22.971435        101.749442   \n",
      "2007-01-19    82.122327  0.500000 -25.514131        101.749442   \n",
      "2007-01-22    81.358948  0.500000 -47.929958        101.749442   \n",
      "2007-01-23    83.085770  0.500000 -55.244172        101.708081   \n",
      "2007-01-24    95.113989  0.491253 -66.085357        101.708081   \n",
      "2007-01-25    67.702465  0.499197 -21.439244        101.538210   \n",
      "2007-01-26    67.175142  0.507934 -46.266594        100.876838   \n",
      "2007-01-29    63.446672  0.501507 -51.801017        100.789167   \n",
      "2007-01-30    50.845120  0.431118 -70.281164        100.590473   \n",
      "2007-01-31    35.916738  0.510849 -52.891716        100.576970   \n",
      "2007-02-01    81.010492  0.529145 -43.904152        100.576970   \n",
      "2007-02-02    67.549909  0.606587 -51.089340        100.576970   \n",
      "2007-02-05    83.404843  0.523778 -48.903791        100.576970   \n",
      "2007-02-06   100.000000  0.512214 -60.025535        100.576970   \n",
      "2007-02-07   100.000000  0.495434 -83.568391        100.576970   \n",
      "2007-02-08    96.520427  0.508116 -50.676600        100.576970   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03        -17.183151     105.113152   20.382717  0.000000  \n",
      "2007-01-04        -16.999821     104.269821   20.077361  1.000000  \n",
      "2007-01-05        -16.999821     103.654456   19.830753  1.000000  \n",
      "2007-01-08        -16.999821     102.714357   19.298016  1.000000  \n",
      "2007-01-09        -16.969306     102.709307   19.462106  1.000000  \n",
      "2007-01-10        -16.969306     102.074263   19.041551  0.680819  \n",
      "2007-01-11        -16.592999     102.074263   19.440117  0.340484  \n",
      "2007-01-12        -16.498312     101.948312   19.333892  0.454165  \n",
      "2007-01-16        -16.421663     101.771661   19.248523  0.327087  \n",
      "2007-01-17        -16.259444     101.749442   19.397061  0.349815  \n",
      "2007-01-18        -16.011042     101.749442   19.990921  0.138342  \n",
      "2007-01-19        -16.011042     101.749442   19.816945  0.248109  \n",
      "2007-01-22        -16.011042     101.749442   19.342292  0.252748  \n",
      "2007-01-23        -16.011042     101.708081   19.343399  0.241249  \n",
      "2007-01-24        -16.011042     101.708081   19.436920  0.162970  \n",
      "2007-01-25        -16.011042     101.538210   19.599132  0.259236  \n",
      "2007-01-26        -16.011042     100.876838   18.865845  0.138020  \n",
      "2007-01-29        -16.011042     100.789167   18.926574  0.130746  \n",
      "2007-01-30        -16.011042     100.590473   18.832489  0.073749  \n",
      "2007-01-31        -16.011042     100.576970   18.976300  0.113151  \n",
      "2007-02-01        -15.873736     100.576970   19.449850  0.106382  \n",
      "2007-02-02        -15.793083     100.576970   19.493042  0.036064  \n",
      "2007-02-05        -15.793083     100.576970   19.416297  0.060402  \n",
      "2007-02-06        -15.631242     100.576970   19.616545  0.029124  \n",
      "2007-02-07        -15.600584     100.576970   19.575764  0.111110  \n",
      "2007-02-08        -15.600584     100.576970   19.559951  0.034489  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "ZION\n",
      "                close   volume       open       high        low      cci_14  \\\n",
      "Date                                                                          \n",
      "2007-01-03  62.279121   693400  82.230003  83.120003  81.180000         NaN   \n",
      "2007-01-04  62.557014   569800  82.800003  83.500000  82.550003   66.666667   \n",
      "2007-01-05  62.369225   839200  83.070000  83.160004  82.599998   29.969615   \n",
      "2007-01-08  62.474396   621300  82.800003  83.220001  82.150002    5.996689   \n",
      "2007-01-09  62.819939   789800  83.269997  83.739998  83.190002  131.120547   \n",
      "2007-01-10  63.188004   578800  83.300003  84.230003  83.129997  124.159547   \n",
      "2007-01-11  62.729801  1189600  83.099998  84.160004  82.949997   65.342216   \n",
      "2007-01-12  62.534527   412300  83.260002  83.449997  82.809998    2.265571   \n",
      "2007-01-16  62.436852   412900  83.080002  83.419998  82.800003   -7.096046   \n",
      "2007-01-17  62.444344   404700  83.169998  83.620003  82.739998    4.126364   \n",
      "2007-01-18  62.406776   315500  83.250000  83.419998  82.959999    2.637842   \n",
      "2007-01-19  62.151379   359200  82.790001  83.410004  82.680000  -44.768963   \n",
      "2007-01-22  62.001160   334800  82.690002  82.809998  82.300003 -126.961002   \n",
      "2007-01-23  61.948597   379800  82.550003  82.839996  82.040001 -126.058657   \n",
      "2007-01-24  62.692261   515700  82.860001  83.750000  82.589996   35.909794   \n",
      "2007-01-25  62.474396   597800  83.220001  84.480003  83.040001  103.983232   \n",
      "2007-01-26  63.465946   673400  84.169998  84.660004  83.639999  186.238894   \n",
      "2007-01-29  63.135433   410800  84.279999  84.550003  83.870003  135.925803   \n",
      "2007-01-30  63.270645   441900  84.180000  84.550003  83.750000  114.024803   \n",
      "2007-01-31  63.713829   411800  83.959999  84.949997  83.540001  127.466003   \n",
      "2007-02-01  63.924126   479800  84.809998  85.269997  84.500000  151.215917   \n",
      "2007-02-02  64.202103   440000  84.989998  85.820000  84.989998  154.824803   \n",
      "2007-02-05  64.141716   477700  85.110001  85.239998  84.180000   90.956844   \n",
      "2007-02-06  64.390724   447500  84.930000  85.660004  84.680000  108.331974   \n",
      "2007-02-07  64.458633   316800  85.379997  85.669998  84.959999  105.445018   \n",
      "2007-02-08  64.307709   303200  85.120003  85.580002  84.779999   82.594160   \n",
      "\n",
      "            stochrsi_14    mfi_14     bop_14  supertrend_14_ub  \\\n",
      "Date                                                             \n",
      "2007-01-03          NaN  0.500000 -10.283947        144.672646   \n",
      "2007-01-04          NaN  0.500000 -21.308477        144.672646   \n",
      "2007-01-05     0.000000  0.500000 -36.965327        144.672646   \n",
      "2007-01-08    20.257978  0.500000 -18.995900        144.672646   \n",
      "2007-01-09    53.551405  0.500000 -37.182233        144.672646   \n",
      "2007-01-10    68.593255  0.500000 -18.283534        144.672646   \n",
      "2007-01-11     6.230477  0.500000 -16.834780        144.672646   \n",
      "2007-01-12     0.000000  0.500000 -32.383586        144.672646   \n",
      "2007-01-16     0.000000  0.500000 -33.295666        144.672646   \n",
      "2007-01-17     0.506759  0.500000 -23.551750        144.672646   \n",
      "2007-01-18     0.000000  0.500000 -45.311446        144.672646   \n",
      "2007-01-19     0.000000  0.500000 -28.271955        144.672646   \n",
      "2007-01-22     0.000000  0.500000 -40.566795        144.672646   \n",
      "2007-01-23     0.000000  0.500000 -25.751905        144.672646   \n",
      "2007-01-24    34.190449  0.371094 -17.385928        144.672646   \n",
      "2007-01-25    32.045154  0.373656 -14.406646        144.672646   \n",
      "2007-01-26    62.370342  0.470991 -20.298005        144.672646   \n",
      "2007-01-29    48.616644  0.483912 -31.094935        144.672646   \n",
      "2007-01-30    52.000600  0.458606 -26.136594        144.672646   \n",
      "2007-01-31    99.235800  0.445613 -14.359026        144.672646   \n",
      "2007-02-01   100.000000  0.575687 -27.124627        144.672646   \n",
      "2007-02-02   100.000000  0.644652 -25.045602        144.672646   \n",
      "2007-02-05    96.194436  0.636975 -19.781446        144.672646   \n",
      "2007-02-06   100.000000  0.640083 -20.958373        144.672646   \n",
      "2007-02-07   100.000000  0.690242 -29.466748        144.672646   \n",
      "2007-02-08    90.051261  0.695418 -26.015268        144.672646   \n",
      "\n",
      "            supertrend_14_lb  supertrend_14  eribull_14    ker_14  \n",
      "Date                                                               \n",
      "2007-01-03         19.627357     144.672646   20.840881  0.000000  \n",
      "2007-01-04         19.911251     144.672646   21.183826  1.000000  \n",
      "2007-01-05         20.233778     144.672646   20.836756  0.193486  \n",
      "2007-01-08         20.233778     144.672646   20.876601  0.342074  \n",
      "2007-01-09         20.573468     144.672646   21.333059  0.590156  \n",
      "2007-01-10         20.573468     144.672646   21.718923  0.707598  \n",
      "2007-01-11         20.573468     144.672646   21.619760  0.258615  \n",
      "2007-01-12         20.573468     144.672646   20.910516  0.131792  \n",
      "2007-01-16         20.573468     144.672646   20.894201  0.077485  \n",
      "2007-01-17         20.573468     144.672646   21.105066  0.080868  \n",
      "2007-01-18         20.573468     144.672646   20.919483  0.061353  \n",
      "2007-01-19         20.573468     144.672646   20.956040  0.054683  \n",
      "2007-01-22         20.573468     144.672646   20.416408  0.111798  \n",
      "2007-01-23         20.573468     144.672646   20.505739  0.130187  \n",
      "2007-01-24         20.573468     144.672646   21.368009  0.125861  \n",
      "2007-01-25         20.573468     144.672646   22.085691  0.025638  \n",
      "2007-01-26         20.573468     144.672646   22.122807  0.272393  \n",
      "2007-01-29         20.586447     144.672646   21.933042  0.155480  \n",
      "2007-01-30         20.586447     144.672646   21.845884  0.111526  \n",
      "2007-01-31         20.586447     144.672646   22.111250  0.127740  \n",
      "2007-02-01         20.999551     144.672646   22.286532  0.308733  \n",
      "2007-02-02         21.359473     144.672646   22.674050  0.422045  \n",
      "2007-02-05         21.359473     144.672646   21.961280  0.435594  \n",
      "2007-02-06         21.359473     144.672646   22.233018  0.468398  \n",
      "2007-02-07         21.359473     144.672646   22.105459  0.490201  \n",
      "2007-02-08         21.359473     144.672646   21.916374  0.528348  \n",
      "&&&&&&&&&&&&&&&&&&&&&&&&\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci_14</th>\n",
       "      <th>BBWI_stochrsi_14</th>\n",
       "      <th>BBWI_mfi_14</th>\n",
       "      <th>BBWI_bop_14</th>\n",
       "      <th>BBWI_supertrend_14_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>ZION_mfi_14</th>\n",
       "      <th>ZION_bop_14</th>\n",
       "      <th>ZION_supertrend_14_ub</th>\n",
       "      <th>ZION_supertrend_14_lb</th>\n",
       "      <th>ZION_supertrend_14</th>\n",
       "      <th>ZION_eribull_14</th>\n",
       "      <th>ZION_ker_14</th>\n",
       "      <th>ZION_momentum</th>\n",
       "      <th>ZION_simple_moving_average</th>\n",
       "      <th>ZION_bollinger_bands</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>10.077239</td>\n",
       "      <td>9288633</td>\n",
       "      <td>24.252222</td>\n",
       "      <td>24.276476</td>\n",
       "      <td>23.540825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.268624</td>\n",
       "      <td>66.506361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-10.283947</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>19.627357</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>20.840881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>9.317532</td>\n",
       "      <td>17476831</td>\n",
       "      <td>22.546482</td>\n",
       "      <td>22.756668</td>\n",
       "      <td>21.891672</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-15.293656</td>\n",
       "      <td>62.557735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-21.308477</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>19.911251</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>21.183826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>9.310717</td>\n",
       "      <td>9970096</td>\n",
       "      <td>21.980598</td>\n",
       "      <td>22.433306</td>\n",
       "      <td>21.147940</td>\n",
       "      <td>-68.036282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-9.857022</td>\n",
       "      <td>61.706631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-36.965327</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>20.233778</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>20.836756</td>\n",
       "      <td>0.193486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>9.130157</td>\n",
       "      <td>9207238</td>\n",
       "      <td>22.029102</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.471302</td>\n",
       "      <td>-53.794551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-12.972357</td>\n",
       "      <td>61.706631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-18.995900</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>20.233778</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>20.876601</td>\n",
       "      <td>0.342074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-09</th>\n",
       "      <td>9.266430</td>\n",
       "      <td>10808659</td>\n",
       "      <td>21.843168</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.827002</td>\n",
       "      <td>-33.004947</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.692959</td>\n",
       "      <td>61.706631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-37.182233</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>20.573468</td>\n",
       "      <td>144.672646</td>\n",
       "      <td>21.333059</td>\n",
       "      <td>0.590156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>41.287586</td>\n",
       "      <td>2461500</td>\n",
       "      <td>40.330002</td>\n",
       "      <td>41.950001</td>\n",
       "      <td>40.150002</td>\n",
       "      <td>-26.952911</td>\n",
       "      <td>50.075488</td>\n",
       "      <td>0.271783</td>\n",
       "      <td>0.531992</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503874</td>\n",
       "      <td>-2.321171</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>40.857792</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>2.522172</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>1.027790</td>\n",
       "      <td>0.023988</td>\n",
       "      <td>0.640840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>41.297436</td>\n",
       "      <td>3022500</td>\n",
       "      <td>41.939999</td>\n",
       "      <td>42.889999</td>\n",
       "      <td>41.810001</td>\n",
       "      <td>46.300354</td>\n",
       "      <td>50.367993</td>\n",
       "      <td>0.338524</td>\n",
       "      <td>-0.594967</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575792</td>\n",
       "      <td>-1.299558</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>41.058295</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>2.848211</td>\n",
       "      <td>0.236112</td>\n",
       "      <td>1.038045</td>\n",
       "      <td>0.029589</td>\n",
       "      <td>0.721970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>40.116104</td>\n",
       "      <td>2138000</td>\n",
       "      <td>41.779999</td>\n",
       "      <td>42.169998</td>\n",
       "      <td>40.630001</td>\n",
       "      <td>-29.822240</td>\n",
       "      <td>6.012646</td>\n",
       "      <td>0.345384</td>\n",
       "      <td>-1.080453</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617888</td>\n",
       "      <td>-2.740685</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>41.105389</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>2.632594</td>\n",
       "      <td>0.115289</td>\n",
       "      <td>1.012453</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.239927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>41.002106</td>\n",
       "      <td>1610500</td>\n",
       "      <td>41.349998</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>41.180000</td>\n",
       "      <td>15.208465</td>\n",
       "      <td>43.168365</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>-0.424260</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607571</td>\n",
       "      <td>-0.544291</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>41.169776</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>2.903731</td>\n",
       "      <td>0.194064</td>\n",
       "      <td>1.033481</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>0.686533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>41.484478</td>\n",
       "      <td>2237000</td>\n",
       "      <td>41.099998</td>\n",
       "      <td>42.209999</td>\n",
       "      <td>40.880001</td>\n",
       "      <td>37.972543</td>\n",
       "      <td>62.418084</td>\n",
       "      <td>0.358997</td>\n",
       "      <td>0.289083</td>\n",
       "      <td>46.013339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601246</td>\n",
       "      <td>-1.345071</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>41.586694</td>\n",
       "      <td>53.253923</td>\n",
       "      <td>2.873101</td>\n",
       "      <td>0.191294</td>\n",
       "      <td>1.015702</td>\n",
       "      <td>0.029683</td>\n",
       "      <td>0.623860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4028 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "Date                                                                   \n",
       "2007-01-03   10.077239      9288633  24.252222  24.276476  23.540825   \n",
       "2007-01-04    9.317532     17476831  22.546482  22.756668  21.891672   \n",
       "2007-01-05    9.310717      9970096  21.980598  22.433306  21.147940   \n",
       "2007-01-08    9.130157      9207238  22.029102  22.465643  21.471302   \n",
       "2007-01-09    9.266430     10808659  21.843168  22.465643  21.827002   \n",
       "...                ...          ...        ...        ...        ...   \n",
       "2022-12-23   41.287586      2461500  40.330002  41.950001  40.150002   \n",
       "2022-12-27   41.297436      3022500  41.939999  42.889999  41.810001   \n",
       "2022-12-28   40.116104      2138000  41.779999  42.169998  40.630001   \n",
       "2022-12-29   41.002106      1610500  41.349998  42.000000  41.180000   \n",
       "2022-12-30   41.484478      2237000  41.099998  42.209999  40.880001   \n",
       "\n",
       "            BBWI_cci_14  BBWI_stochrsi_14  BBWI_mfi_14  BBWI_bop_14  \\\n",
       "Date                                                                  \n",
       "2007-01-03          NaN               NaN     0.500000   -19.268624   \n",
       "2007-01-04   -66.666667               NaN     0.500000   -15.293656   \n",
       "2007-01-05   -68.036282               NaN     0.500000    -9.857022   \n",
       "2007-01-08   -53.794551               NaN     0.500000   -12.972357   \n",
       "2007-01-09   -33.004947        100.000000     0.500000   -19.692959   \n",
       "...                 ...               ...          ...          ...   \n",
       "2022-12-23   -26.952911         50.075488     0.271783     0.531992   \n",
       "2022-12-27    46.300354         50.367993     0.338524    -0.594967   \n",
       "2022-12-28   -29.822240          6.012646     0.345384    -1.080453   \n",
       "2022-12-29    15.208465         43.168365     0.393974    -0.424260   \n",
       "2022-12-30    37.972543         62.418084     0.358997     0.289083   \n",
       "\n",
       "            BBWI_supertrend_14_ub  ...  ZION_mfi_14  ZION_bop_14  \\\n",
       "Date                               ...                             \n",
       "2007-01-03              66.506361  ...     0.500000   -10.283947   \n",
       "2007-01-04              62.557735  ...     0.500000   -21.308477   \n",
       "2007-01-05              61.706631  ...     0.500000   -36.965327   \n",
       "2007-01-08              61.706631  ...     0.500000   -18.995900   \n",
       "2007-01-09              61.706631  ...     0.500000   -37.182233   \n",
       "...                           ...  ...          ...          ...   \n",
       "2022-12-23              46.013339  ...     0.503874    -2.321171   \n",
       "2022-12-27              46.013339  ...     0.575792    -1.299558   \n",
       "2022-12-28              46.013339  ...     0.617888    -2.740685   \n",
       "2022-12-29              46.013339  ...     0.607571    -0.544291   \n",
       "2022-12-30              46.013339  ...     0.601246    -1.345071   \n",
       "\n",
       "            ZION_supertrend_14_ub  ZION_supertrend_14_lb  ZION_supertrend_14  \\\n",
       "Date                                                                           \n",
       "2007-01-03             144.672646              19.627357          144.672646   \n",
       "2007-01-04             144.672646              19.911251          144.672646   \n",
       "2007-01-05             144.672646              20.233778          144.672646   \n",
       "2007-01-08             144.672646              20.233778          144.672646   \n",
       "2007-01-09             144.672646              20.573468          144.672646   \n",
       "...                           ...                    ...                 ...   \n",
       "2022-12-23              53.253923              40.857792           53.253923   \n",
       "2022-12-27              53.253923              41.058295           53.253923   \n",
       "2022-12-28              53.253923              41.105389           53.253923   \n",
       "2022-12-29              53.253923              41.169776           53.253923   \n",
       "2022-12-30              53.253923              41.586694           53.253923   \n",
       "\n",
       "            ZION_eribull_14  ZION_ker_14  ZION_momentum  \\\n",
       "Date                                                      \n",
       "2007-01-03        20.840881     0.000000            NaN   \n",
       "2007-01-04        21.183826     1.000000            NaN   \n",
       "2007-01-05        20.836756     0.193486            NaN   \n",
       "2007-01-08        20.876601     0.342074            NaN   \n",
       "2007-01-09        21.333059     0.590156            NaN   \n",
       "...                     ...          ...            ...   \n",
       "2022-12-23         2.522172     0.003709       1.027790   \n",
       "2022-12-27         2.848211     0.236112       1.038045   \n",
       "2022-12-28         2.632594     0.115289       1.012453   \n",
       "2022-12-29         2.903731     0.194064       1.033481   \n",
       "2022-12-30         2.873101     0.191294       1.015702   \n",
       "\n",
       "            ZION_simple_moving_average  ZION_bollinger_bands  \n",
       "Date                                                          \n",
       "2007-01-03                         NaN                   NaN  \n",
       "2007-01-04                         NaN                   NaN  \n",
       "2007-01-05                         NaN                   NaN  \n",
       "2007-01-08                         NaN                   NaN  \n",
       "2007-01-09                         NaN                   NaN  \n",
       "...                                ...                   ...  \n",
       "2022-12-23                    0.023988              0.640840  \n",
       "2022-12-27                    0.029589              0.721970  \n",
       "2022-12-28                    0.009849              0.239927  \n",
       "2022-12-29                    0.030582              0.686533  \n",
       "2022-12-30                    0.029683              0.623860  \n",
       "\n",
       "[4028 rows x 170 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stockstats import StockDataFrame\n",
    "def extract_and_wrap_ticker_data(df, columns):\n",
    "    # Split column names into parts\n",
    "    parts = [col.split('_') for col in columns]\n",
    "\n",
    "    # Create a dictionary to store information about each ticker\n",
    "    ticker_info = {}\n",
    "\n",
    "    # Iterate through the parts list\n",
    "    for part in parts:\n",
    "        data_type = part[0]  # e.g., 'Adj Close', 'Close', etc.\n",
    "        ticker = part[1]     # e.g., 'AAPL', 'GOOGL', etc.\n",
    "\n",
    "        # Check if the ticker is already in the dictionary\n",
    "        if ticker not in ticker_info:\n",
    "            ticker_info[ticker] = {}\n",
    "\n",
    "        # Add information for the ticker\n",
    "        ticker_info[ticker][data_type] = df[f'{data_type}_{ticker}']\n",
    "\n",
    "    # Create a dictionary to store stockstats DataFrames for each ticker\n",
    "    ticker_dataframes = pd.DataFrame()\n",
    "#     var_importance_all=\n",
    "\n",
    "    # Iterate through ticker_info and wrap data with stockstats\n",
    "    for ticker, data in ticker_info.items():\n",
    "        print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        df = pd.DataFrame(data)\n",
    "        print(ticker)\n",
    "#         print(df)\n",
    "        \n",
    "        stock_df = StockDataFrame.retype(df)\n",
    "#         print(stock_df)\n",
    "\n",
    "\n",
    "        stock_df['cci_14']\n",
    "        stock_df['stochrsi_14']\n",
    "#         stock_df['ichimoku cloud']=stock_df['ichimoku cloud']\n",
    "        stock_df['mfi_14']\n",
    "        stock_df['bop_14']\n",
    "        stock_df['supertrend_14']\n",
    "        stock_df['eribull_14']\n",
    "#         stock_df['cti_14']\n",
    "#         stock_df['qqe_14']\n",
    "        stock_df['ker_14']\n",
    "        print(stock_df.head(26))\n",
    "        roll_stat=rolling_statistics(stock_df,cols='close',\n",
    "                      window=14)\n",
    "        stock_df = pd.concat([stock_df,roll_stat],axis=1)\n",
    "        print(\"&&&&&&&&&&&&&&&&&&&&&&&&\")\n",
    "        \n",
    "        \n",
    "\n",
    "        stock_df.columns = [ticker+\"_\" + col for col in pd.DataFrame(stock_df).columns]\n",
    "        ticker_dataframes = pd.concat([ticker_dataframes,stock_df],axis=1)\n",
    "        \n",
    "    return ticker_dataframes\n",
    "\n",
    "# Sample DataFrame with specified columns\n",
    "columns = list(df2.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Extract and wrap ticker data\n",
    "ticker_dataframes = extract_and_wrap_ticker_data(df2, columns)\n",
    "\n",
    "# Access information for a specific ticker (e.g., AAPL)\n",
    "# print(ticker_dataframes)\n",
    "\n",
    "ticker_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edfe6dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            wti_spot  10y2y_spread  10y3m_spread  3m_rate  ltiit  ted_spread  \\\n",
      "Date                                                                           \n",
      "2007-01-02     60.77         -0.12         -0.39     4.94   2.33        0.42   \n",
      "2007-01-03     58.31         -0.09         -0.38     4.92   2.32        0.44   \n",
      "2007-01-04     55.65         -0.09         -0.42     4.91   2.30        0.45   \n",
      "2007-01-05     56.29         -0.11         -0.40     4.92   2.33        0.44   \n",
      "2007-01-08     56.08         -0.12         -0.42     4.95   2.34        0.41   \n",
      "...              ...           ...           ...      ...    ...         ...   \n",
      "2022-07-12       NaN         -0.07          0.74     2.16   1.12         NaN   \n",
      "2022-07-13       NaN         -0.22          0.52     2.33   1.06         NaN   \n",
      "2022-07-14       NaN         -0.19          0.56     2.33   1.07         NaN   \n",
      "2022-07-15       NaN         -0.20          0.56     2.29   1.03         NaN   \n",
      "2022-07-18       NaN         -0.19          0.46      NaN    NaN         NaN   \n",
      "\n",
      "             var_wti  \n",
      "Date                  \n",
      "2007-01-02       NaN  \n",
      "2007-01-03 -0.040481  \n",
      "2007-01-04 -0.045618  \n",
      "2007-01-05  0.011500  \n",
      "2007-01-08 -0.003731  \n",
      "...              ...  \n",
      "2022-07-12  0.000000  \n",
      "2022-07-13  0.000000  \n",
      "2022-07-14  0.000000  \n",
      "2022-07-15  0.000000  \n",
      "2022-07-18  0.000000  \n",
      "\n",
      "[3944 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the tickers\n",
    "new_tickers = ['EIA/PET_RWTC_D','FRED/T10Y2Y','FRED/T10Y3M','FRED/DTB3','FRED/DLTIIT','FRED/TEDRATE']\n",
    "\n",
    "# # Define the date range\n",
    "# start='2020-01-01'\n",
    "# end='2021-01-01'\n",
    "names = ['wti_spot','10y2y_spread','10y3m_spread','3m_rate','ltiit','ted_spread']\n",
    "# add_factors = quandl.get(tickers, start=start_date, end=end_date)\n",
    "\n",
    "# # Retrieve data\n",
    "data_quandl = quandl.get(new_tickers, start_date=start_date, end_date=end_date)\n",
    "data_quandl.columns = names\n",
    "data_quandl['var_wti'] = data_quandl['wti_spot'].pct_change()\n",
    "# print(data_quandl)\n",
    "\n",
    "# # Fill NaN values:\n",
    "# data_quandl.fillna(method='ffill',inplace=True)\n",
    "# data_quandl.fillna(method='bfill',inplace=True)\n",
    "\n",
    "\n",
    "# # Print the data\n",
    "print(data_quandl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1459bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci_14</th>\n",
       "      <th>BBWI_stochrsi_14</th>\n",
       "      <th>BBWI_mfi_14</th>\n",
       "      <th>BBWI_bop_14</th>\n",
       "      <th>...</th>\n",
       "      <th>ZION_momentum</th>\n",
       "      <th>ZION_simple_moving_average</th>\n",
       "      <th>ZION_bollinger_bands</th>\n",
       "      <th>wti_spot</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-01-03</td>\n",
       "      <td>10.077239</td>\n",
       "      <td>9288633</td>\n",
       "      <td>24.252222</td>\n",
       "      <td>24.276476</td>\n",
       "      <td>23.540825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.268624</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.31</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.040481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-01-04</td>\n",
       "      <td>9.317532</td>\n",
       "      <td>17476831</td>\n",
       "      <td>22.546482</td>\n",
       "      <td>22.756668</td>\n",
       "      <td>21.891672</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-15.293656</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.65</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.91</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.045618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-01-05</td>\n",
       "      <td>9.310717</td>\n",
       "      <td>9970096</td>\n",
       "      <td>21.980598</td>\n",
       "      <td>22.433306</td>\n",
       "      <td>21.147940</td>\n",
       "      <td>-68.036282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-9.857022</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-01-08</td>\n",
       "      <td>9.130157</td>\n",
       "      <td>9207238</td>\n",
       "      <td>22.029102</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.471302</td>\n",
       "      <td>-53.794551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-12.972357</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.003731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-01-09</td>\n",
       "      <td>9.266430</td>\n",
       "      <td>10808659</td>\n",
       "      <td>21.843168</td>\n",
       "      <td>22.465643</td>\n",
       "      <td>21.827002</td>\n",
       "      <td>-33.004947</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-19.692959</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.65</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.007668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>2022-07-12</td>\n",
       "      <td>26.133543</td>\n",
       "      <td>3131200</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>27.610001</td>\n",
       "      <td>26.680000</td>\n",
       "      <td>-60.147031</td>\n",
       "      <td>71.722590</td>\n",
       "      <td>0.398109</td>\n",
       "      <td>-0.716619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>-0.025107</td>\n",
       "      <td>-0.656465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>26.678602</td>\n",
       "      <td>3244200</td>\n",
       "      <td>26.450001</td>\n",
       "      <td>27.540001</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>-57.069015</td>\n",
       "      <td>99.104697</td>\n",
       "      <td>0.413616</td>\n",
       "      <td>0.156576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975020</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>25.267288</td>\n",
       "      <td>4278500</td>\n",
       "      <td>26.870001</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>25.809999</td>\n",
       "      <td>-93.255723</td>\n",
       "      <td>63.458153</td>\n",
       "      <td>0.423011</td>\n",
       "      <td>-1.324555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910182</td>\n",
       "      <td>-0.050377</td>\n",
       "      <td>-1.002122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>26.775932</td>\n",
       "      <td>4123700</td>\n",
       "      <td>26.459999</td>\n",
       "      <td>27.680000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>-34.397938</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.374416</td>\n",
       "      <td>0.220932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962365</td>\n",
       "      <td>-0.005737</td>\n",
       "      <td>-0.130583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>27.817379</td>\n",
       "      <td>5343900</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>138.383635</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.393358</td>\n",
       "      <td>-0.162671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965458</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3912 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "0    2007-01-03   10.077239      9288633  24.252222  24.276476  23.540825   \n",
       "1    2007-01-04    9.317532     17476831  22.546482  22.756668  21.891672   \n",
       "2    2007-01-05    9.310717      9970096  21.980598  22.433306  21.147940   \n",
       "3    2007-01-08    9.130157      9207238  22.029102  22.465643  21.471302   \n",
       "4    2007-01-09    9.266430     10808659  21.843168  22.465643  21.827002   \n",
       "...         ...         ...          ...        ...        ...        ...   \n",
       "3907 2022-07-12   26.133543      3131200  26.799999  27.610001  26.680000   \n",
       "3908 2022-07-13   26.678602      3244200  26.450001  27.540001  26.080000   \n",
       "3909 2022-07-14   25.267288      4278500  26.870001  27.020000  25.809999   \n",
       "3910 2022-07-15   26.775932      4123700  26.459999  27.680000  26.250000   \n",
       "3911 2022-07-18   27.817379      5343900  28.049999  29.480000  28.049999   \n",
       "\n",
       "      BBWI_cci_14  BBWI_stochrsi_14  BBWI_mfi_14  BBWI_bop_14  ...  \\\n",
       "0             NaN               NaN     0.500000   -19.268624  ...   \n",
       "1      -66.666667               NaN     0.500000   -15.293656  ...   \n",
       "2      -68.036282               NaN     0.500000    -9.857022  ...   \n",
       "3      -53.794551               NaN     0.500000   -12.972357  ...   \n",
       "4      -33.004947        100.000000     0.500000   -19.692959  ...   \n",
       "...           ...               ...          ...          ...  ...   \n",
       "3907   -60.147031         71.722590     0.398109    -0.716619  ...   \n",
       "3908   -57.069015         99.104697     0.413616     0.156576  ...   \n",
       "3909   -93.255723         63.458153     0.423011    -1.324555  ...   \n",
       "3910   -34.397938        100.000000     0.374416     0.220932  ...   \n",
       "3911   138.383635        100.000000     0.393358    -0.162671  ...   \n",
       "\n",
       "      ZION_momentum  ZION_simple_moving_average  ZION_bollinger_bands  \\\n",
       "0               NaN                         NaN                   NaN   \n",
       "1               NaN                         NaN                   NaN   \n",
       "2               NaN                         NaN                   NaN   \n",
       "3               NaN                         NaN                   NaN   \n",
       "4               NaN                         NaN                   NaN   \n",
       "...             ...                         ...                   ...   \n",
       "3907       0.955099                   -0.025107             -0.656465   \n",
       "3908       0.975020                   -0.033333             -0.808520   \n",
       "3909       0.910182                   -0.050377             -1.002122   \n",
       "3910       0.962365                   -0.005737             -0.130583   \n",
       "3911       0.965458                    0.001420              0.035792   \n",
       "\n",
       "      wti_spot  10y2y_spread  10y3m_spread  3m_rate  ltiit  ted_spread  \\\n",
       "0        58.31         -0.09         -0.38     4.92   2.32        0.44   \n",
       "1        55.65         -0.09         -0.42     4.91   2.30        0.45   \n",
       "2        56.29         -0.11         -0.40     4.92   2.33        0.44   \n",
       "3        56.08         -0.12         -0.42     4.95   2.34        0.41   \n",
       "4        55.65         -0.13         -0.42     4.95   2.35        0.41   \n",
       "...        ...           ...           ...      ...    ...         ...   \n",
       "3907       NaN         -0.07          0.74     2.16   1.12         NaN   \n",
       "3908       NaN         -0.22          0.52     2.33   1.06         NaN   \n",
       "3909       NaN         -0.19          0.56     2.33   1.07         NaN   \n",
       "3910       NaN         -0.20          0.56     2.29   1.03         NaN   \n",
       "3911       NaN         -0.19          0.46      NaN    NaN         NaN   \n",
       "\n",
       "       var_wti  \n",
       "0    -0.040481  \n",
       "1    -0.045618  \n",
       "2     0.011500  \n",
       "3    -0.003731  \n",
       "4    -0.007668  \n",
       "...        ...  \n",
       "3907  0.000000  \n",
       "3908  0.000000  \n",
       "3909  0.000000  \n",
       "3910  0.000000  \n",
       "3911  0.000000  \n",
       "\n",
       "[3912 rows x 178 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df=pd.merge(ticker_dataframes.reset_index(),data_quandl.reset_index(),on=['Date'],how='inner')\n",
    "# final_df=ticker_dataframes.copy()\n",
    "\n",
    "# Fill NaN values:\n",
    "# final_df.fillna(method='ffill',inplace=True)\n",
    "# final_df.fillna(method='bfill',inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6053a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci_14</th>\n",
       "      <th>BBWI_stochrsi_14</th>\n",
       "      <th>BBWI_mfi_14</th>\n",
       "      <th>BBWI_bop_14</th>\n",
       "      <th>BBWI_supertrend_14_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>ZION_ker_14</th>\n",
       "      <th>ZION_momentum</th>\n",
       "      <th>ZION_simple_moving_average</th>\n",
       "      <th>ZION_bollinger_bands</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-02-05</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>3568003</td>\n",
       "      <td>23.055780</td>\n",
       "      <td>23.233629</td>\n",
       "      <td>22.926435</td>\n",
       "      <td>97.129259</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.559890</td>\n",
       "      <td>-43.324430</td>\n",
       "      <td>61.163120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435594</td>\n",
       "      <td>1.027182</td>\n",
       "      <td>0.018154</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.005423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-06</th>\n",
       "      <td>0.019573</td>\n",
       "      <td>6720374</td>\n",
       "      <td>23.120453</td>\n",
       "      <td>23.678253</td>\n",
       "      <td>23.104284</td>\n",
       "      <td>157.611932</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.675272</td>\n",
       "      <td>-22.967954</td>\n",
       "      <td>61.163120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468398</td>\n",
       "      <td>1.031791</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>0.727985</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.003749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-07</th>\n",
       "      <td>0.004114</td>\n",
       "      <td>5023086</td>\n",
       "      <td>23.492319</td>\n",
       "      <td>24.155214</td>\n",
       "      <td>23.338722</td>\n",
       "      <td>184.858791</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.681892</td>\n",
       "      <td>-16.551140</td>\n",
       "      <td>61.163120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490201</td>\n",
       "      <td>1.037123</td>\n",
       "      <td>0.018567</td>\n",
       "      <td>0.652141</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.019691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-08</th>\n",
       "      <td>-0.008194</td>\n",
       "      <td>3266546</td>\n",
       "      <td>23.637833</td>\n",
       "      <td>23.759094</td>\n",
       "      <td>23.257881</td>\n",
       "      <td>113.131323</td>\n",
       "      <td>82.672656</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>-27.415793</td>\n",
       "      <td>61.163120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528348</td>\n",
       "      <td>1.037202</td>\n",
       "      <td>0.013715</td>\n",
       "      <td>0.496373</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>5.03</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.034805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-09</th>\n",
       "      <td>-0.018932</td>\n",
       "      <td>5109923</td>\n",
       "      <td>23.565077</td>\n",
       "      <td>23.637833</td>\n",
       "      <td>23.023443</td>\n",
       "      <td>64.566839</td>\n",
       "      <td>48.626041</td>\n",
       "      <td>0.612201</td>\n",
       "      <td>-22.552091</td>\n",
       "      <td>61.163120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609964</td>\n",
       "      <td>1.041858</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>0.564524</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12</th>\n",
       "      <td>-0.002600</td>\n",
       "      <td>3131200</td>\n",
       "      <td>26.799999</td>\n",
       "      <td>27.610001</td>\n",
       "      <td>26.680000</td>\n",
       "      <td>-60.147031</td>\n",
       "      <td>71.722590</td>\n",
       "      <td>0.398109</td>\n",
       "      <td>-0.716619</td>\n",
       "      <td>32.801144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254973</td>\n",
       "      <td>0.955099</td>\n",
       "      <td>-0.025107</td>\n",
       "      <td>-0.656465</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-13</th>\n",
       "      <td>0.020857</td>\n",
       "      <td>3244200</td>\n",
       "      <td>26.450001</td>\n",
       "      <td>27.540001</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>-57.069015</td>\n",
       "      <td>99.104697</td>\n",
       "      <td>0.413616</td>\n",
       "      <td>0.156576</td>\n",
       "      <td>32.374991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266251</td>\n",
       "      <td>0.975020</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-14</th>\n",
       "      <td>-0.052901</td>\n",
       "      <td>4278500</td>\n",
       "      <td>26.870001</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>25.809999</td>\n",
       "      <td>-93.255723</td>\n",
       "      <td>63.458153</td>\n",
       "      <td>0.423011</td>\n",
       "      <td>-1.324555</td>\n",
       "      <td>31.841777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218217</td>\n",
       "      <td>0.910182</td>\n",
       "      <td>-0.050377</td>\n",
       "      <td>-1.002122</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>0.059707</td>\n",
       "      <td>4123700</td>\n",
       "      <td>26.459999</td>\n",
       "      <td>27.680000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>-34.397938</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.374416</td>\n",
       "      <td>0.220932</td>\n",
       "      <td>31.841777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266272</td>\n",
       "      <td>0.962365</td>\n",
       "      <td>-0.005737</td>\n",
       "      <td>-0.130583</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>0.038895</td>\n",
       "      <td>5343900</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>138.383635</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.393358</td>\n",
       "      <td>-0.162671</td>\n",
       "      <td>31.841777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180124</td>\n",
       "      <td>0.965458</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3890 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "Date                                                                   \n",
       "2007-02-05    0.001751      3568003  23.055780  23.233629  22.926435   \n",
       "2007-02-06    0.019573      6720374  23.120453  23.678253  23.104284   \n",
       "2007-02-07    0.004114      5023086  23.492319  24.155214  23.338722   \n",
       "2007-02-08   -0.008194      3266546  23.637833  23.759094  23.257881   \n",
       "2007-02-09   -0.018932      5109923  23.565077  23.637833  23.023443   \n",
       "...                ...          ...        ...        ...        ...   \n",
       "2022-07-12   -0.002600      3131200  26.799999  27.610001  26.680000   \n",
       "2022-07-13    0.020857      3244200  26.450001  27.540001  26.080000   \n",
       "2022-07-14   -0.052901      4278500  26.870001  27.020000  25.809999   \n",
       "2022-07-15    0.059707      4123700  26.459999  27.680000  26.250000   \n",
       "2022-07-18    0.038895      5343900  28.049999  29.480000  28.049999   \n",
       "\n",
       "            BBWI_cci_14  BBWI_stochrsi_14  BBWI_mfi_14  BBWI_bop_14  \\\n",
       "Date                                                                  \n",
       "2007-02-05    97.129259        100.000000     0.559890   -43.324430   \n",
       "2007-02-06   157.611932        100.000000     0.675272   -22.967954   \n",
       "2007-02-07   184.858791        100.000000     0.681892   -16.551140   \n",
       "2007-02-08   113.131323         82.672656     0.700400   -27.415793   \n",
       "2007-02-09    64.566839         48.626041     0.612201   -22.552091   \n",
       "...                 ...               ...          ...          ...   \n",
       "2022-07-12   -60.147031         71.722590     0.398109    -0.716619   \n",
       "2022-07-13   -57.069015         99.104697     0.413616     0.156576   \n",
       "2022-07-14   -93.255723         63.458153     0.423011    -1.324555   \n",
       "2022-07-15   -34.397938        100.000000     0.374416     0.220932   \n",
       "2022-07-18   138.383635        100.000000     0.393358    -0.162671   \n",
       "\n",
       "            BBWI_supertrend_14_ub  ...  ZION_ker_14  ZION_momentum  \\\n",
       "Date                               ...                               \n",
       "2007-02-05              61.163120  ...     0.435594       1.027182   \n",
       "2007-02-06              61.163120  ...     0.468398       1.031791   \n",
       "2007-02-07              61.163120  ...     0.490201       1.037123   \n",
       "2007-02-08              61.163120  ...     0.528348       1.037202   \n",
       "2007-02-09              61.163120  ...     0.609964       1.041858   \n",
       "...                           ...  ...          ...            ...   \n",
       "2022-07-12              32.801144  ...     0.254973       0.955099   \n",
       "2022-07-13              32.374991  ...     0.266251       0.975020   \n",
       "2022-07-14              31.841777  ...     0.218217       0.910182   \n",
       "2022-07-15              31.841777  ...     0.266272       0.962365   \n",
       "2022-07-18              31.841777  ...     0.180124       0.965458   \n",
       "\n",
       "            ZION_simple_moving_average  ZION_bollinger_bands  10y2y_spread  \\\n",
       "Date                                                                         \n",
       "2007-02-05                    0.018154              0.716702         -0.11   \n",
       "2007-02-06                    0.019856              0.727985         -0.13   \n",
       "2007-02-07                    0.018567              0.652141         -0.13   \n",
       "2007-02-08                    0.013715              0.496373         -0.14   \n",
       "2007-02-09                    0.014501              0.564524         -0.13   \n",
       "...                                ...                   ...           ...   \n",
       "2022-07-12                   -0.025107             -0.656465         -0.07   \n",
       "2022-07-13                   -0.033333             -0.808520         -0.22   \n",
       "2022-07-14                   -0.050377             -1.002122         -0.19   \n",
       "2022-07-15                   -0.005737             -0.130583         -0.20   \n",
       "2022-07-18                    0.001420              0.035792         -0.19   \n",
       "\n",
       "            10y3m_spread  3m_rate  ltiit  ted_spread   var_wti  \n",
       "Date                                                            \n",
       "2007-02-05         -0.34     5.02   2.39        0.34 -0.005423  \n",
       "2007-02-06         -0.37     5.01   2.39        0.35  0.003749  \n",
       "2007-02-07         -0.41     5.02   2.38        0.34 -0.019691  \n",
       "2007-02-08         -0.43     5.03   2.37        0.33  0.034805  \n",
       "2007-02-09         -0.36     5.02   2.41        0.34  0.001673  \n",
       "...                  ...      ...    ...         ...       ...  \n",
       "2022-07-12          0.74     2.16   1.12         NaN  0.000000  \n",
       "2022-07-13          0.52     2.33   1.06         NaN  0.000000  \n",
       "2022-07-14          0.56     2.33   1.07         NaN  0.000000  \n",
       "2022-07-15          0.56     2.29   1.03         NaN  0.000000  \n",
       "2022-07-18          0.46      NaN    NaN         NaN  0.000000  \n",
       "\n",
       "[3890 rows x 176 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2=final_df.copy()\n",
    "for ticker in tickers:\n",
    "    final_df2[ticker+\"_close\"] = final_df2[ticker+\"_close\"].pct_change()\n",
    "#     final_df.drop(columns=[ticker+\"_adj close\"],inplace=True)\n",
    "    final_df2=final_df2.dropna(subset=[ticker+\"_close\",ticker+\"_momentum\",ticker+\"_bollinger_bands\"])\n",
    "final_df2.set_index(['Date'],inplace=True)\n",
    "final_df2.drop(columns=['wti_spot'],inplace=True)\n",
    "final_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc91a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def check_var_importance(df,ticker):\n",
    "    # Separate features (X) and target (y)\n",
    "    # AAPL_df.drop(columns=[\"AAPL_cr\"],inplace=True)\n",
    "    df= df.dropna() \n",
    "    X = df  # Use relevant columns\n",
    "    y = df[ticker+'_close']  # Target variable is 'Close'\n",
    "\n",
    "    # Standardize features (optional but often recommended for neural networks)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Create sequences of length 5 for X and corresponding y for training\n",
    "    X_train, y_train = [], []\n",
    "\n",
    "    for i in range(5, len(X)):\n",
    "        X_train.append(X[i-5:i])\n",
    "        y_train.append(y[i])\n",
    "\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "#     print(X_train)\n",
    "#     print(\"***************\")\n",
    "#     print(y_train)\n",
    "\n",
    "    # Step 2: Check Variable Importance\n",
    "    # Use RandomForestRegressor to determine importance\n",
    "    rf = RandomForestRegressor(n_estimators=10)\n",
    "    rf.fit(X_train.reshape(-1, 5 * X.shape[1]), y_train)\n",
    "\n",
    "    # Get feature importances along with their corresponding feature names\n",
    "    feature_importances = rf.feature_importances_\n",
    "    feature_names = [col.split(ticker+\"_\")[-1] for col in df.columns]\n",
    "#     print(feature_names)\n",
    "\n",
    "    # Combine feature names with their importances\n",
    "    feature_importance_info = list(zip(feature_names, feature_importances))\n",
    "\n",
    "    # Sort the list by importance (in descending order)\n",
    "    feature_importance_info.sort(key=lambda x: x[0])\n",
    "\n",
    "    return pd.DataFrame(feature_importance_info,columns=['Features',ticker+'_Relative_Importance'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f8d4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Features DIS_Relative_Importance BIIB_Relative_Importance  \\\n",
      "0         bollinger_bands                0.008701                 0.005558   \n",
      "1                  bop_14                 0.01115                 0.034658   \n",
      "2                  cci_14                0.012515                 0.010502   \n",
      "3                   close                0.046393                 0.019462   \n",
      "4              eribull_14                 0.01012                 0.018783   \n",
      "5                    high                0.005242                 0.003294   \n",
      "6                  ker_14                0.012999                 0.015705   \n",
      "7                     low                0.003171                 0.004664   \n",
      "8                  mfi_14                0.011963                 0.042517   \n",
      "9                momentum                0.019631                 0.008287   \n",
      "10                   open                0.007208                 0.004785   \n",
      "11  simple_moving_average                0.009434                 0.009297   \n",
      "12            stochrsi_14                0.008208                   0.0066   \n",
      "13          supertrend_14                0.001702                 0.005752   \n",
      "14       supertrend_14_lb                0.007274                 0.005739   \n",
      "15       supertrend_14_ub                0.001865                 0.015328   \n",
      "16                 volume                0.018641                 0.013938   \n",
      "\n",
      "   PGR_Relative_Importance SWK_Relative_Importance PEP_Relative_Importance  \\\n",
      "0                 0.009329                0.011137                0.008136   \n",
      "1                 0.014757                0.014789                0.017913   \n",
      "2                 0.017664                 0.00826                0.010023   \n",
      "3                  0.04437                0.027619                0.037742   \n",
      "4                 0.014069                0.042906                0.010319   \n",
      "5                 0.008316                0.004113                0.003189   \n",
      "6                 0.013093                0.013679                0.015937   \n",
      "7                  0.01163                0.004595                0.007028   \n",
      "8                 0.009414                 0.01182                0.013814   \n",
      "9                 0.012366                0.025572                0.012631   \n",
      "10                0.009244                  0.0036                0.004364   \n",
      "11                0.013273                0.014972                0.009158   \n",
      "12                0.012524                0.009636                0.008393   \n",
      "13                0.002752                0.004798                0.002034   \n",
      "14                0.001435                0.001806                0.001407   \n",
      "15                0.000755                0.001849                0.000954   \n",
      "16                0.015431                0.018969                0.019783   \n",
      "\n",
      "   ZION_Relative_Importance LUV_Relative_Importance BBWI_Relative_Importance  \\\n",
      "0                  0.013858                 0.00988                 0.007467   \n",
      "1                  0.014086                0.020513                 0.012985   \n",
      "2                  0.016428                 0.01153                  0.01164   \n",
      "3                   0.03264                0.025628                 0.025663   \n",
      "4                  0.009046                0.013363                 0.010259   \n",
      "5                  0.003919                 0.00371                 0.005022   \n",
      "6                  0.011429                0.022097                 0.021638   \n",
      "7                  0.006966                0.003766                 0.004775   \n",
      "8                  0.014534                 0.01459                 0.017209   \n",
      "9                  0.028109                0.023645                 0.031177   \n",
      "10                 0.006495                0.004535                 0.004099   \n",
      "11                 0.017969                0.012807                 0.017762   \n",
      "12                 0.006524                0.006995                  0.00714   \n",
      "13                 0.001136                0.004755                 0.004915   \n",
      "14                 0.011664                0.004999                 0.001803   \n",
      "15                 0.001612                0.003175                  0.00176   \n",
      "16                 0.009878                0.015535                 0.017234   \n",
      "\n",
      "   TFC_Relative_Importance CBRE_Relative_Importance  \\\n",
      "0                 0.007494                 0.008979   \n",
      "1                 0.009001                 0.011571   \n",
      "2                 0.011633                 0.009901   \n",
      "3                 0.031652                 0.019922   \n",
      "4                 0.013735                 0.008977   \n",
      "5                 0.004651                 0.003093   \n",
      "6                 0.013658                 0.007903   \n",
      "7                 0.004321                 0.009099   \n",
      "8                 0.012148                 0.013393   \n",
      "9                  0.01756                  0.02522   \n",
      "10                0.005491                 0.008526   \n",
      "11                0.023161                 0.018685   \n",
      "12                0.009109                 0.008843   \n",
      "13                0.001581                 0.005728   \n",
      "14                0.002993                 0.006171   \n",
      "15                0.001425                 0.004147   \n",
      "16                0.011396                 0.014022   \n",
      "\n",
      "    Average_relative_Importance  \n",
      "0                      0.009054  \n",
      "1                      0.016142  \n",
      "2                      0.012009  \n",
      "3                      0.031109  \n",
      "4                      0.015158  \n",
      "5                      0.004455  \n",
      "6                      0.014814  \n",
      "7                      0.006001  \n",
      "8                      0.016140  \n",
      "9                      0.020420  \n",
      "10                     0.005835  \n",
      "11                     0.014652  \n",
      "12                     0.008397  \n",
      "13                     0.003515  \n",
      "14                     0.004529  \n",
      "15                     0.003287  \n",
      "16                     0.015483  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>DIS_Relative_Importance</th>\n",
       "      <th>BIIB_Relative_Importance</th>\n",
       "      <th>PGR_Relative_Importance</th>\n",
       "      <th>SWK_Relative_Importance</th>\n",
       "      <th>PEP_Relative_Importance</th>\n",
       "      <th>ZION_Relative_Importance</th>\n",
       "      <th>LUV_Relative_Importance</th>\n",
       "      <th>BBWI_Relative_Importance</th>\n",
       "      <th>TFC_Relative_Importance</th>\n",
       "      <th>CBRE_Relative_Importance</th>\n",
       "      <th>Average_relative_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bop_14</td>\n",
       "      <td>0.01115</td>\n",
       "      <td>0.034658</td>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>0.012985</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>0.011571</td>\n",
       "      <td>0.016142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cci_14</td>\n",
       "      <td>0.012515</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>0.017664</td>\n",
       "      <td>0.00826</td>\n",
       "      <td>0.010023</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.01153</td>\n",
       "      <td>0.01164</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.012009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>close</td>\n",
       "      <td>0.046393</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>0.04437</td>\n",
       "      <td>0.027619</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.025628</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.031652</td>\n",
       "      <td>0.019922</td>\n",
       "      <td>0.031109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eribull_14</td>\n",
       "      <td>0.01012</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>0.042906</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.010259</td>\n",
       "      <td>0.013735</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.015158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ker_14</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.013093</td>\n",
       "      <td>0.013679</td>\n",
       "      <td>0.015937</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.021638</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.014814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mfi_14</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.009414</td>\n",
       "      <td>0.01182</td>\n",
       "      <td>0.013814</td>\n",
       "      <td>0.014534</td>\n",
       "      <td>0.01459</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.016140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>momentum</td>\n",
       "      <td>0.019631</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.012366</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>0.028109</td>\n",
       "      <td>0.023645</td>\n",
       "      <td>0.031177</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.02522</td>\n",
       "      <td>0.020420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>simple_moving_average</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009297</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.014972</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>0.017969</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.014652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>volume</td>\n",
       "      <td>0.018641</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.018969</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.017234</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.014022</td>\n",
       "      <td>0.015483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Features DIS_Relative_Importance BIIB_Relative_Importance  \\\n",
       "0                 bop_14                 0.01115                 0.034658   \n",
       "1                 cci_14                0.012515                 0.010502   \n",
       "2                  close                0.046393                 0.019462   \n",
       "3             eribull_14                 0.01012                 0.018783   \n",
       "4                 ker_14                0.012999                 0.015705   \n",
       "5                 mfi_14                0.011963                 0.042517   \n",
       "6               momentum                0.019631                 0.008287   \n",
       "7  simple_moving_average                0.009434                 0.009297   \n",
       "8                 volume                0.018641                 0.013938   \n",
       "\n",
       "  PGR_Relative_Importance SWK_Relative_Importance PEP_Relative_Importance  \\\n",
       "0                0.014757                0.014789                0.017913   \n",
       "1                0.017664                 0.00826                0.010023   \n",
       "2                 0.04437                0.027619                0.037742   \n",
       "3                0.014069                0.042906                0.010319   \n",
       "4                0.013093                0.013679                0.015937   \n",
       "5                0.009414                 0.01182                0.013814   \n",
       "6                0.012366                0.025572                0.012631   \n",
       "7                0.013273                0.014972                0.009158   \n",
       "8                0.015431                0.018969                0.019783   \n",
       "\n",
       "  ZION_Relative_Importance LUV_Relative_Importance BBWI_Relative_Importance  \\\n",
       "0                 0.014086                0.020513                 0.012985   \n",
       "1                 0.016428                 0.01153                  0.01164   \n",
       "2                  0.03264                0.025628                 0.025663   \n",
       "3                 0.009046                0.013363                 0.010259   \n",
       "4                 0.011429                0.022097                 0.021638   \n",
       "5                 0.014534                 0.01459                 0.017209   \n",
       "6                 0.028109                0.023645                 0.031177   \n",
       "7                 0.017969                0.012807                 0.017762   \n",
       "8                 0.009878                0.015535                 0.017234   \n",
       "\n",
       "  TFC_Relative_Importance CBRE_Relative_Importance  \\\n",
       "0                0.009001                 0.011571   \n",
       "1                0.011633                 0.009901   \n",
       "2                0.031652                 0.019922   \n",
       "3                0.013735                 0.008977   \n",
       "4                0.013658                 0.007903   \n",
       "5                0.012148                 0.013393   \n",
       "6                 0.01756                  0.02522   \n",
       "7                0.023161                 0.018685   \n",
       "8                0.011396                 0.014022   \n",
       "\n",
       "   Average_relative_Importance  \n",
       "0                     0.016142  \n",
       "1                     0.012009  \n",
       "2                     0.031109  \n",
       "3                     0.015158  \n",
       "4                     0.014814  \n",
       "5                     0.016140  \n",
       "6                     0.020420  \n",
       "7                     0.014652  \n",
       "8                     0.015483  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2.head(2)\n",
    "feat_importance_all=pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "\n",
    "    matching_columns = [col for col in final_df2.columns if ticker in col]\n",
    "#     matching_columns.append([])\n",
    "    # # Print matching columns\n",
    "    # for col in matching_columns:\n",
    "    #     print(col)\n",
    "\n",
    "    temp_df=final_df2[matching_columns]\n",
    "    feat_importance_temp=check_var_importance(temp_df,ticker)\n",
    "    feat_importance_all=pd.concat([feat_importance_all,feat_importance_temp], axis=1)\n",
    "#     print(feat_importance_all)\n",
    "feat_importance_all=feat_importance_all.T.drop_duplicates(keep='first').T\n",
    "feat_importance_all[\"Average_relative_Importance\"]=feat_importance_all.iloc[:,1:].mean(axis=1)\n",
    "print(feat_importance_all)\n",
    "threshold = .01  # Example threshold value\n",
    "\n",
    "# Create a boolean mask based on the condition\n",
    "mask = feat_importance_all['Average_relative_Importance'] >= threshold\n",
    "\n",
    "# Apply the mask to the DataFrame\n",
    "filtered_df = feat_importance_all[mask]\n",
    "\n",
    "# df['Average_Columns'] = df[['Column1', 'Column2']].mean(axis=1)\n",
    "#     print(temp_df)\n",
    "\n",
    "filtered_df.reset_index(drop=True,inplace=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62ec0eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBWI_close</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>BBWI_open</th>\n",
       "      <th>BBWI_high</th>\n",
       "      <th>BBWI_low</th>\n",
       "      <th>BBWI_cci_14</th>\n",
       "      <th>BBWI_stochrsi_14</th>\n",
       "      <th>BBWI_mfi_14</th>\n",
       "      <th>BBWI_bop_14</th>\n",
       "      <th>BBWI_supertrend_14_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>ZION_ker_14</th>\n",
       "      <th>ZION_momentum</th>\n",
       "      <th>ZION_simple_moving_average</th>\n",
       "      <th>ZION_bollinger_bands</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-02-05</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>3568003</td>\n",
       "      <td>23.055780</td>\n",
       "      <td>23.233629</td>\n",
       "      <td>22.926435</td>\n",
       "      <td>97.129259</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.559890</td>\n",
       "      <td>-43.324430</td>\n",
       "      <td>61.16312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435594</td>\n",
       "      <td>1.027182</td>\n",
       "      <td>0.018154</td>\n",
       "      <td>0.716702</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.005423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-06</th>\n",
       "      <td>0.019573</td>\n",
       "      <td>6720374</td>\n",
       "      <td>23.120453</td>\n",
       "      <td>23.678253</td>\n",
       "      <td>23.104284</td>\n",
       "      <td>157.611932</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.675272</td>\n",
       "      <td>-22.967954</td>\n",
       "      <td>61.16312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468398</td>\n",
       "      <td>1.031791</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>0.727985</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.003749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BBWI_close  BBWI_volume  BBWI_open  BBWI_high   BBWI_low  \\\n",
       "Date                                                                   \n",
       "2007-02-05    0.001751      3568003  23.055780  23.233629  22.926435   \n",
       "2007-02-06    0.019573      6720374  23.120453  23.678253  23.104284   \n",
       "\n",
       "            BBWI_cci_14  BBWI_stochrsi_14  BBWI_mfi_14  BBWI_bop_14  \\\n",
       "Date                                                                  \n",
       "2007-02-05    97.129259             100.0     0.559890   -43.324430   \n",
       "2007-02-06   157.611932             100.0     0.675272   -22.967954   \n",
       "\n",
       "            BBWI_supertrend_14_ub  ...  ZION_ker_14  ZION_momentum  \\\n",
       "Date                               ...                               \n",
       "2007-02-05               61.16312  ...     0.435594       1.027182   \n",
       "2007-02-06               61.16312  ...     0.468398       1.031791   \n",
       "\n",
       "            ZION_simple_moving_average  ZION_bollinger_bands  10y2y_spread  \\\n",
       "Date                                                                         \n",
       "2007-02-05                    0.018154              0.716702         -0.11   \n",
       "2007-02-06                    0.019856              0.727985         -0.13   \n",
       "\n",
       "            10y3m_spread  3m_rate  ltiit  ted_spread   var_wti  \n",
       "Date                                                            \n",
       "2007-02-05         -0.34     5.02   2.39        0.34 -0.005423  \n",
       "2007-02-06         -0.37     5.01   2.39        0.35  0.003749  \n",
       "\n",
       "[2 rows x 176 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa75597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIS_bop_14', 'BIIB_bop_14', 'PGR_bop_14', 'SWK_bop_14', 'PEP_bop_14', 'ZION_bop_14', 'LUV_bop_14', 'BBWI_bop_14', 'TFC_bop_14', 'CBRE_bop_14', 'DIS_cci_14', 'BIIB_cci_14', 'PGR_cci_14', 'SWK_cci_14', 'PEP_cci_14', 'ZION_cci_14', 'LUV_cci_14', 'BBWI_cci_14', 'TFC_cci_14', 'CBRE_cci_14', 'DIS_close', 'BIIB_close', 'PGR_close', 'SWK_close', 'PEP_close', 'ZION_close', 'LUV_close', 'BBWI_close', 'TFC_close', 'CBRE_close', 'DIS_eribull_14', 'BIIB_eribull_14', 'PGR_eribull_14', 'SWK_eribull_14', 'PEP_eribull_14', 'ZION_eribull_14', 'LUV_eribull_14', 'BBWI_eribull_14', 'TFC_eribull_14', 'CBRE_eribull_14', 'DIS_ker_14', 'BIIB_ker_14', 'PGR_ker_14', 'SWK_ker_14', 'PEP_ker_14', 'ZION_ker_14', 'LUV_ker_14', 'BBWI_ker_14', 'TFC_ker_14', 'CBRE_ker_14', 'DIS_mfi_14', 'BIIB_mfi_14', 'PGR_mfi_14', 'SWK_mfi_14', 'PEP_mfi_14', 'ZION_mfi_14', 'LUV_mfi_14', 'BBWI_mfi_14', 'TFC_mfi_14', 'CBRE_mfi_14', 'DIS_momentum', 'BIIB_momentum', 'PGR_momentum', 'SWK_momentum', 'PEP_momentum', 'ZION_momentum', 'LUV_momentum', 'BBWI_momentum', 'TFC_momentum', 'CBRE_momentum', 'DIS_simple_moving_average', 'BIIB_simple_moving_average', 'PGR_simple_moving_average', 'SWK_simple_moving_average', 'PEP_simple_moving_average', 'ZION_simple_moving_average', 'LUV_simple_moving_average', 'BBWI_simple_moving_average', 'TFC_simple_moving_average', 'CBRE_simple_moving_average', 'DIS_volume', 'BIIB_volume', 'PGR_volume', 'SWK_volume', 'PEP_volume', 'ZION_volume', 'LUV_volume', 'BBWI_volume', 'TFC_volume', 'CBRE_volume']\n"
     ]
    }
   ],
   "source": [
    "imp_col_names=[filtered_df['Features'].values][0].tolist()\n",
    "\n",
    "# imp_col_names = [ticker +\"_\"+ element for element in imp_col_names]\n",
    "final_col_names = []\n",
    "for col in imp_col_names:\n",
    "    for ticker in tickers:\n",
    "        final_col_names.append(f'{ticker}_{col}')\n",
    "\n",
    "print(final_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10481870",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_col_names=final_col_names+['10y2y_spread','10y3m_spread','3m_rate','ltiit','ted_spread','var_wti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38092fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIS_bop_14',\n",
       " 'BIIB_bop_14',\n",
       " 'PGR_bop_14',\n",
       " 'SWK_bop_14',\n",
       " 'PEP_bop_14',\n",
       " 'ZION_bop_14',\n",
       " 'LUV_bop_14',\n",
       " 'BBWI_bop_14',\n",
       " 'TFC_bop_14',\n",
       " 'CBRE_bop_14',\n",
       " 'DIS_cci_14',\n",
       " 'BIIB_cci_14',\n",
       " 'PGR_cci_14',\n",
       " 'SWK_cci_14',\n",
       " 'PEP_cci_14',\n",
       " 'ZION_cci_14',\n",
       " 'LUV_cci_14',\n",
       " 'BBWI_cci_14',\n",
       " 'TFC_cci_14',\n",
       " 'CBRE_cci_14',\n",
       " 'DIS_close',\n",
       " 'BIIB_close',\n",
       " 'PGR_close',\n",
       " 'SWK_close',\n",
       " 'PEP_close',\n",
       " 'ZION_close',\n",
       " 'LUV_close',\n",
       " 'BBWI_close',\n",
       " 'TFC_close',\n",
       " 'CBRE_close',\n",
       " 'DIS_eribull_14',\n",
       " 'BIIB_eribull_14',\n",
       " 'PGR_eribull_14',\n",
       " 'SWK_eribull_14',\n",
       " 'PEP_eribull_14',\n",
       " 'ZION_eribull_14',\n",
       " 'LUV_eribull_14',\n",
       " 'BBWI_eribull_14',\n",
       " 'TFC_eribull_14',\n",
       " 'CBRE_eribull_14',\n",
       " 'DIS_ker_14',\n",
       " 'BIIB_ker_14',\n",
       " 'PGR_ker_14',\n",
       " 'SWK_ker_14',\n",
       " 'PEP_ker_14',\n",
       " 'ZION_ker_14',\n",
       " 'LUV_ker_14',\n",
       " 'BBWI_ker_14',\n",
       " 'TFC_ker_14',\n",
       " 'CBRE_ker_14',\n",
       " 'DIS_mfi_14',\n",
       " 'BIIB_mfi_14',\n",
       " 'PGR_mfi_14',\n",
       " 'SWK_mfi_14',\n",
       " 'PEP_mfi_14',\n",
       " 'ZION_mfi_14',\n",
       " 'LUV_mfi_14',\n",
       " 'BBWI_mfi_14',\n",
       " 'TFC_mfi_14',\n",
       " 'CBRE_mfi_14',\n",
       " 'DIS_momentum',\n",
       " 'BIIB_momentum',\n",
       " 'PGR_momentum',\n",
       " 'SWK_momentum',\n",
       " 'PEP_momentum',\n",
       " 'ZION_momentum',\n",
       " 'LUV_momentum',\n",
       " 'BBWI_momentum',\n",
       " 'TFC_momentum',\n",
       " 'CBRE_momentum',\n",
       " 'DIS_simple_moving_average',\n",
       " 'BIIB_simple_moving_average',\n",
       " 'PGR_simple_moving_average',\n",
       " 'SWK_simple_moving_average',\n",
       " 'PEP_simple_moving_average',\n",
       " 'ZION_simple_moving_average',\n",
       " 'LUV_simple_moving_average',\n",
       " 'BBWI_simple_moving_average',\n",
       " 'TFC_simple_moving_average',\n",
       " 'CBRE_simple_moving_average',\n",
       " 'DIS_volume',\n",
       " 'BIIB_volume',\n",
       " 'PGR_volume',\n",
       " 'SWK_volume',\n",
       " 'PEP_volume',\n",
       " 'ZION_volume',\n",
       " 'LUV_volume',\n",
       " 'BBWI_volume',\n",
       " 'TFC_volume',\n",
       " 'CBRE_volume',\n",
       " '10y2y_spread',\n",
       " '10y3m_spread',\n",
       " '3m_rate',\n",
       " 'ltiit',\n",
       " 'ted_spread',\n",
       " 'var_wti']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c1602f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DIS_bop_14</th>\n",
       "      <th>BIIB_bop_14</th>\n",
       "      <th>PGR_bop_14</th>\n",
       "      <th>SWK_bop_14</th>\n",
       "      <th>PEP_bop_14</th>\n",
       "      <th>ZION_bop_14</th>\n",
       "      <th>LUV_bop_14</th>\n",
       "      <th>BBWI_bop_14</th>\n",
       "      <th>TFC_bop_14</th>\n",
       "      <th>CBRE_bop_14</th>\n",
       "      <th>...</th>\n",
       "      <th>LUV_volume</th>\n",
       "      <th>BBWI_volume</th>\n",
       "      <th>TFC_volume</th>\n",
       "      <th>CBRE_volume</th>\n",
       "      <th>10y2y_spread</th>\n",
       "      <th>10y3m_spread</th>\n",
       "      <th>3m_rate</th>\n",
       "      <th>ltiit</th>\n",
       "      <th>ted_spread</th>\n",
       "      <th>var_wti</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-02-05</th>\n",
       "      <td>-17.075763</td>\n",
       "      <td>-0.231710</td>\n",
       "      <td>-37.175131</td>\n",
       "      <td>-15.557579</td>\n",
       "      <td>-44.319380</td>\n",
       "      <td>-19.781446</td>\n",
       "      <td>-5.701382</td>\n",
       "      <td>-43.324430</td>\n",
       "      <td>-48.903791</td>\n",
       "      <td>0.139236</td>\n",
       "      <td>...</td>\n",
       "      <td>7286100</td>\n",
       "      <td>3568003</td>\n",
       "      <td>1937500</td>\n",
       "      <td>1558600</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.005423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-06</th>\n",
       "      <td>-15.459108</td>\n",
       "      <td>-0.081969</td>\n",
       "      <td>-63.277943</td>\n",
       "      <td>-22.295886</td>\n",
       "      <td>-42.985149</td>\n",
       "      <td>-20.958373</td>\n",
       "      <td>-5.687057</td>\n",
       "      <td>-22.967954</td>\n",
       "      <td>-60.025535</td>\n",
       "      <td>-0.098765</td>\n",
       "      <td>...</td>\n",
       "      <td>7018700</td>\n",
       "      <td>6720374</td>\n",
       "      <td>1164400</td>\n",
       "      <td>2146900</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>5.01</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.003749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-07</th>\n",
       "      <td>-14.284782</td>\n",
       "      <td>0.610391</td>\n",
       "      <td>-48.795845</td>\n",
       "      <td>-25.842834</td>\n",
       "      <td>-50.882284</td>\n",
       "      <td>-29.466748</td>\n",
       "      <td>-9.059952</td>\n",
       "      <td>-16.551140</td>\n",
       "      <td>-83.568391</td>\n",
       "      <td>0.320224</td>\n",
       "      <td>...</td>\n",
       "      <td>6945300</td>\n",
       "      <td>5023086</td>\n",
       "      <td>1034700</td>\n",
       "      <td>3448700</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.019691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-08</th>\n",
       "      <td>-5.632069</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>-46.440357</td>\n",
       "      <td>-27.547278</td>\n",
       "      <td>-18.328859</td>\n",
       "      <td>-26.015268</td>\n",
       "      <td>-6.967289</td>\n",
       "      <td>-27.415793</td>\n",
       "      <td>-50.676600</td>\n",
       "      <td>0.082194</td>\n",
       "      <td>...</td>\n",
       "      <td>6835400</td>\n",
       "      <td>3266546</td>\n",
       "      <td>1070300</td>\n",
       "      <td>1572800</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>5.03</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.034805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-09</th>\n",
       "      <td>-4.609494</td>\n",
       "      <td>-0.245455</td>\n",
       "      <td>-27.631604</td>\n",
       "      <td>-15.479156</td>\n",
       "      <td>-14.433311</td>\n",
       "      <td>-22.820337</td>\n",
       "      <td>-6.964341</td>\n",
       "      <td>-22.552091</td>\n",
       "      <td>-47.461276</td>\n",
       "      <td>-0.575667</td>\n",
       "      <td>...</td>\n",
       "      <td>6758400</td>\n",
       "      <td>5109923</td>\n",
       "      <td>1602800</td>\n",
       "      <td>3252800</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.001673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-12</th>\n",
       "      <td>0.010308</td>\n",
       "      <td>-0.238938</td>\n",
       "      <td>-0.864706</td>\n",
       "      <td>-1.373918</td>\n",
       "      <td>-2.045917</td>\n",
       "      <td>-1.803389</td>\n",
       "      <td>0.180817</td>\n",
       "      <td>-0.716619</td>\n",
       "      <td>-1.991529</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>...</td>\n",
       "      <td>8130800</td>\n",
       "      <td>3131200</td>\n",
       "      <td>3948000</td>\n",
       "      <td>2029500</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-13</th>\n",
       "      <td>0.366072</td>\n",
       "      <td>0.589164</td>\n",
       "      <td>0.189750</td>\n",
       "      <td>-0.471402</td>\n",
       "      <td>-1.174611</td>\n",
       "      <td>-1.948663</td>\n",
       "      <td>0.377454</td>\n",
       "      <td>0.156576</td>\n",
       "      <td>-3.175125</td>\n",
       "      <td>-0.047060</td>\n",
       "      <td>...</td>\n",
       "      <td>10121900</td>\n",
       "      <td>3244200</td>\n",
       "      <td>4792400</td>\n",
       "      <td>2285100</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-14</th>\n",
       "      <td>0.198891</td>\n",
       "      <td>0.269312</td>\n",
       "      <td>-0.669892</td>\n",
       "      <td>-1.868440</td>\n",
       "      <td>-0.856435</td>\n",
       "      <td>-2.680103</td>\n",
       "      <td>-0.561558</td>\n",
       "      <td>-1.324555</td>\n",
       "      <td>-4.036404</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>...</td>\n",
       "      <td>5987800</td>\n",
       "      <td>4278500</td>\n",
       "      <td>6074200</td>\n",
       "      <td>1995200</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>0.874042</td>\n",
       "      <td>0.454293</td>\n",
       "      <td>0.177281</td>\n",
       "      <td>-2.093828</td>\n",
       "      <td>-2.026866</td>\n",
       "      <td>-0.812139</td>\n",
       "      <td>-0.231170</td>\n",
       "      <td>0.220932</td>\n",
       "      <td>-0.818140</td>\n",
       "      <td>0.832001</td>\n",
       "      <td>...</td>\n",
       "      <td>5121500</td>\n",
       "      <td>4123700</td>\n",
       "      <td>6939500</td>\n",
       "      <td>1831500</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>-0.336737</td>\n",
       "      <td>-0.562721</td>\n",
       "      <td>-0.978708</td>\n",
       "      <td>-2.032164</td>\n",
       "      <td>-2.493611</td>\n",
       "      <td>-1.768889</td>\n",
       "      <td>-0.897394</td>\n",
       "      <td>-0.162671</td>\n",
       "      <td>-2.297897</td>\n",
       "      <td>-0.459118</td>\n",
       "      <td>...</td>\n",
       "      <td>5433800</td>\n",
       "      <td>5343900</td>\n",
       "      <td>6735400</td>\n",
       "      <td>1907700</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3890 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DIS_bop_14  BIIB_bop_14  PGR_bop_14  SWK_bop_14  PEP_bop_14  \\\n",
       "Date                                                                      \n",
       "2007-02-05  -17.075763    -0.231710  -37.175131  -15.557579  -44.319380   \n",
       "2007-02-06  -15.459108    -0.081969  -63.277943  -22.295886  -42.985149   \n",
       "2007-02-07  -14.284782     0.610391  -48.795845  -25.842834  -50.882284   \n",
       "2007-02-08   -5.632069     0.564706  -46.440357  -27.547278  -18.328859   \n",
       "2007-02-09   -4.609494    -0.245455  -27.631604  -15.479156  -14.433311   \n",
       "...                ...          ...         ...         ...         ...   \n",
       "2022-07-12    0.010308    -0.238938   -0.864706   -1.373918   -2.045917   \n",
       "2022-07-13    0.366072     0.589164    0.189750   -0.471402   -1.174611   \n",
       "2022-07-14    0.198891     0.269312   -0.669892   -1.868440   -0.856435   \n",
       "2022-07-15    0.874042     0.454293    0.177281   -2.093828   -2.026866   \n",
       "2022-07-18   -0.336737    -0.562721   -0.978708   -2.032164   -2.493611   \n",
       "\n",
       "            ZION_bop_14  LUV_bop_14  BBWI_bop_14  TFC_bop_14  CBRE_bop_14  \\\n",
       "Date                                                                        \n",
       "2007-02-05   -19.781446   -5.701382   -43.324430  -48.903791     0.139236   \n",
       "2007-02-06   -20.958373   -5.687057   -22.967954  -60.025535    -0.098765   \n",
       "2007-02-07   -29.466748   -9.059952   -16.551140  -83.568391     0.320224   \n",
       "2007-02-08   -26.015268   -6.967289   -27.415793  -50.676600     0.082194   \n",
       "2007-02-09   -22.820337   -6.964341   -22.552091  -47.461276    -0.575667   \n",
       "...                 ...         ...          ...         ...          ...   \n",
       "2022-07-12    -1.803389    0.180817    -0.716619   -1.991529     0.641791   \n",
       "2022-07-13    -1.948663    0.377454     0.156576   -3.175125    -0.047060   \n",
       "2022-07-14    -2.680103   -0.561558    -1.324555   -4.036404     0.836601   \n",
       "2022-07-15    -0.812139   -0.231170     0.220932   -0.818140     0.832001   \n",
       "2022-07-18    -1.768889   -0.897394    -0.162671   -2.297897    -0.459118   \n",
       "\n",
       "            ...  LUV_volume  BBWI_volume  TFC_volume  CBRE_volume  \\\n",
       "Date        ...                                                     \n",
       "2007-02-05  ...     7286100      3568003     1937500      1558600   \n",
       "2007-02-06  ...     7018700      6720374     1164400      2146900   \n",
       "2007-02-07  ...     6945300      5023086     1034700      3448700   \n",
       "2007-02-08  ...     6835400      3266546     1070300      1572800   \n",
       "2007-02-09  ...     6758400      5109923     1602800      3252800   \n",
       "...         ...         ...          ...         ...          ...   \n",
       "2022-07-12  ...     8130800      3131200     3948000      2029500   \n",
       "2022-07-13  ...    10121900      3244200     4792400      2285100   \n",
       "2022-07-14  ...     5987800      4278500     6074200      1995200   \n",
       "2022-07-15  ...     5121500      4123700     6939500      1831500   \n",
       "2022-07-18  ...     5433800      5343900     6735400      1907700   \n",
       "\n",
       "            10y2y_spread  10y3m_spread  3m_rate  ltiit  ted_spread   var_wti  \n",
       "Date                                                                          \n",
       "2007-02-05         -0.11         -0.34     5.02   2.39        0.34 -0.005423  \n",
       "2007-02-06         -0.13         -0.37     5.01   2.39        0.35  0.003749  \n",
       "2007-02-07         -0.13         -0.41     5.02   2.38        0.34 -0.019691  \n",
       "2007-02-08         -0.14         -0.43     5.03   2.37        0.33  0.034805  \n",
       "2007-02-09         -0.13         -0.36     5.02   2.41        0.34  0.001673  \n",
       "...                  ...           ...      ...    ...         ...       ...  \n",
       "2022-07-12         -0.07          0.74     2.16   1.12         NaN  0.000000  \n",
       "2022-07-13         -0.22          0.52     2.33   1.06         NaN  0.000000  \n",
       "2022-07-14         -0.19          0.56     2.33   1.07         NaN  0.000000  \n",
       "2022-07-15         -0.20          0.56     2.29   1.03         NaN  0.000000  \n",
       "2022-07-18         -0.19          0.46      NaN    NaN         NaN  0.000000  \n",
       "\n",
       "[3890 rows x 96 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data=final_df2[final_col_names]\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df01faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    \"\"\"This class takes time series data that is in a sequential format, transforming\n",
    "    it into pairs of inputs and labels, so that the inputs are windows of consecutive\n",
    "    samples from the data.\n",
    "    \"\"\"\n",
    "    def __init__(self,input_width=5,label_width=1,shift=1, train_df=None, val_df=None,\n",
    "                 test_df=None, label_columns=None,batch_size=None,shuffle=False):\n",
    "        \"\"\"This method initiates the WindowGenerator class.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "        input_width (int, default=5): the width of the window, which represents the \n",
    "            amount of time steps from the earliest input observation to the last.\n",
    "        label_width (int, default=1): the width of the label. This determines the amount\n",
    "             of time steps that will be predicted.\n",
    "        shift (int, default=1): jump between the last input in the window and the first \n",
    "            label.\n",
    "        train_df (pandas Dataframe, default=None): array-like object containing the train \n",
    "            data which comes in a time series format.\n",
    "        val_df (pandas Dataframe, default=None): array-like object containing the \n",
    "            validation data.\n",
    "        test_df (pandas Dataframe, default=None): array-like object containing the test \n",
    "            data.\n",
    "        label_columns (list|string, default=None): name of the column(s) that are used \n",
    "            as labels.\n",
    "        batch_size (int, deafault=None): the size of the batches of the tf.data.Dataset\n",
    "            object (whose dimensions are (batch,input_width,features) for the input and\n",
    "            (batch,label_width,label_columns) for the labels).\n",
    "        shuffle (boolean, default=False): determines if the data inside the tf.data.Dataset\n",
    "            is shuffled.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Define attributes of the class:\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.label_columns = label_columns\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Define information about columns:\n",
    "        if isinstance(label_columns,type(None)):\n",
    "            self.label_columns_indices = {name:i for i,name in enumerate(label_columns)}\n",
    "        self.column_indices = {name:i for i,name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Define window information:\n",
    "        self.total_window_size = input_width+shift\n",
    "        self.input_slice = slice(0,input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size-self.label_width\n",
    "        self.labels_slice = slice(self.label_start,None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"This method determines what is returned when an instance of the object\n",
    "        is called\n",
    "        \"\"\"\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'\n",
    "        ])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        \"\"\"This method converts a list of consecutive inputs to a window of\n",
    "        inputs and a window of labels.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "        features (pandas Dataset): features in the dataframe\n",
    "\n",
    "        Outputs:\n",
    "        --------\n",
    "        inputs ()\n",
    "        \"\"\"\n",
    "        inputs = features[:, self.input_slice,:]\n",
    "        labels = features[:,self.labels_slice,:]\n",
    "        if not isinstance(self.label_columns,type(None)):\n",
    "            labels = tf.stack(\n",
    "                [labels[:,:,self.column_indices[name]] for name in self.label_columns],\n",
    "                axis = -1\n",
    "            )\n",
    "        \n",
    "        # Set the shapes of the informaiton:\n",
    "        inputs.set_shape([None,self.input_width,None])\n",
    "        labels.set_shape([None,self.label_width,None])\n",
    "\n",
    "        return inputs,labels\n",
    "    \n",
    "    def make_dataset(self,data):\n",
    "        \"\"\"This method takes a time series DataFrame and convert it to a \n",
    "        tf.data.Dataset of (input_window,label_window) pairs, using the\n",
    "        tf.keras.preprocessing.timeseries_dataset_from_array function.\n",
    "\n",
    "        Input:\n",
    "        ------\n",
    "        data (pandas DataFrame): dataframe containing the time series information\n",
    "            of the inputs and labels, which will transformed into windows and then \n",
    "            a tf.Dataset object.\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data = data,\n",
    "            targets = None,\n",
    "            sequence_length = self.total_window_size,\n",
    "            sequence_stride = 1,\n",
    "            shuffle = self.shuffle,\n",
    "            batch_size = self.batch_size\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        print(\"************************************************\")\n",
    "        print(ds)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    # Adding properties for accessing the train, val and test as tf.data.Dataset objects\n",
    "    @property\n",
    "    def train(self):\n",
    "        if isinstance(self.train_df,type(None)):\n",
    "            return None\n",
    "        else:\n",
    "            return self.make_dataset(self.train_df)\n",
    "        print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")    \n",
    "        print(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        if isinstance(self.val_df,type(None)):\n",
    "            return None\n",
    "        else:\n",
    "            return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        if isinstance(self.test_df,type(None)):\n",
    "            return None\n",
    "        else:\n",
    "            return self.make_dataset(self.test_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08ae0298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIS_close', 'BIIB_close', 'PGR_close', 'SWK_close', 'PEP_close', 'ZION_close', 'LUV_close', 'BBWI_close', 'TFC_close', 'CBRE_close']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/mxqr91nj7135zkb7ptp3mq5c0000gn/T/ipykernel_13390/3367714174.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_data.fillna(method='ffill',inplace=True)\n",
      "/var/folders/dk/mxqr91nj7135zkb7ptp3mq5c0000gn/T/ipykernel_13390/3367714174.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_data.fillna(method='bfill',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Define the label columns:\n",
    "\n",
    "# total_df=final_df2.copy()\n",
    "# total_df.set_index('Date',inplace=True)\n",
    "# label_cols = list(total_df.columns)\n",
    "# label_cols=label_cols[:len(tickers)]\n",
    "\n",
    "\n",
    "\n",
    "# matching_columns = [col for col in df.columns if \"close\" in col]\n",
    "model_data.fillna(method='ffill',inplace=True)\n",
    "model_data.fillna(method='bfill',inplace=True)\n",
    "\n",
    "model_data\n",
    "    \n",
    "label_cols=[col for col in model_data.columns if \"close\" in col]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ['AAPL_returns', 'GOOGL_returns', 'MSFT_returns', 'SPY_returns']\n",
    "print(label_cols)\n",
    "\n",
    "# Define train (70%), val (20%) and test (10%) dataframes: \n",
    "train_p, val_p, test_p = 0.7,0.2,0.1\n",
    "window_size = 5\n",
    "num_features = model_data.shape[1]\n",
    "total_size = len(model_data)\n",
    "train_size = int(total_size*train_p)\n",
    "val_size = int(total_size*val_p)\n",
    "test_size = int(total_size*test_p)\n",
    "train_df = model_data.iloc[:train_size,:]\n",
    "val_df = model_data.iloc[train_size-window_size:train_size+val_size,:]\n",
    "test_df = model_data.iloc[train_size+val_size-window_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d6d6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DIS_bop_14', 'BIIB_bop_14', 'PGR_bop_14', 'SWK_bop_14', 'PEP_bop_14',\n",
       "       'ZION_bop_14', 'LUV_bop_14', 'BBWI_bop_14', 'TFC_bop_14', 'CBRE_bop_14',\n",
       "       'DIS_cci_14', 'BIIB_cci_14', 'PGR_cci_14', 'SWK_cci_14', 'PEP_cci_14',\n",
       "       'ZION_cci_14', 'LUV_cci_14', 'BBWI_cci_14', 'TFC_cci_14', 'CBRE_cci_14',\n",
       "       'DIS_close', 'BIIB_close', 'PGR_close', 'SWK_close', 'PEP_close',\n",
       "       'ZION_close', 'LUV_close', 'BBWI_close', 'TFC_close', 'CBRE_close',\n",
       "       'DIS_eribull_14', 'BIIB_eribull_14', 'PGR_eribull_14', 'SWK_eribull_14',\n",
       "       'PEP_eribull_14', 'ZION_eribull_14', 'LUV_eribull_14',\n",
       "       'BBWI_eribull_14', 'TFC_eribull_14', 'CBRE_eribull_14', 'DIS_ker_14',\n",
       "       'BIIB_ker_14', 'PGR_ker_14', 'SWK_ker_14', 'PEP_ker_14', 'ZION_ker_14',\n",
       "       'LUV_ker_14', 'BBWI_ker_14', 'TFC_ker_14', 'CBRE_ker_14', 'DIS_mfi_14',\n",
       "       'BIIB_mfi_14', 'PGR_mfi_14', 'SWK_mfi_14', 'PEP_mfi_14', 'ZION_mfi_14',\n",
       "       'LUV_mfi_14', 'BBWI_mfi_14', 'TFC_mfi_14', 'CBRE_mfi_14',\n",
       "       'DIS_momentum', 'BIIB_momentum', 'PGR_momentum', 'SWK_momentum',\n",
       "       'PEP_momentum', 'ZION_momentum', 'LUV_momentum', 'BBWI_momentum',\n",
       "       'TFC_momentum', 'CBRE_momentum', 'DIS_simple_moving_average',\n",
       "       'BIIB_simple_moving_average', 'PGR_simple_moving_average',\n",
       "       'SWK_simple_moving_average', 'PEP_simple_moving_average',\n",
       "       'ZION_simple_moving_average', 'LUV_simple_moving_average',\n",
       "       'BBWI_simple_moving_average', 'TFC_simple_moving_average',\n",
       "       'CBRE_simple_moving_average', 'DIS_volume', 'BIIB_volume', 'PGR_volume',\n",
       "       'SWK_volume', 'PEP_volume', 'ZION_volume', 'LUV_volume', 'BBWI_volume',\n",
       "       'TFC_volume', 'CBRE_volume', '10y2y_spread', '10y3m_spread', '3m_rate',\n",
       "       'ltiit', 'ted_spread', 'var_wti'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2af088d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total window size: 6\n",
      "Input indices: [0 1 2 3 4]\n",
      "Label indices: [5]\n",
      "Label column name(s): ['DIS_close', 'BIIB_close', 'PGR_close', 'SWK_close', 'PEP_close', 'ZION_close', 'LUV_close', 'BBWI_close', 'TFC_close', 'CBRE_close']\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Train input shape: (512, 5, 96)\n",
      "Train target shape: (512, 1, 10)\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Validation input shape: (512, 5, 96)\n",
      "Validation target shape: (512, 1, 10)\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Test input shape: (389, 5, 96)\n",
      "Test target shape: (389, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size:\n",
    "batch_size = 512\n",
    "\n",
    "# Create an instance of the WindowGenerator object:\n",
    "my_window = WindowGenerator(input_width=window_size,label_width=1,shift=1,train_df=train_df,val_df=val_df,\n",
    "                            test_df=test_df,label_columns=label_cols,\n",
    "                            batch_size=batch_size,shuffle=True)\n",
    "print(my_window)\n",
    "\n",
    "# Print the shapes for one batch of each sub dataset:\n",
    "for example_inputs, example_labels in my_window.train.take(1):\n",
    "    print(\"Train input shape:\",example_inputs.shape)\n",
    "    print(\"Train target shape:\",example_labels.shape)\n",
    "for example_inputs, example_labels in my_window.val.take(1):\n",
    "    print(\"Validation input shape:\",example_inputs.shape)\n",
    "    print(\"Validation target shape:\",example_labels.shape)\n",
    "for example_inputs, example_labels in my_window.test.take(1):\n",
    "    print(\"Test input shape:\",example_inputs.shape)\n",
    "    print(\"Test target shape:\",example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5edca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def initial_eda(dataset):\n",
    "#     for batch in dataset.take(1):  # Assuming you want to analyze the first batch\n",
    "#         print(\"Dimensions:\", batch[0].shape[0], \"samples,\", batch[0].shape[1], \"features\")\n",
    "#         print(\"Labels shape:\", batch[1].shape)\n",
    "#         print(\"Labels sample:\", batch[1].numpy()[:5])  # Printing first 5 labels as an example\n",
    "#         print(\"Features sample:\", batch[0].numpy()[:5])\n",
    "#         print(\"dataframe of the features\")\n",
    "#         print(pd.DataFrame(batch[0].numpy()[:5]))\n",
    "# # Assuming my_window.train is your TensorFlow dataset\n",
    "# initial_eda(my_window.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ac977e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88a5f9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1220184543019634e-08"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-8 * 10**(1/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d201bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-8 * 10**(100/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf88a659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579\n",
      "Epoch 1: Learning Rate = 9.99999993922529e-09, Loss = 0.0027095370460301638\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.02110, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 603ms/step - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579 - val_loss: 0.0211 - val_root_mean_squared_error: 0.2054 - val_mae: 0.1486 - lr: 1.0000e-08\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0737 - mae: 0.0579\n",
      "Epoch 2: Learning Rate = 1.122018478127984e-08, Loss = 0.002709662541747093\n",
      "\n",
      "Epoch 2: val_loss improved from 0.02110 to 0.01880, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 515ms/step - loss: 0.0027 - root_mean_squared_error: 0.0737 - mae: 0.0579 - val_loss: 0.0188 - val_root_mean_squared_error: 0.1939 - val_mae: 0.1350 - lr: 1.1220e-08\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579\n",
      "Epoch 3: Learning Rate = 1.2589254083650303e-08, Loss = 0.002712674904614687\n",
      "\n",
      "Epoch 3: val_loss improved from 0.01880 to 0.01663, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 549ms/step - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1824 - val_mae: 0.1256 - lr: 1.2589e-08\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579\n",
      "Epoch 4: Learning Rate = 1.4125375535911644e-08, Loss = 0.0027066778857260942\n",
      "\n",
      "Epoch 4: val_loss improved from 0.01663 to 0.01466, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 544ms/step - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579 - val_loss: 0.0147 - val_root_mean_squared_error: 0.1711 - val_mae: 0.1178 - lr: 1.4125e-08\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579\n",
      "Epoch 5: Learning Rate = 1.5848931056439142e-08, Loss = 0.002705493476241827\n",
      "\n",
      "Epoch 5: val_loss improved from 0.01466 to 0.01294, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 544ms/step - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579 - val_loss: 0.0129 - val_root_mean_squared_error: 0.1610 - val_mae: 0.1112 - lr: 1.5849e-08\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0579\n",
      "Epoch 6: Learning Rate = 1.7782793904075334e-08, Loss = 0.002706842264160514\n",
      "\n",
      "Epoch 6: val_loss improved from 0.01294 to 0.01148, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 541ms/step - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0579 - val_loss: 0.0115 - val_root_mean_squared_error: 0.1517 - val_mae: 0.1051 - lr: 1.7783e-08\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579\n",
      "Epoch 7: Learning Rate = 1.9952622665186937e-08, Loss = 0.0027078415732830763\n",
      "\n",
      "Epoch 7: val_loss improved from 0.01148 to 0.01026, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 556ms/step - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579 - val_loss: 0.0103 - val_root_mean_squared_error: 0.1432 - val_mae: 0.0998 - lr: 1.9953e-08\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579\n",
      "Epoch 8: Learning Rate = 2.238721208414063e-08, Loss = 0.0027082390151917934\n",
      "\n",
      "Epoch 8: val_loss improved from 0.01026 to 0.00923, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 547ms/step - loss: 0.0027 - root_mean_squared_error: 0.0736 - mae: 0.0579 - val_loss: 0.0092 - val_root_mean_squared_error: 0.1362 - val_mae: 0.0951 - lr: 2.2387e-08\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578\n",
      "Epoch 9: Learning Rate = 2.5118863433704064e-08, Loss = 0.002703165402635932\n",
      "\n",
      "Epoch 9: val_loss improved from 0.00923 to 0.00835, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 526ms/step - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578 - val_loss: 0.0083 - val_root_mean_squared_error: 0.1295 - val_mae: 0.0909 - lr: 2.5119e-08\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0579\n",
      "Epoch 10: Learning Rate = 2.8183828604255723e-08, Loss = 0.00270278612151742\n",
      "\n",
      "Epoch 10: val_loss improved from 0.00835 to 0.00761, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 556ms/step - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0579 - val_loss: 0.0076 - val_root_mean_squared_error: 0.1233 - val_mae: 0.0871 - lr: 2.8184e-08\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0579\n",
      "Epoch 11: Learning Rate = 3.1622775509276835e-08, Loss = 0.0026989190373569727\n",
      "\n",
      "Epoch 11: val_loss improved from 0.00761 to 0.00698, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 562ms/step - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0579 - val_loss: 0.0070 - val_root_mean_squared_error: 0.1182 - val_mae: 0.0839 - lr: 3.1623e-08\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0577\n",
      "Epoch 12: Learning Rate = 3.548133875597159e-08, Loss = 0.0026947816368192434\n",
      "\n",
      "Epoch 12: val_loss improved from 0.00698 to 0.00645, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 531ms/step - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0577 - val_loss: 0.0065 - val_root_mean_squared_error: 0.1138 - val_mae: 0.0813 - lr: 3.5481e-08\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0578\n",
      "Epoch 13: Learning Rate = 3.981071827752203e-08, Loss = 0.002697268035262823\n",
      "\n",
      "Epoch 13: val_loss improved from 0.00645 to 0.00599, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 560ms/step - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0578 - val_loss: 0.0060 - val_root_mean_squared_error: 0.1099 - val_mae: 0.0790 - lr: 3.9811e-08\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0578\n",
      "Epoch 14: Learning Rate = 4.466835790140067e-08, Loss = 0.002697056857869029\n",
      "\n",
      "Epoch 14: val_loss improved from 0.00599 to 0.00561, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 0.0027 - root_mean_squared_error: 0.0735 - mae: 0.0578 - val_loss: 0.0056 - val_root_mean_squared_error: 0.1062 - val_mae: 0.0768 - lr: 4.4668e-08\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578\n",
      "Epoch 15: Learning Rate = 5.011872161730935e-08, Loss = 0.002693706192076206\n",
      "\n",
      "Epoch 15: val_loss improved from 0.00561 to 0.00528, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 524ms/step - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578 - val_loss: 0.0053 - val_root_mean_squared_error: 0.1028 - val_mae: 0.0747 - lr: 5.0119e-08\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578\n",
      "Epoch 16: Learning Rate = 5.623413201760741e-08, Loss = 0.0026940349489450455\n",
      "\n",
      "Epoch 16: val_loss improved from 0.00528 to 0.00500, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 564ms/step - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578 - val_loss: 0.0050 - val_root_mean_squared_error: 0.1001 - val_mae: 0.0731 - lr: 5.6234e-08\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578\n",
      "Epoch 17: Learning Rate = 6.309573308271865e-08, Loss = 0.002689026528969407\n",
      "\n",
      "Epoch 17: val_loss improved from 0.00500 to 0.00475, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 562ms/step - loss: 0.0027 - root_mean_squared_error: 0.0734 - mae: 0.0578 - val_loss: 0.0048 - val_root_mean_squared_error: 0.0976 - val_mae: 0.0717 - lr: 6.3096e-08\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0577\n",
      "Epoch 18: Learning Rate = 7.079457731151706e-08, Loss = 0.0026850563008338213\n",
      "\n",
      "Epoch 18: val_loss improved from 0.00475 to 0.00455, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 530ms/step - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0577 - val_loss: 0.0045 - val_root_mean_squared_error: 0.0956 - val_mae: 0.0708 - lr: 7.0795e-08\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0577\n",
      "Epoch 19: Learning Rate = 7.943282298583654e-08, Loss = 0.002682984806597233\n",
      "\n",
      "Epoch 19: val_loss improved from 0.00455 to 0.00437, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 556ms/step - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0577 - val_loss: 0.0044 - val_root_mean_squared_error: 0.0933 - val_mae: 0.0694 - lr: 7.9433e-08\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0576\n",
      "Epoch 20: Learning Rate = 8.91250948598099e-08, Loss = 0.002680414356291294\n",
      "\n",
      "Epoch 20: val_loss improved from 0.00437 to 0.00421, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 551ms/step - loss: 0.0027 - root_mean_squared_error: 0.0733 - mae: 0.0576 - val_loss: 0.0042 - val_root_mean_squared_error: 0.0918 - val_mae: 0.0687 - lr: 8.9125e-08\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0731 - mae: 0.0575\n",
      "Epoch 21: Learning Rate = 1.0000000116860974e-07, Loss = 0.0026735984720289707\n",
      "\n",
      "Epoch 21: val_loss improved from 0.00421 to 0.00407, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 525ms/step - loss: 0.0027 - root_mean_squared_error: 0.0731 - mae: 0.0575 - val_loss: 0.0041 - val_root_mean_squared_error: 0.0899 - val_mae: 0.0675 - lr: 1.0000e-07\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0731 - mae: 0.0575\n",
      "Epoch 22: Learning Rate = 1.1220184603644157e-07, Loss = 0.0026696932036429644\n",
      "\n",
      "Epoch 22: val_loss improved from 0.00407 to 0.00396, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 577ms/step - loss: 0.0027 - root_mean_squared_error: 0.0731 - mae: 0.0575 - val_loss: 0.0040 - val_root_mean_squared_error: 0.0891 - val_mae: 0.0672 - lr: 1.1220e-07\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0730 - mae: 0.0574\n",
      "Epoch 23: Learning Rate = 1.2589254083650303e-07, Loss = 0.0026623818557709455\n",
      "\n",
      "Epoch 23: val_loss improved from 0.00396 to 0.00385, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 0.0027 - root_mean_squared_error: 0.0730 - mae: 0.0574 - val_loss: 0.0039 - val_root_mean_squared_error: 0.0878 - val_mae: 0.0667 - lr: 1.2589e-07\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0730 - mae: 0.0574\n",
      "Epoch 24: Learning Rate = 1.4125375003004592e-07, Loss = 0.0026564516592770815\n",
      "\n",
      "Epoch 24: val_loss improved from 0.00385 to 0.00376, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 519ms/step - loss: 0.0027 - root_mean_squared_error: 0.0730 - mae: 0.0574 - val_loss: 0.0038 - val_root_mean_squared_error: 0.0871 - val_mae: 0.0663 - lr: 1.4125e-07\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0729 - mae: 0.0574\n",
      "Epoch 25: Learning Rate = 1.584893141171051e-07, Loss = 0.0026476772036403418\n",
      "\n",
      "Epoch 25: val_loss improved from 0.00376 to 0.00368, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 558ms/step - loss: 0.0026 - root_mean_squared_error: 0.0729 - mae: 0.0574 - val_loss: 0.0037 - val_root_mean_squared_error: 0.0859 - val_mae: 0.0656 - lr: 1.5849e-07\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0728 - mae: 0.0573\n",
      "Epoch 26: Learning Rate = 1.7782794259346701e-07, Loss = 0.0026451710145920515\n",
      "\n",
      "Epoch 26: val_loss improved from 0.00368 to 0.00361, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 548ms/step - loss: 0.0026 - root_mean_squared_error: 0.0728 - mae: 0.0573 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0853 - val_mae: 0.0654 - lr: 1.7783e-07\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0727 - mae: 0.0572\n",
      "Epoch 27: Learning Rate = 1.9952622665186937e-07, Loss = 0.002635543467476964\n",
      "\n",
      "Epoch 27: val_loss improved from 0.00361 to 0.00354, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 532ms/step - loss: 0.0026 - root_mean_squared_error: 0.0727 - mae: 0.0572 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0840 - val_mae: 0.0647 - lr: 1.9953e-07\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0725 - mae: 0.0570\n",
      "Epoch 28: Learning Rate = 2.2387212084140629e-07, Loss = 0.0026238281279802322\n",
      "\n",
      "Epoch 28: val_loss improved from 0.00354 to 0.00349, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 557ms/step - loss: 0.0026 - root_mean_squared_error: 0.0725 - mae: 0.0570 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0833 - val_mae: 0.0642 - lr: 2.2387e-07\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0723 - mae: 0.0570\n",
      "Epoch 29: Learning Rate = 2.5118865210060903e-07, Loss = 0.0026116520166397095\n",
      "\n",
      "Epoch 29: val_loss improved from 0.00349 to 0.00343, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 562ms/step - loss: 0.0026 - root_mean_squared_error: 0.0723 - mae: 0.0570 - val_loss: 0.0034 - val_root_mean_squared_error: 0.0829 - val_mae: 0.0642 - lr: 2.5119e-07\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0722 - mae: 0.0568\n",
      "Epoch 30: Learning Rate = 2.8183828248984355e-07, Loss = 0.002600231673568487\n",
      "\n",
      "Epoch 30: val_loss improved from 0.00343 to 0.00339, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 524ms/step - loss: 0.0026 - root_mean_squared_error: 0.0722 - mae: 0.0568 - val_loss: 0.0034 - val_root_mean_squared_error: 0.0820 - val_mae: 0.0635 - lr: 2.8184e-07\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0720 - mae: 0.0567\n",
      "Epoch 31: Learning Rate = 3.1622775509276835e-07, Loss = 0.002588885137811303\n",
      "\n",
      "Epoch 31: val_loss improved from 0.00339 to 0.00335, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 558ms/step - loss: 0.0026 - root_mean_squared_error: 0.0720 - mae: 0.0567 - val_loss: 0.0033 - val_root_mean_squared_error: 0.0817 - val_mae: 0.0635 - lr: 3.1623e-07\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0717 - mae: 0.0564\n",
      "Epoch 32: Learning Rate = 3.5481338045428856e-07, Loss = 0.002572505036368966\n",
      "\n",
      "Epoch 32: val_loss improved from 0.00335 to 0.00331, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 549ms/step - loss: 0.0026 - root_mean_squared_error: 0.0717 - mae: 0.0564 - val_loss: 0.0033 - val_root_mean_squared_error: 0.0815 - val_mae: 0.0634 - lr: 3.5481e-07\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0026 - root_mean_squared_error: 0.0715 - mae: 0.0563\n",
      "Epoch 33: Learning Rate = 3.981071756697929e-07, Loss = 0.002557134721428156\n",
      "\n",
      "Epoch 33: val_loss improved from 0.00331 to 0.00327, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 541ms/step - loss: 0.0026 - root_mean_squared_error: 0.0715 - mae: 0.0563 - val_loss: 0.0033 - val_root_mean_squared_error: 0.0809 - val_mae: 0.0631 - lr: 3.9811e-07\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0025 - root_mean_squared_error: 0.0713 - mae: 0.0561\n",
      "Epoch 34: Learning Rate = 4.4668360033028875e-07, Loss = 0.0025375261902809143\n",
      "\n",
      "Epoch 34: val_loss improved from 0.00327 to 0.00323, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 557ms/step - loss: 0.0025 - root_mean_squared_error: 0.0713 - mae: 0.0561 - val_loss: 0.0032 - val_root_mean_squared_error: 0.0806 - val_mae: 0.0630 - lr: 4.4668e-07\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0025 - root_mean_squared_error: 0.0710 - mae: 0.0559\n",
      "Epoch 35: Learning Rate = 5.011872303839482e-07, Loss = 0.0025207032449543476\n",
      "\n",
      "Epoch 35: val_loss improved from 0.00323 to 0.00320, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 550ms/step - loss: 0.0025 - root_mean_squared_error: 0.0710 - mae: 0.0559 - val_loss: 0.0032 - val_root_mean_squared_error: 0.0801 - val_mae: 0.0626 - lr: 5.0119e-07\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0025 - root_mean_squared_error: 0.0707 - mae: 0.0557\n",
      "Epoch 36: Learning Rate = 5.623413130706467e-07, Loss = 0.0024956329725682735\n",
      "\n",
      "Epoch 36: val_loss improved from 0.00320 to 0.00316, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 525ms/step - loss: 0.0025 - root_mean_squared_error: 0.0707 - mae: 0.0557 - val_loss: 0.0032 - val_root_mean_squared_error: 0.0799 - val_mae: 0.0626 - lr: 5.6234e-07\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0025 - root_mean_squared_error: 0.0702 - mae: 0.0553\n",
      "Epoch 37: Learning Rate = 6.309573450380412e-07, Loss = 0.0024711291771382093\n",
      "\n",
      "Epoch 37: val_loss improved from 0.00316 to 0.00313, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 556ms/step - loss: 0.0025 - root_mean_squared_error: 0.0702 - mae: 0.0553 - val_loss: 0.0031 - val_root_mean_squared_error: 0.0792 - val_mae: 0.0622 - lr: 6.3096e-07\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0024 - root_mean_squared_error: 0.0699 - mae: 0.0550\n",
      "Epoch 38: Learning Rate = 7.079457873260253e-07, Loss = 0.0024440898559987545\n",
      "\n",
      "Epoch 38: val_loss improved from 0.00313 to 0.00309, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 565ms/step - loss: 0.0024 - root_mean_squared_error: 0.0699 - mae: 0.0550 - val_loss: 0.0031 - val_root_mean_squared_error: 0.0783 - val_mae: 0.0616 - lr: 7.0795e-07\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0024 - root_mean_squared_error: 0.0695 - mae: 0.0548\n",
      "Epoch 39: Learning Rate = 7.943282298583654e-07, Loss = 0.002415648428723216\n",
      "\n",
      "Epoch 39: val_loss improved from 0.00309 to 0.00306, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 550ms/step - loss: 0.0024 - root_mean_squared_error: 0.0695 - mae: 0.0548 - val_loss: 0.0031 - val_root_mean_squared_error: 0.0779 - val_mae: 0.0614 - lr: 7.9433e-07\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0024 - root_mean_squared_error: 0.0691 - mae: 0.0544\n",
      "Epoch 40: Learning Rate = 8.912509201763896e-07, Loss = 0.0023812572471797466\n",
      "\n",
      "Epoch 40: val_loss improved from 0.00306 to 0.00303, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 526ms/step - loss: 0.0024 - root_mean_squared_error: 0.0691 - mae: 0.0544 - val_loss: 0.0030 - val_root_mean_squared_error: 0.0777 - val_mae: 0.0612 - lr: 8.9125e-07\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0023 - root_mean_squared_error: 0.0686 - mae: 0.0540\n",
      "Epoch 41: Learning Rate = 9.999999974752427e-07, Loss = 0.002348226262256503\n",
      "\n",
      "Epoch 41: val_loss improved from 0.00303 to 0.00299, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 561ms/step - loss: 0.0023 - root_mean_squared_error: 0.0686 - mae: 0.0540 - val_loss: 0.0030 - val_root_mean_squared_error: 0.0775 - val_mae: 0.0611 - lr: 1.0000e-06\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0023 - root_mean_squared_error: 0.0679 - mae: 0.0534\n",
      "Epoch 42: Learning Rate = 1.122018488786125e-06, Loss = 0.002306138863787055\n",
      "\n",
      "Epoch 42: val_loss improved from 0.00299 to 0.00295, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 549ms/step - loss: 0.0023 - root_mean_squared_error: 0.0679 - mae: 0.0534 - val_loss: 0.0029 - val_root_mean_squared_error: 0.0767 - val_mae: 0.0606 - lr: 1.1220e-06\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0023 - root_mean_squared_error: 0.0673 - mae: 0.0530\n",
      "Epoch 43: Learning Rate = 1.2589254083650303e-06, Loss = 0.0022620337549597025\n",
      "\n",
      "Epoch 43: val_loss improved from 0.00295 to 0.00290, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 539ms/step - loss: 0.0023 - root_mean_squared_error: 0.0673 - mae: 0.0530 - val_loss: 0.0029 - val_root_mean_squared_error: 0.0765 - val_mae: 0.0604 - lr: 1.2589e-06\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0022 - root_mean_squared_error: 0.0667 - mae: 0.0525\n",
      "Epoch 44: Learning Rate = 1.4125375855655875e-06, Loss = 0.0022185444831848145\n",
      "\n",
      "Epoch 44: val_loss improved from 0.00290 to 0.00286, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 563ms/step - loss: 0.0022 - root_mean_squared_error: 0.0667 - mae: 0.0525 - val_loss: 0.0029 - val_root_mean_squared_error: 0.0756 - val_mae: 0.0597 - lr: 1.4125e-06\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0022 - root_mean_squared_error: 0.0658 - mae: 0.0518\n",
      "Epoch 45: Learning Rate = 1.5848931980144698e-06, Loss = 0.0021693094167858362\n",
      "\n",
      "Epoch 45: val_loss improved from 0.00286 to 0.00281, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 555ms/step - loss: 0.0022 - root_mean_squared_error: 0.0658 - mae: 0.0518 - val_loss: 0.0028 - val_root_mean_squared_error: 0.0753 - val_mae: 0.0595 - lr: 1.5849e-06\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0021 - root_mean_squared_error: 0.0650 - mae: 0.0512\n",
      "Epoch 46: Learning Rate = 1.7782793975129607e-06, Loss = 0.0021138344891369343\n",
      "\n",
      "Epoch 46: val_loss improved from 0.00281 to 0.00276, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 526ms/step - loss: 0.0021 - root_mean_squared_error: 0.0650 - mae: 0.0512 - val_loss: 0.0028 - val_root_mean_squared_error: 0.0741 - val_mae: 0.0587 - lr: 1.7783e-06\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0021 - root_mean_squared_error: 0.0641 - mae: 0.0505\n",
      "Epoch 47: Learning Rate = 1.995262209675275e-06, Loss = 0.0020557953976094723\n",
      "\n",
      "Epoch 47: val_loss improved from 0.00276 to 0.00271, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 561ms/step - loss: 0.0021 - root_mean_squared_error: 0.0641 - mae: 0.0505 - val_loss: 0.0027 - val_root_mean_squared_error: 0.0732 - val_mae: 0.0580 - lr: 1.9953e-06\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0020 - root_mean_squared_error: 0.0632 - mae: 0.0497\n",
      "Epoch 48: Learning Rate = 2.238721208414063e-06, Loss = 0.0019943618681281805\n",
      "\n",
      "Epoch 48: val_loss improved from 0.00271 to 0.00265, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 557ms/step - loss: 0.0020 - root_mean_squared_error: 0.0632 - mae: 0.0497 - val_loss: 0.0027 - val_root_mean_squared_error: 0.0729 - val_mae: 0.0575 - lr: 2.2387e-06\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0019 - root_mean_squared_error: 0.0622 - mae: 0.0489\n",
      "Epoch 49: Learning Rate = 2.5118863504758338e-06, Loss = 0.001930264406837523\n",
      "\n",
      "Epoch 49: val_loss improved from 0.00265 to 0.00259, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 565ms/step - loss: 0.0019 - root_mean_squared_error: 0.0622 - mae: 0.0489 - val_loss: 0.0026 - val_root_mean_squared_error: 0.0721 - val_mae: 0.0570 - lr: 2.5119e-06\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0019 - root_mean_squared_error: 0.0611 - mae: 0.0480\n",
      "Epoch 50: Learning Rate = 2.8183828817418544e-06, Loss = 0.0018618301255628467\n",
      "\n",
      "Epoch 50: val_loss improved from 0.00259 to 0.00252, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 604ms/step - loss: 0.0019 - root_mean_squared_error: 0.0611 - mae: 0.0480 - val_loss: 0.0025 - val_root_mean_squared_error: 0.0710 - val_mae: 0.0561 - lr: 2.8184e-06\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0018 - root_mean_squared_error: 0.0598 - mae: 0.0470\n",
      "Epoch 51: Learning Rate = 3.1622776077711023e-06, Loss = 0.0017867987044155598\n",
      "\n",
      "Epoch 51: val_loss improved from 0.00252 to 0.00245, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 629ms/step - loss: 0.0018 - root_mean_squared_error: 0.0598 - mae: 0.0470 - val_loss: 0.0025 - val_root_mean_squared_error: 0.0701 - val_mae: 0.0554 - lr: 3.1623e-06\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0017 - root_mean_squared_error: 0.0586 - mae: 0.0460\n",
      "Epoch 52: Learning Rate = 3.5481339182297233e-06, Loss = 0.0017130585620179772\n",
      "\n",
      "Epoch 52: val_loss improved from 0.00245 to 0.00238, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 82s 16s/step - loss: 0.0017 - root_mean_squared_error: 0.0586 - mae: 0.0460 - val_loss: 0.0024 - val_root_mean_squared_error: 0.0696 - val_mae: 0.0549 - lr: 3.5481e-06\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0016 - root_mean_squared_error: 0.0572 - mae: 0.0449\n",
      "Epoch 53: Learning Rate = 3.981071586167673e-06, Loss = 0.0016327924095094204\n",
      "\n",
      "Epoch 53: val_loss improved from 0.00238 to 0.00230, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 602ms/step - loss: 0.0016 - root_mean_squared_error: 0.0572 - mae: 0.0449 - val_loss: 0.0023 - val_root_mean_squared_error: 0.0678 - val_mae: 0.0536 - lr: 3.9811e-06\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0016 - root_mean_squared_error: 0.0558 - mae: 0.0438\n",
      "Epoch 54: Learning Rate = 4.46683588961605e-06, Loss = 0.001553912297822535\n",
      "\n",
      "Epoch 54: val_loss improved from 0.00230 to 0.00222, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 561ms/step - loss: 0.0016 - root_mean_squared_error: 0.0558 - mae: 0.0438 - val_loss: 0.0022 - val_root_mean_squared_error: 0.0670 - val_mae: 0.0528 - lr: 4.4668e-06\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0015 - root_mean_squared_error: 0.0543 - mae: 0.0425\n",
      "Epoch 55: Learning Rate = 5.01187241752632e-06, Loss = 0.0014709982788190246\n",
      "\n",
      "Epoch 55: val_loss improved from 0.00222 to 0.00214, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 547ms/step - loss: 0.0015 - root_mean_squared_error: 0.0543 - mae: 0.0425 - val_loss: 0.0021 - val_root_mean_squared_error: 0.0652 - val_mae: 0.0514 - lr: 5.0119e-06\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0014 - root_mean_squared_error: 0.0526 - mae: 0.0412\n",
      "Epoch 56: Learning Rate = 5.62341347176698e-06, Loss = 0.0013894893927499652\n",
      "\n",
      "Epoch 56: val_loss improved from 0.00214 to 0.00206, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 631ms/step - loss: 0.0014 - root_mean_squared_error: 0.0526 - mae: 0.0412 - val_loss: 0.0021 - val_root_mean_squared_error: 0.0643 - val_mae: 0.0505 - lr: 5.6234e-06\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0013 - root_mean_squared_error: 0.0511 - mae: 0.0398\n",
      "Epoch 57: Learning Rate = 6.30957356406725e-06, Loss = 0.0013041375204920769\n",
      "\n",
      "Epoch 57: val_loss improved from 0.00206 to 0.00197, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 603ms/step - loss: 0.0013 - root_mean_squared_error: 0.0511 - mae: 0.0398 - val_loss: 0.0020 - val_root_mean_squared_error: 0.0626 - val_mae: 0.0493 - lr: 6.3096e-06\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0012 - root_mean_squared_error: 0.0494 - mae: 0.0384\n",
      "Epoch 58: Learning Rate = 7.079457645886578e-06, Loss = 0.001223430153913796\n",
      "\n",
      "Epoch 58: val_loss improved from 0.00197 to 0.00189, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 526ms/step - loss: 0.0012 - root_mean_squared_error: 0.0494 - mae: 0.0384 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0612 - val_mae: 0.0480 - lr: 7.0795e-06\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0011 - root_mean_squared_error: 0.0479 - mae: 0.0371\n",
      "Epoch 59: Learning Rate = 7.943282071209978e-06, Loss = 0.0011435803025960922\n",
      "\n",
      "Epoch 59: val_loss improved from 0.00189 to 0.00180, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 571ms/step - loss: 0.0011 - root_mean_squared_error: 0.0479 - mae: 0.0371 - val_loss: 0.0018 - val_root_mean_squared_error: 0.0600 - val_mae: 0.0469 - lr: 7.9433e-06\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0011 - root_mean_squared_error: 0.0462 - mae: 0.0356\n",
      "Epoch 60: Learning Rate = 8.912509656511247e-06, Loss = 0.0010655197547748685\n",
      "\n",
      "Epoch 60: val_loss improved from 0.00180 to 0.00172, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 554ms/step - loss: 0.0011 - root_mean_squared_error: 0.0462 - mae: 0.0356 - val_loss: 0.0017 - val_root_mean_squared_error: 0.0586 - val_mae: 0.0458 - lr: 8.9125e-06\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 9.9219e-04 - root_mean_squared_error: 0.0446 - mae: 0.0342\n",
      "Epoch 61: Learning Rate = 9.999999747378752e-06, Loss = 0.0009921850869432092\n",
      "\n",
      "Epoch 61: val_loss improved from 0.00172 to 0.00164, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 534ms/step - loss: 9.9219e-04 - root_mean_squared_error: 0.0446 - mae: 0.0342 - val_loss: 0.0016 - val_root_mean_squared_error: 0.0574 - val_mae: 0.0446 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 9.2260e-04 - root_mean_squared_error: 0.0431 - mae: 0.0329\n",
      "Epoch 62: Learning Rate = 1.1220184205740225e-05, Loss = 0.0009226041729561985\n",
      "\n",
      "Epoch 62: val_loss improved from 0.00164 to 0.00156, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 607ms/step - loss: 9.2260e-04 - root_mean_squared_error: 0.0431 - mae: 0.0329 - val_loss: 0.0016 - val_root_mean_squared_error: 0.0561 - val_mae: 0.0435 - lr: 1.1220e-05\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.5896e-04 - root_mean_squared_error: 0.0415 - mae: 0.0315\n",
      "Epoch 63: Learning Rate = 1.2589253856276628e-05, Loss = 0.0008589584613218904\n",
      "\n",
      "Epoch 63: val_loss improved from 0.00156 to 0.00149, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 588ms/step - loss: 8.5896e-04 - root_mean_squared_error: 0.0415 - mae: 0.0315 - val_loss: 0.0015 - val_root_mean_squared_error: 0.0546 - val_mae: 0.0421 - lr: 1.2589e-05\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.9956e-04 - root_mean_squared_error: 0.0400 - mae: 0.0303\n",
      "Epoch 64: Learning Rate = 1.4125375855655875e-05, Loss = 0.0007995597552508116\n",
      "\n",
      "Epoch 64: val_loss improved from 0.00149 to 0.00143, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 551ms/step - loss: 7.9956e-04 - root_mean_squared_error: 0.0400 - mae: 0.0303 - val_loss: 0.0014 - val_root_mean_squared_error: 0.0537 - val_mae: 0.0413 - lr: 1.4125e-05\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.4837e-04 - root_mean_squared_error: 0.0387 - mae: 0.0291\n",
      "Epoch 65: Learning Rate = 1.5848931070649996e-05, Loss = 0.0007483718800358474\n",
      "\n",
      "Epoch 65: val_loss improved from 0.00143 to 0.00137, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 568ms/step - loss: 7.4837e-04 - root_mean_squared_error: 0.0387 - mae: 0.0291 - val_loss: 0.0014 - val_root_mean_squared_error: 0.0522 - val_mae: 0.0400 - lr: 1.5849e-05\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.0147e-04 - root_mean_squared_error: 0.0375 - mae: 0.0280\n",
      "Epoch 66: Learning Rate = 1.778279329300858e-05, Loss = 0.0007014749571681023\n",
      "\n",
      "Epoch 66: val_loss improved from 0.00137 to 0.00131, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 588ms/step - loss: 7.0147e-04 - root_mean_squared_error: 0.0375 - mae: 0.0280 - val_loss: 0.0013 - val_root_mean_squared_error: 0.0514 - val_mae: 0.0394 - lr: 1.7783e-05\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.5974e-04 - root_mean_squared_error: 0.0364 - mae: 0.0270\n",
      "Epoch 67: Learning Rate = 1.995262391574215e-05, Loss = 0.0006597414612770081\n",
      "\n",
      "Epoch 67: val_loss improved from 0.00131 to 0.00127, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 528ms/step - loss: 6.5974e-04 - root_mean_squared_error: 0.0364 - mae: 0.0270 - val_loss: 0.0013 - val_root_mean_squared_error: 0.0507 - val_mae: 0.0387 - lr: 1.9953e-05\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.2339e-04 - root_mean_squared_error: 0.0353 - mae: 0.0261\n",
      "Epoch 68: Learning Rate = 2.238721208414063e-05, Loss = 0.0006233884487301111\n",
      "\n",
      "Epoch 68: val_loss improved from 0.00127 to 0.00123, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 568ms/step - loss: 6.2339e-04 - root_mean_squared_error: 0.0353 - mae: 0.0261 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0493 - val_mae: 0.0377 - lr: 2.2387e-05\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.9319e-04 - root_mean_squared_error: 0.0345 - mae: 0.0254\n",
      "Epoch 69: Learning Rate = 2.511886486900039e-05, Loss = 0.000593193166423589\n",
      "\n",
      "Epoch 69: val_loss improved from 0.00123 to 0.00119, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 560ms/step - loss: 5.9319e-04 - root_mean_squared_error: 0.0345 - mae: 0.0254 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0491 - val_mae: 0.0372 - lr: 2.5119e-05\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.6377e-04 - root_mean_squared_error: 0.0337 - mae: 0.0246\n",
      "Epoch 70: Learning Rate = 2.8183829272165895e-05, Loss = 0.0005637705326080322\n",
      "\n",
      "Epoch 70: val_loss improved from 0.00119 to 0.00116, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 560ms/step - loss: 5.6377e-04 - root_mean_squared_error: 0.0337 - mae: 0.0246 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0481 - val_mae: 0.0365 - lr: 2.8184e-05\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.3892e-04 - root_mean_squared_error: 0.0329 - mae: 0.0239\n",
      "Epoch 71: Learning Rate = 3.162277789670043e-05, Loss = 0.0005389159196056426\n",
      "\n",
      "Epoch 71: val_loss improved from 0.00116 to 0.00114, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 527ms/step - loss: 5.3892e-04 - root_mean_squared_error: 0.0329 - mae: 0.0239 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0477 - val_mae: 0.0362 - lr: 3.1623e-05\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.1830e-04 - root_mean_squared_error: 0.0322 - mae: 0.0233\n",
      "Epoch 72: Learning Rate = 3.548133827280253e-05, Loss = 0.0005183033063076437\n",
      "\n",
      "Epoch 72: val_loss improved from 0.00114 to 0.00112, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 572ms/step - loss: 5.1830e-04 - root_mean_squared_error: 0.0322 - mae: 0.0233 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0473 - val_mae: 0.0359 - lr: 3.5481e-05\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.9894e-04 - root_mean_squared_error: 0.0317 - mae: 0.0227\n",
      "Epoch 73: Learning Rate = 3.981071859016083e-05, Loss = 0.0004989402368664742\n",
      "\n",
      "Epoch 73: val_loss improved from 0.00112 to 0.00111, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 623ms/step - loss: 4.9894e-04 - root_mean_squared_error: 0.0317 - mae: 0.0227 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0472 - val_mae: 0.0357 - lr: 3.9811e-05\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.8216e-04 - root_mean_squared_error: 0.0310 - mae: 0.0222\n",
      "Epoch 74: Learning Rate = 4.46683588961605e-05, Loss = 0.0004821584152523428\n",
      "\n",
      "Epoch 74: val_loss improved from 0.00111 to 0.00110, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 560ms/step - loss: 4.8216e-04 - root_mean_squared_error: 0.0310 - mae: 0.0222 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0469 - val_mae: 0.0355 - lr: 4.4668e-05\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.6550e-04 - root_mean_squared_error: 0.0305 - mae: 0.0216\n",
      "Epoch 75: Learning Rate = 5.0118724175263196e-05, Loss = 0.00046550188562832773\n",
      "\n",
      "Epoch 75: val_loss improved from 0.00110 to 0.00109, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 569ms/step - loss: 4.6550e-04 - root_mean_squared_error: 0.0305 - mae: 0.0216 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0463 - val_mae: 0.0351 - lr: 5.0119e-05\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.5150e-04 - root_mean_squared_error: 0.0299 - mae: 0.0211\n",
      "Epoch 76: Learning Rate = 5.62341338081751e-05, Loss = 0.0004514950851444155\n",
      "\n",
      "Epoch 76: val_loss improved from 0.00109 to 0.00108, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 558ms/step - loss: 4.5150e-04 - root_mean_squared_error: 0.0299 - mae: 0.0211 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0464 - val_mae: 0.0351 - lr: 5.6234e-05\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.3824e-04 - root_mean_squared_error: 0.0296 - mae: 0.0207\n",
      "Epoch 77: Learning Rate = 6.30957365501672e-05, Loss = 0.00043823642772622406\n",
      "\n",
      "Epoch 77: val_loss improved from 0.00108 to 0.00106, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 537ms/step - loss: 4.3824e-04 - root_mean_squared_error: 0.0296 - mae: 0.0207 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0461 - val_mae: 0.0349 - lr: 6.3096e-05\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.2618e-04 - root_mean_squared_error: 0.0292 - mae: 0.0203\n",
      "Epoch 78: Learning Rate = 7.079458009684458e-05, Loss = 0.00042618357110768557\n",
      "\n",
      "Epoch 78: val_loss improved from 0.00106 to 0.00105, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 562ms/step - loss: 4.2618e-04 - root_mean_squared_error: 0.0292 - mae: 0.0203 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0459 - val_mae: 0.0347 - lr: 7.0795e-05\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.1444e-04 - root_mean_squared_error: 0.0288 - mae: 0.0199\n",
      "Epoch 79: Learning Rate = 7.943282253108919e-05, Loss = 0.00041443732334300876\n",
      "\n",
      "Epoch 79: val_loss improved from 0.00105 to 0.00104, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 572ms/step - loss: 4.1444e-04 - root_mean_squared_error: 0.0288 - mae: 0.0199 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0456 - val_mae: 0.0344 - lr: 7.9433e-05\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.0407e-04 - root_mean_squared_error: 0.0286 - mae: 0.0195\n",
      "Epoch 80: Learning Rate = 8.912509656511247e-05, Loss = 0.00040406829793937504\n",
      "\n",
      "Epoch 80: val_loss improved from 0.00104 to 0.00102, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 527ms/step - loss: 4.0407e-04 - root_mean_squared_error: 0.0286 - mae: 0.0195 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0447 - val_mae: 0.0339 - lr: 8.9125e-05\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.9483e-04 - root_mean_squared_error: 0.0281 - mae: 0.0191\n",
      "Epoch 81: Learning Rate = 9.999999747378752e-05, Loss = 0.00039483033469878137\n",
      "\n",
      "Epoch 81: val_loss improved from 0.00102 to 0.00101, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 564ms/step - loss: 3.9483e-04 - root_mean_squared_error: 0.0281 - mae: 0.0191 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0450 - val_mae: 0.0339 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.8671e-04 - root_mean_squared_error: 0.0279 - mae: 0.0188\n",
      "Epoch 82: Learning Rate = 0.00011220184387639165, Loss = 0.00038671112270094454\n",
      "\n",
      "Epoch 82: val_loss improved from 0.00101 to 0.00100, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 554ms/step - loss: 3.8671e-04 - root_mean_squared_error: 0.0279 - mae: 0.0188 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0448 - val_mae: 0.0337 - lr: 1.1220e-04\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.7860e-04 - root_mean_squared_error: 0.0275 - mae: 0.0184\n",
      "Epoch 83: Learning Rate = 0.0001258925476577133, Loss = 0.00037859761505387723\n",
      "\n",
      "Epoch 83: val_loss improved from 0.00100 to 0.00099, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 529ms/step - loss: 3.7860e-04 - root_mean_squared_error: 0.0275 - mae: 0.0184 - val_loss: 9.9206e-04 - val_root_mean_squared_error: 0.0447 - val_mae: 0.0336 - lr: 1.2589e-04\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.7148e-04 - root_mean_squared_error: 0.0274 - mae: 0.0182\n",
      "Epoch 84: Learning Rate = 0.00014125375309959054, Loss = 0.00037148233968764544\n",
      "\n",
      "Epoch 84: val_loss improved from 0.00099 to 0.00098, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 567ms/step - loss: 3.7148e-04 - root_mean_squared_error: 0.0274 - mae: 0.0182 - val_loss: 9.7735e-04 - val_root_mean_squared_error: 0.0441 - val_mae: 0.0333 - lr: 1.4125e-04\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.6481e-04 - root_mean_squared_error: 0.0271 - mae: 0.0179\n",
      "Epoch 85: Learning Rate = 0.00015848931798245758, Loss = 0.0003648114507086575\n",
      "\n",
      "Epoch 85: val_loss improved from 0.00098 to 0.00096, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 583ms/step - loss: 3.6481e-04 - root_mean_squared_error: 0.0271 - mae: 0.0179 - val_loss: 9.6194e-04 - val_root_mean_squared_error: 0.0441 - val_mae: 0.0330 - lr: 1.5849e-04\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.5946e-04 - root_mean_squared_error: 0.0269 - mae: 0.0177\n",
      "Epoch 86: Learning Rate = 0.00017782794020604342, Loss = 0.0003594649024307728\n",
      "\n",
      "Epoch 86: val_loss improved from 0.00096 to 0.00095, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 530ms/step - loss: 3.5946e-04 - root_mean_squared_error: 0.0269 - mae: 0.0177 - val_loss: 9.5186e-04 - val_root_mean_squared_error: 0.0436 - val_mae: 0.0327 - lr: 1.7783e-04\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.5375e-04 - root_mean_squared_error: 0.0266 - mae: 0.0174\n",
      "Epoch 87: Learning Rate = 0.0001995262282434851, Loss = 0.0003537533339112997\n",
      "\n",
      "Epoch 87: val_loss improved from 0.00095 to 0.00095, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 568ms/step - loss: 3.5375e-04 - root_mean_squared_error: 0.0266 - mae: 0.0174 - val_loss: 9.4548e-04 - val_root_mean_squared_error: 0.0435 - val_mae: 0.0325 - lr: 1.9953e-04\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.4931e-04 - root_mean_squared_error: 0.0264 - mae: 0.0172\n",
      "Epoch 88: Learning Rate = 0.00022387212084140629, Loss = 0.00034931389382109046\n",
      "\n",
      "Epoch 88: val_loss improved from 0.00095 to 0.00094, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 576ms/step - loss: 3.4931e-04 - root_mean_squared_error: 0.0264 - mae: 0.0172 - val_loss: 9.3676e-04 - val_root_mean_squared_error: 0.0434 - val_mae: 0.0325 - lr: 2.2387e-04\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.4560e-04 - root_mean_squared_error: 0.0263 - mae: 0.0170\n",
      "Epoch 89: Learning Rate = 0.0002511886414140463, Loss = 0.0003455985279288143\n",
      "\n",
      "Epoch 89: val_loss improved from 0.00094 to 0.00093, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 531ms/step - loss: 3.4560e-04 - root_mean_squared_error: 0.0263 - mae: 0.0170 - val_loss: 9.2771e-04 - val_root_mean_squared_error: 0.0433 - val_mae: 0.0324 - lr: 2.5119e-04\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.4083e-04 - root_mean_squared_error: 0.0262 - mae: 0.0168\n",
      "Epoch 90: Learning Rate = 0.00028183829272165895, Loss = 0.00034082919592037797\n",
      "\n",
      "Epoch 90: val_loss improved from 0.00093 to 0.00090, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 587ms/step - loss: 3.4083e-04 - root_mean_squared_error: 0.0262 - mae: 0.0168 - val_loss: 8.9684e-04 - val_root_mean_squared_error: 0.0425 - val_mae: 0.0315 - lr: 2.8184e-04\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3789e-04 - root_mean_squared_error: 0.0260 - mae: 0.0167\n",
      "Epoch 91: Learning Rate = 0.0003162277571391314, Loss = 0.00033789468579925597\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.00090\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 3.3789e-04 - root_mean_squared_error: 0.0260 - mae: 0.0167 - val_loss: 9.0802e-04 - val_root_mean_squared_error: 0.0426 - val_mae: 0.0317 - lr: 3.1623e-04\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3475e-04 - root_mean_squared_error: 0.0259 - mae: 0.0165\n",
      "Epoch 92: Learning Rate = 0.0003548133827280253, Loss = 0.00033475476084277034\n",
      "\n",
      "Epoch 92: val_loss improved from 0.00090 to 0.00089, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 571ms/step - loss: 3.3475e-04 - root_mean_squared_error: 0.0259 - mae: 0.0165 - val_loss: 8.8734e-04 - val_root_mean_squared_error: 0.0419 - val_mae: 0.0313 - lr: 3.5481e-04\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3249e-04 - root_mean_squared_error: 0.0258 - mae: 0.0164\n",
      "Epoch 93: Learning Rate = 0.0003981071640737355, Loss = 0.0003324865538161248\n",
      "\n",
      "Epoch 93: val_loss improved from 0.00089 to 0.00088, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 537ms/step - loss: 3.3249e-04 - root_mean_squared_error: 0.0258 - mae: 0.0164 - val_loss: 8.8488e-04 - val_root_mean_squared_error: 0.0421 - val_mae: 0.0313 - lr: 3.9811e-04\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3045e-04 - root_mean_squared_error: 0.0258 - mae: 0.0163\n",
      "Epoch 94: Learning Rate = 0.00044668358168564737, Loss = 0.00033044893643818796\n",
      "\n",
      "Epoch 94: val_loss improved from 0.00088 to 0.00088, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 568ms/step - loss: 3.3045e-04 - root_mean_squared_error: 0.0258 - mae: 0.0163 - val_loss: 8.7813e-04 - val_root_mean_squared_error: 0.0419 - val_mae: 0.0311 - lr: 4.4668e-04\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2804e-04 - root_mean_squared_error: 0.0255 - mae: 0.0161\n",
      "Epoch 95: Learning Rate = 0.0005011872272007167, Loss = 0.0003280378587078303\n",
      "\n",
      "Epoch 95: val_loss improved from 0.00088 to 0.00087, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 579ms/step - loss: 3.2804e-04 - root_mean_squared_error: 0.0255 - mae: 0.0161 - val_loss: 8.7017e-04 - val_root_mean_squared_error: 0.0417 - val_mae: 0.0310 - lr: 5.0119e-04\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2587e-04 - root_mean_squared_error: 0.0256 - mae: 0.0161\n",
      "Epoch 96: Learning Rate = 0.000562341301701963, Loss = 0.00032586793531663716\n",
      "\n",
      "Epoch 96: val_loss improved from 0.00087 to 0.00085, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 565ms/step - loss: 3.2587e-04 - root_mean_squared_error: 0.0256 - mae: 0.0161 - val_loss: 8.4567e-04 - val_root_mean_squared_error: 0.0411 - val_mae: 0.0304 - lr: 5.6234e-04\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2594e-04 - root_mean_squared_error: 0.0256 - mae: 0.0161\n",
      "Epoch 97: Learning Rate = 0.000630957365501672, Loss = 0.00032593621290288866\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.00085\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 3.2594e-04 - root_mean_squared_error: 0.0256 - mae: 0.0161 - val_loss: 8.5189e-04 - val_root_mean_squared_error: 0.0415 - val_mae: 0.0307 - lr: 6.3096e-04\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2412e-04 - root_mean_squared_error: 0.0254 - mae: 0.0159\n",
      "Epoch 98: Learning Rate = 0.0007079457864165306, Loss = 0.0003241216763854027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.00085\n",
      "6/6 [==============================] - 1s 237ms/step - loss: 3.2412e-04 - root_mean_squared_error: 0.0254 - mae: 0.0159 - val_loss: 8.4615e-04 - val_root_mean_squared_error: 0.0405 - val_mae: 0.0302 - lr: 7.0795e-04\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2332e-04 - root_mean_squared_error: 0.0254 - mae: 0.0159\n",
      "Epoch 99: Learning Rate = 0.0007943282253108919, Loss = 0.0003233209135942161\n",
      "\n",
      "Epoch 99: val_loss improved from 0.00085 to 0.00082, saving model to model_inder_rnn999\n",
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_inder_rnn999/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 595ms/step - loss: 3.2332e-04 - root_mean_squared_error: 0.0254 - mae: 0.0159 - val_loss: 8.1692e-04 - val_root_mean_squared_error: 0.0404 - val_mae: 0.0298 - lr: 7.9433e-04\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2051e-04 - root_mean_squared_error: 0.0254 - mae: 0.0158\n",
      "Epoch 100: Learning Rate = 0.0008912509656511247, Loss = 0.00032050721347332\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.00082\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 3.2051e-04 - root_mean_squared_error: 0.0254 - mae: 0.0158 - val_loss: 8.2734e-04 - val_root_mean_squared_error: 0.0406 - val_mae: 0.0300 - lr: 8.9125e-04\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, GRU, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "# Define your learning rate scheduler function\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 1e-8\n",
    "    return initial_lr * 10**(epoch/20)\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,          # Stop training after no improvement for 10 epochs\n",
    "    restore_best_weights=True,  # Restore the best weights when stopping\n",
    "min_delta=0.0000001)\n",
    "\n",
    "# Create a model (you need to replace this with your actual model)\n",
    "model_1 = Sequential([\n",
    "    BatchNormalization(\n",
    "        input_shape = (window_size,num_features),\n",
    "        name = 'Batch_Norm_1'),\n",
    "    LSTM(512,return_sequences=True,name='LSTM_1'),\n",
    "    LSTM(512,name='LSTM_2'),\n",
    "    Dense(256,activation='relu',name='Dense_1'),\n",
    "    Dense(len(tickers),name='Returns')\n",
    "])\n",
    "\n",
    "\n",
    "checkpont_rnn = ModelCheckpoint(\n",
    "    filepath='model_inder_rnn999',\n",
    "    save_weights_only=False,\n",
    "    save_freq = 'epoch',\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only = True,\n",
    "    verbose = 1)\n",
    "\n",
    "# Compile the model with an optimizer (Adam in this example)\n",
    "optimizer = tf.keras.optimizers.Adam()  # Set initial learning rate to 0.0\n",
    "model_1.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    metrics=[tf.metrics.RootMeanSquaredError(),'mae'],\n",
    "    optimizer=optimizer)\n",
    "\n",
    "# Set up the learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize lists to store loss and learning rates\n",
    "losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Custom callback to log loss and learning rate\n",
    "class LossLearningRateCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        loss = logs['loss']\n",
    "        learning_rates.append(lr)\n",
    "#         print(learning_rates)\n",
    "        losses.append(loss)\n",
    "        print()\n",
    "        print(f\"Epoch {epoch+1}: Learning Rate = {lr.numpy()}, Loss = {loss}\")\n",
    "\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "loss_lr_callback = LossLearningRateCallback()\n",
    "\n",
    "# Train your model\n",
    "# history = model.fit(x_train, y_train, epochs=num_epochs, callbacks=[lr_scheduler, loss_lr_callback])\n",
    "history = model_1.fit(\n",
    "    my_window.train,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=my_window.val,\n",
    "    callbacks=[lr_scheduler, loss_lr_callback, early_stopping, checkpont_rnn]\n",
    ")\n",
    "\n",
    "# Access the losses from the training history\n",
    "losses = history.history['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3600f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final learning rate used for the model is: 0.0008912509656511247\n"
     ]
    }
   ],
   "source": [
    "final_learning_rate = model_1.optimizer.lr.numpy()\n",
    "print(f\"The final learning rate used for the model is: {final_learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0b275de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_schedule(epoch)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1ca7a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbMAAALFCAYAAAD5p35EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAADErElEQVR4nOzdd3hUddr/8c+U1ElvhBIIHaRICbAgKopdQWy4Fra6rouNZ1HXdV3XsrrrT9S14equbRXFhhVQRBAREVRQkd6SQBJIbzNpU35/BEZOEiCBJGeSeb+uK1dy7nNm5p7Ad559Pny9j8Xn8/kEAAAAAAAAAEAAs5rdAAAAAAAAAAAAR0OYDQAAAAAAAAAIeITZAAAAAAAAAICAR5gNAAAAAAAAAAh4hNkAAAAAAAAAgIBHmA0AAAAAAAAACHiE2QAAAAAAAACAgEeYDQAAAAAAAAAIeITZAAAAQCvyeDxmtxA0+F0DAAAEF8JsAACAIDRjxgwNHDiwxV+333672a1rwYIF/n6ysrLMbsdg9erV+t3vfteoHsg9d1SH+103x969e/1/Hm+++WYrdwYAAIC2QpgNAAAAtII333xTv/rVr5SZmWl2K50ev2sAAIDgZDe7AQAAAJinW7du+vDDD5t9fUhISBt207Ht27fvsOeioqLUs2dPSfwOW8ORftfNERIS4v/ziIqKao2WAAAA0A4IswEAAIKYxWKRw+Ewu41O76yzztJZZ51ldhs4oEuXLvrkk0/MbgMAAAAtxJgRAAAAAAAAAEDAI8wGAABAi9XW1mrs2LEaOHCg7rzzziNee8cdd2jgwIE6/fTT5fP5DOfWr1+vv/71rzrvvPOUkZGhoUOHavz48ZoxY4ZeeuklVVdXN7un5t7U7/TTT9fAgQN1yy23NHk+JydHDz/8sC699FL97Gc/05AhQzRmzBhNmzZNc+bM0f79+w3XH7y545NPPul//ME+1qxZY7jmSDeA/Oyzz3T99ddr4sSJGjp0qMaNG6cZM2botddeU11dXZOPOXgjz0cffVRer1fz58/X5ZdfroyMDI0YMUIXXnihnn322Rb9Hg964oknNHDgQM2YMUOStHTpUv3yl7/U2LFjNXLkSF188cV6/fXX/deXlZXpwQcf1Jlnnqlhw4bppJNO0uzZs7Vnz57DvobP59OHH36o3/3ud5owYYKGDh2qiRMn6vrrr9eKFSsaXd+c3/XBvq+44gqVlpZq9uzZGjlypEaNGqWLL75YW7dubdbflZycHD366KOaOnWqRo8erREjRmjKlCl65JFHVFpa2uRjVq1apZtuukknn3yyhg4dqrFjx+qSSy7RY489puLi4mb93gEAAHB4jBkBAABAi4WGhuqcc87R66+/rk8++UR/+9vfmpwFXVtb6x/nMHXqVFksFkmSx+PR3XffrTfeeKPRY4qLi7V27VqtXbtWb7/9tubNm6fo6Oi2fUMHvPnmm7rnnnsahcfl5eUqLy/X5s2b9cYbb+iFF17QkCFDWuU1q6qqNHv2bH366aeGemlpqf/38Oqrr+rf//63unfv3uRz1NXV6ZprrtGqVasM9S1btmjLli1atGiRXnnllWOeD/3AAw/opZdeMtQ2btyou+66S1lZWbriiiv0i1/8Qrm5uf7zhYWF+vDDD7Vq1SotWLBA3bp1Mzy+vLxcN9xwgz+EPqigoEBLly7V0qVLNXXqVN1///0KDQ1tcc+1tbW65pprtGHDBn8tOztbvXr1UmFh4REfu3jxYt1xxx1yuVyG+rZt27Rt2zYtWLBAzz//vAYMGOA/99hjj2nu3LmG68vKylRWVqYff/xRr7zyip577jkNHz68xe8FAAAA9diZDQAAgGNy4YUXSqoPXRuGqAetWLFC5eXlkurD7INefPFFf5B9/vnna/78+Vq1apWWLVumZ599ViNHjpQkbd26Vc8//3xbvg2/H374QX/9619VV1enoUOH6plnntHy5cu1atUqzZ8/X9OmTZNUH1A+8MAD/sdNnTpV69at0+9//3tJ9TfVXLdundatW6eMjIyjvu4f//hHf5B98B8I1qxZo0WLFunaa6+V3W7Xtm3b9Jvf/EaVlZVNPserr76qVatWadq0aVqwYIG++uorvfHGGxo/frwkafPmzcf8e/z+++/10ksvaezYsZo3b56+/PJLPffcc+ratask6aWXXtJvfvMbVVRU6O6779bnn3+uFStW6MYbb5TValVJSYn+/e9/G57T6/Xq+uuv15o1a2S32/W73/1OH374odasWaP33ntPM2bMkMVi0fvvv6/777/f/7iW/K5//PFHbdiwQTfeeKNWrlyp9957T/fdd5/Cw8OP+H7XrVunP/7xj3K5XEpLS9PDDz+slStXatmyZbrzzjvlcDhUUFCg66+/XrW1tZLq/wuDg0H2lClT9NZbb2n16tVaunSp7rnnHkVFRam8vFx/+tOf5PV6j+nPAQAAAOzMBgAACGo+n09Op7NZ11qtVkVERPiPR48erZ49eyo7O1sLFy7UpEmTGj3mww8/lCQNHz5cffr0kVQfZL7wwguSpJNOOkkPP/ywf8e2JHXv3l1jx47VWWedpfz8fK1cuVI333zzsb7FZnvuuefk8/mUkJCg559/XrGxsf5zSUlJGjlypCorK7V06VJ9++23cjqdcjgcstvtstvt/p3pLbmp5vLly7Vs2TJJ0i9/+Uvdcccd/nNxcXGaPXu2hgwZoptvvlmZmZmaO3eubrvttkbPU1VVpV//+te6/fbb/bX4+Hg988wzOvPMM7V//3598sknuummm1r8e6mpqdHQoUP1/PPP+9/jxIkT9ac//UmzZs2S2+1Wdna25s2bZwiUb7jhBu3YsUOLFy/Wl19+aXjOd955R2vXrpUkPfroo4abY8bFxenOO+9UWlqaHnjgAc2fP1/Tp0/XkCFDWvy7njp1qm644QZJUkpKigYNGnTU93v33XfL6/WqW7dueuONN5SQkOA/N2PGDHXt2lXXX3+9/+/9RRddpCVLlkiSevXqpYceesj/9zkhIUE///nPFR4erj/96U/atWuXNm/e3Gq7+gEAAIINYTYAAEAQy83N1ahRo5p1bffu3f3B60FTp07Vk08+qU8//VQ1NTUKCwvzn6usrNRnn30m6add3JLkdDp1ySWXaO/evZo+fbohyD4oIiJCw4cP19KlS1VSUnIM76zlRo0apaioKA0ePNgQZB9q7NixWrp0qXw+n8rKypodWh/Owd3piYmJh53hfc455+j000/XsmXL9MYbb2j27Nmy2WyGaywWi6699tpGjw0LC9P48eP17rvvau/evcfc529+85tGY2QODa5HjhzZ5C70ESNGaPHixY3mjL/22muSpDFjxhiC7ENdffXVeumll5STk6M33nhD99xzT4v7Pvvss1t0/fbt27V161ZJ0vXXX28Isg8644wzNGbMGFmtVv8u64M7tF0ul1wuV6O/F2effbaio6OVlpam9PT0Fr8PAAAA1CPMBgAAwDG78MIL9eSTT8rpdOqzzz4zhIdLly5VdXW1QkJCdN555/nr0dHR+r//+7/DPqfb7dbmzZtVVFTkP24Pv/zlL494PjMzUzt37vQft0ZfX3/9tSTptNNOO+Jc6HPOOUfLli1TRUWFtmzZ0mhnb48ePZoMXiX561VVVcfcZ1NznhMTE/0/H26n8cEZ3QfDXqn+Hzk2bdokSTrhhBOO+F8GDBs2TDk5OVq3bt0x9T148OAWXb969Wr/z6eddtphr3vllVcMxxkZGXrllVdUUFCgiy++WJdffrlOPfVU9e3bV1L9P85Mnjy5Rb0AAACgMcJsAACAINbUbuuW6Nmzp0aOHKn169dr4cKFhjD74IiRiRMnHjZozc/P16pVq7Rr1y5lZWUpKytLu3btMoSfPp/vmPs7FlVVVVq5cqW2bdumrKws7dmzRzt27FBFRYXhuuPtq7Ky0v+cB0PPwzn0fF5eXqPwOD4+/rCPPRiSH0+/TT2/1frT7XcOd2PJQ685KCcnRx6PR1L9vO2GN5ZsSl5eXnNbNYiLi2vR9Qd3kEdFRRnC+qM566yzdNppp2n58uXKzMzUgw8+qAcffFDdunXTxIkTddppp2nixInHdCNLAAAA/IQwGwAAAMflwgsv1Pr167VixQr/HOni4mL/LtdDR4wcVF5ergcffFDvvfee6urqDOccDofGjx+vgoIC/w7e9uD1evWf//xHzz//vEpLSw3nQkJCNHLkSMXExGjFihWt8nqH7kiOjIw84rWHzipvaiez3d62/7P+aDdNbInD3cSytR8jyTD2pjnKysokGX/fzWGz2TR37ly99dZbmj9/vjZu3CipfozPG2+84Z+9fdttt+miiy5q0XMDAADgJ4TZAAAAOC7nnXee7r//flVXV2vZsmWaMmWKFi9eLLfbrejoaJ1++umG691ut37729/qhx9+kFQ/omHChAkaMGCA+vbtq/T0dFmtVt1yyy1tEmZXV1c3Wf9//+//+W9MmZ6ersmTJ2vQoEHq27ev+vfvr9DQUL355putFmYfGmC7XK4jXtuS4DvQHRoU33PPPfr5z39uYjdGB0P7YxnJYrVaNX36dE2fPl379u3TypUrtXr1aq1atUqlpaUqLi7W7bffrqioKJ155pmt3ToAAEBQIMwGAADAcYmNjdWkSZP0ySefaMmSJZoyZYoWLlwoqX7Wc8PdsR999JE/yL711lt1zTXXNPm8Lb3x46E3RWy42/sgj8ej8vLyRvV9+/b5x12cfvrpevLJJxvdZPFYejqSqKgoxcTEqLy83DCLuymHnu/WrVur9WCG1NRU/89Huymlz+dr8gahbeXg77ayslLFxcWHHY+zaNEi7d69W/3792/yBpapqam67LLLdNlll8nj8Wjx4sX685//rNraWv3vf/8jzAYAADhGjYfYAQAAAC00bdo0SdIXX3yh7Oxs/w37mhoxsn79ev/PV155ZZPPV1VVpe+++05S/fiP5jh0x+/hQueNGzc2GXR///33/te5/PLLmwyyJeMNAhv21dLQ1WKxaPTo0ZKk5cuXG+aEN/Txxx9Lqt+VPWDAgBa9TqBJSEjwzwBftmzZYWd5e71enX/++Tr55JN16623Gs61VcA9atQo/88rV6487HUvvPCCHn/8cb366quSpJtvvlmTJ0/WQw891Oham82mCy64QBMnTpRUPyceAAAAx4YwGwAAAMftlFNOUVxcnFwulx544AH5fD51795dGRkZja499KaAO3bsaHTe6/Xq3nvv9c9JPtwu64bi4uL8N/xbunRpo7DZ4/HoySefbPKxh4bXTfUkSW+//ba+/PJL/3HDvg4+x5FC6YamT58uSSoqKtKcOXOavGbp0qVavny5JOmiiy5SSEhIs58/UB183zt37tRzzz3X5DX/+9//tHPnTuXn56tfv36Gc8fyu26OESNG+F/rqaeeanTTT0lasWKF/78sOP/88yXVj67Zu3ev3n//fRUXFzd6TG1trX93fc+ePVu1ZwAAgGBCmA0AABDEfD6fnE5ni76aEhoaqnPPPVeS/MHrlClTmtxBe3CHqiTNnj1bn376qfLz85WXl6dPPvlEV199tRYsWOC/5mjzpA91cD73pk2bNGvWLG3ZssV/M8pf//rXWrFihWJjYxs9bvTo0f55yU8++aTmzZunvXv3qrCwUF9//bX+9Kc/6Y477jA8pmFfB4P0wsJCffbZZyotLVVNTc1R+z3Y80svvaRZs2bp+++/V1lZmXbu3KlHH31Us2bNkiSlpaXpj3/8Y7N/F4Hsyiuv1AknnCBJeuihh3THHXfoxx9/VGlpqbZu3ap//vOf+uc//ympfn75jBkzDI8/lt91c915552yWq3KysrSz3/+cy1dulTFxcXKysrSiy++6P8zGDBggP+/SPjtb38rqX7X9S9/+UstWbJEOTk5Kiws1Nq1a3XdddcpKytLknT11Ve3Sp8AAADBiJnZAAAAQSw3N9cwWqE5vv76a8XExDSqT5s2Ta+99pr/uKkRI5J06qmn6vzzz9fChQuVnZ2tmTNnNromJSVFkydP1muvvaba2lrl5eWpa9euR+3t5ptv1po1a5STk6OPP/7YP57joBkzZqiqqkpvvfWWoR4fH6/bb79d99xzj6qqqnTvvfc2eu7Q0FD99re/1dNPPy1JyszM1IgRI/znx4wZI5vNJo/Ho9///veSpH/84x+6+OKLj9jznDlzdMstt2jZsmVavHixFi9e3OiaIUOG6LHHHlNUVNRRfwcdQWhoqJ599lnNnDlTP/zwg95++229/fbbja5LT0/Xf/7zn0Y3vTzW33VzjB8/Xv/4xz905513aseOHbr++usbXdOnTx8988wz/l3yY8eO1ezZs/XII49o27ZtuvHGGxs9xmq16uabb9app5563D0CAAAEK3ZmAwAAoFWMGDFC6enpkqRhw4apT58+h7324Ycf1r333quRI0fK4XDIbrcrLi5OI0eO1B//+Ed9+OGHuu666/wjSZYsWdKsHlJTU/Xuu+/quuuuU9++fRUWFqa4uDiddNJJmjt3ru68887DPvaKK67QCy+8oEmTJik+Pl42m00Oh0MDBgzQjBkz9MEHH2jWrFn+9/jJJ58YHt+vXz898sgjGjBggMLCwhQbG9usG0Y6HA49/fTTmjt3riZPnqzk5GSFhIQoJSVFEyZM0IMPPqj58+crLS2tWb+DjiI5OVnz58/Xgw8+qJNPPlmJiYmy2+2KiorSyJEjdfvtt+u9995rcizHsf6um2vatGlatGiRrrrqKqWnpyssLEwREREaMmSIbrnlFi1YsKDRjTivvfZavfbaa5o6dap69Oih0NBQhYWFKS0tTZdcconefPNNXXfdda3WIwAAQDCy+A53xxUAAAAAAAAAAAIEO7MBAAAAAAAAAAGPMBsAAAAAAAAAEPAIswEAAAAAAAAAAY8wGwAAAAAAAAAQ8AizAQAAAAAAAAABjzAbAAAAAAAAABDwCLMBAAAAAAAAAAHPbnYDbamgoMLsFo6b1WpRQoJDxcVOeb0+s9sB0I5Y/0Bw4zMACF6sfyB4sf6B4MX6l5KTo496DTuzA5zVapHFYpHVajG7FQDtjPUPBDc+A4DgxfoHghfrHwherP/mIcwGAAAAAAAAAAQ8wmwAAAAAAAAAQMAjzAYAAAAAAAAABDzCbAAAAAAAAABAwCPMBgAAAAAAAAAEPMJsAAAAAAAAAEDAI8wGAAAAAAAAAAQ8wmwAAAAAAAAAQMAjzAYAAAAAAAAABDzCbAAAAAAAAABAwCPMBgAAAAAAAAAEPMJsAAAAAAAAAEDAs5vdAAAAAAAAAIDg4PX6VFld166vGRUeIqvV0uzrJ07M0OOP/1ujRmW0YVc4FoTZAAAAAAAAANrc11vyNW/JVpW72jfMjokM0VVnDdSYQSnt+rpofYTZAAAAAAAAANrci4u3qKrG3e6vW+6q04uLt7RamP3BB+/q9ddfVX7+fqWmpurqq3+ls846V5L03Xfr9MQTjyonZ49iY+M0YcLJuv76m2W32/XZZ5/qv/99RgUF+5WUlKwzzzxHv/rVNa3SU7AgzAYAAAAAAACAZli06AM9+eSjeuCBORoxYpS++26d7rjjFoWFhevUU0/TfffdpWuuuU7nnnuB8vJy9Yc//FYnnjhC48efpHvvvUtz5jymUaMytG3bFl1//bUaN268Bg8eYvbb6jAIswEAAAAAAAC0uV+dO8jUMSOtYeHC93XhhRdr9OgxkqTRo8fowgsv1nvvLdCpp56msLAwLVv2iWJiYjVy5CgtWLBQVqtVNTXVCgsL08KF78nr9WrYsBP18cefyWq1tkpfwYIwGwAAAAAAAECbGzMoRaMHJAf8DSCPpLi4SN26dTfUunbtri+++FyS9NhjT+v555/Vww//U0VFhRo3boJuueV2paR00dNPP6eXXnpO99xzp5xOp0477XTdfPOtiomJaZXeggFhNgAAAAAAAIB2YbVaFBMZanYbx6xr127Kyckx1HJy9ioxMUk1NTXKzNyl2bNvl91uV3Z2lh588O96/PFH9Oc//1WFhQX629/+Lknavn2r7r77L3r55Rd0/fU3m/FWOiT2sQMAAAAAAADAIUpLS5Wfv9/w5Xa7dcEF0/Teewv07bdfy+PxaN26b/T+++/o/POnymKx6O67/6L581+R2+1WYmKi7Ha74uLiVFVVpVtvvVlLlnwkn8+npKRkWSxWxcbGmv1WOxSLz+fzmd1EWykoqDC7heNmt1sVH+9QSYlTbrfX7HYAtCPWPxDc+AwAghfrHwherH8gMEycmNFkfd68t9SrV7o+/PBdzZ//qvbvz1NycoqmT79S06ZdIknasOF7Pfnkv5SZuUtWq03jx5+kP/7xT4qKitIXX3yu//znaeXm5igsLEyTJ5+pG274P4WEhLD+JSUnRx/1GsLsAMdfZCB4sf6B4MZnABC8WP9A8GL9A8GL9d+8MJsxIwAAAAAAAACAgMcNIDsZr8+rvRW58sqrXtFpslha506tAAAAAAAAAGAmwuxOZsGOD7V8zxeSpNN6TNSlA6aa3BEAAAAAAAAAHD/GjHQiPp9PX+R85T/+bO8qVdRWmtgRAAAAAAAAALQOwuxOxCefrBar4TizPNvEjgAAAAAAAACgdRBmdyJWi1U9o3sYalnle0zqBgAAAAAAAABaD2F2J9MzpmGYvdekTgAAAAAAAACg9RBmdzLpMT0Nx1nle+Tz+UzqBgAAAAAAAABaB2F2J9OrwZgRp9uloupik7oBAAAAAAAAgNZBmN3JJITHKyrEYahlMjcbAAAAAAAAQAdnN7sBtC6LxaL0mDT9WLTFX8sq36OMLiPMawoAAAAAAACQ5PV55axztetrOkIiZbW07Z7eRYs+0PPPP6u33vrgqNc+99wzWr/+Wz355LNt2lNnRJjdCfVsIswGAAAAAAAAzLQu/we9sfVdVdRVtuvrRodEafrAaRqVMrxdXxetjzC7E0qPSTMcZ1fkyOP1yGa1mdQRAAAAAAAAgt2rW95Slbu63V+3oq5Sr255q1lh9n333SWPx6O7777fX7vrrj8rNjZO48aN1yuvvKi9e/eoqsqlwYOH6E9/ulNpaT2Pq7/PP/9ML730X+3du0eJiUmaNu0SXXrpz2W1WrVr1049/PA/tXPnDjkcDo0cOVp//ONtiox06Lvv1umJJx5VTs4excbGacKEk3X99TfLbu+8kS8zszuhXtHGMLvOW6d9rnyTugEAAAAAAAA6hqlTL9LKlZ/J6azfPV5RUaEvvvhckyefqbvuul1XX/0rffjhJ1qwYKF8Pp9efPE/x/V669Z9o7vuul0zZvxKa9eu1b33PqD58+fpzTdfkyQ98siDysgYq8WLl+m5517R9u1b9f7770iqD94vvfRyffTRZ/rXv+Zq+fKl+uKLFcfVT6AjzO6EokIdSgxPMNQyy7NN6gYAAAAAAACQrhx0qaJDotr9daNDonTloEubde2JJ45Uly6pWr58qSRp6dKP1atXLw0ZMkwvv/yGJk48RS6XU/n5+xUbG6eCgoLj6m3hwvd18smTdMYZZ8lut2vQoMG6+upf6b33FkiSQkPD9NVXX2r58k9ltVr0wguv6uc/v1qSFBYWpmXLPtGqVSsVGxurBQsWatKkycfVT6DrvHvOg1x6TJqKqov9x1nle3VSt3EmdgQAAAAAAIBgNipluEYkDw34G0BecME0ffTRIl1wwTQtWvSBLrhgmux2uz755CO9994CWSwW9enTV06nUzbb8Y31LSkpVv/+Aw21rl27ad++PEnSvff+Q88//4yeffYp3X13joYNO1GzZ9+uPn366rHHntbzzz+rhx/+p4qKCjVu3ATdcsvtSknpclw9BTJ2ZndSPWN6GI65CSQAAAAAAADMZrVYFR0a1a5fLQmyJenccy/Qpk0/6uuv12jnzh0688xztGzZJ3r77Tf0xBPPaMGChZoz53ENGDDw6E92FKmpXZWTs9dQy83dq8TEJHm9Xm3btkW/+c3vNX/+O3rzzfcVH5+gBx64RzU1NcrM3KXZs2/XggUL9fLLb8jprNTjjz9y3D0FMsLsTio9xjh4Pte5T7WeWpO6AQAAAAAAADqG+Ph4TZhwsh588O+aNOl0xcTEqLKyUlarVWFhYfL5fPrqqy/10UcL5Xa7j+u1zj//Qn3xxQp9+ukn8ng82rp1i+bN+5/OP3+qrFar/vWvh/Sf/8xVTU2N4uLiFRYWqtjYOFksFt199180f/4rcrvdSkxMlN1uV1xcXOv8EgIUYXYnlRbdXRZZ/Mden1d7K3NN7AgAAAAAAADoGKZOvUj79uXpggsulFS/WzsjY6xmzJiuCy44Qy+99JymT79S2dlZqqurO+bXGTJkqP7+9wf1v/+9oIyMDP3pT7M1bdolmjHj15Kk++57UJmZmbrwwnM0depZqqio1G233aHQ0FD985+PaOXKFTr//Mm69NKpSkxM0nXX3dgq7z9QWXw+n8/sJtpKQUGF2S0cN7vdqvh4h0pKnHK7vS167P1rHlGuc5//+JL+U3R62smt3SKANnI86x9Ax8dnABC8WP9A8GL9A8GL9S8lJ0cf9Rp2ZndivWLSDMfMzQYAAAAAAADQUdnNbgBtp1dMmlbnfe0/JswGAAAAAAAA2s5nn32q+++/+7Dnhw8fqYcffrz9GupkCLM7sfQGO7MLqorkrHPJERJpUkcAAAAAAABA5zVp0mRNmjTZ7DY6LcaMdGLdHKkKsRr/vSK7fK9J3QAAAAAAAADAsSPM7sRsVpt6RHU31DIZNQIAAAAAAACgAyLM7uR6xfQwHGdVEGYDAAAAAAAA6HgIszu5Xg3mZmeV75HP5zOpGwAAAAAAAAA4NoTZnVzDm0CW11aotKbMpG4AAAAAAAAA4NgQZndyyRFJirBHGGpZzM0GAAAAAAAA0EJ79mSb+votDrOLioo0c+ZMZWRkaNy4cbr//vvldrubvHbFihWaMmWKRowYoXPPPVfLly/3n6upqdH999+vU045RaNHj9Zll12mr776yn/e5XLpz3/+s8aNG6fRo0frtttuk9PpPIa3GNwsFot6RTecm73XpG4AAAAAAAAQ7AoLC4/5q6qq6rDPW1RU1ORjWmrixAytW/fN8bzFVvPQQw/ooYceMLsNSdLbb7+h//f/7je1B3tLHzBr1ix16dJFK1euVGFhof7whz/oxRdf1DXXXGO4LjMzUzfeeKMeeeQRTZo0SUuWLNGsWbO0ZMkSdenSRXPmzNG6dev0+uuvKyUlRW+//bauu+46LVq0SN26ddN9992nvLw8ffzxx/J4PJo1a5bmzJmjv/3tb6325oNFr5g0bSnZ7j/OZGc2AAAAAAAATHLCCX2O+bH/+Mcc/fa31zZ5buLEDBUVFTWq5+eXH/Prme3WW+8wuwW/0tIS0+/F16IwOysrS2vXrtXnn3+uiIgIpaWlaebMmXrooYcahdnvvPOOMjIydMYZZ0iSzjvvPC1YsECvv/66brrpJtXU1Oimm25S165dJUnTp0/XnDlztHHjRsXHx+uDDz7Q//73P8XFxUmSbrnlFv3iF7/QbbfdpogI49gMHFnDm0Bml++V1+eV1cKUGQAAAAAAAKAlli79WC+//IL27ctT9+5puu66GzR27M8kSYWFBXr88Ue0efNGFRcXKSEhSb/85W90wQUXSqoP3C+99HJ98slHGjJkuCZNOl0ffPCuBg8+QZ988pEk6aSTTtEtt/xZdrtd999/tyTpL3+5W88994x2796p0NAwffnlSkVEROrss8/TddfdIEmqqanWE088qk8//UQRERE699wLtGTJYv35z3dp1KgMw3vIy8vVZZdN1eWXX6WFC9/XmWeeo5tu+qOefXauvvxypfLz8xUWFqbJk8/UrFm36qOPFurll1+Q1+vVOedM0kcffSaXy6l///tJffHF56qtrdXo0Rm6+eZblJCQ2Ga/+xaF2du3b1dcXJy6dOnir/Xt21e5ubkqLy9XTEyMv75jxw4NGDDA8Ph+/fppy5YtkqR7773XcG716tWqqKjQoEGDlJWVpbq6OsPj+/btq+rqamVmZmrw4MHN6tdqtchqtbTkLQYcm81q+H4s+sb3NBxXe6pVXFusVEfKcfUGoG21xvoH0HHxGQAEL9Y/ELxY/8DR2WwW2e0tWyMtvb7+daxNPu7LL7/QnDn/0EMPParhw0do9epVuvPO2/Tf/76kPn366sEH/67Y2Fi99tqbCgkJ1euvv6ZHH31IZ511tiIjIyVJubk5eu+9Raqrc2vFiuXasOF7TZw4UStXrtQ333ynP/zhdxo37mc688yzZbFY/O/BarVoxYrl+utf79Hf/nav1qxZrdmzb9akSZM0dOhwPfzwo9qyZZNeeWW+oqNj9NBD/1BeXm6T7+Xg50x1tUuLFy9VdXW13nrrNa1Z86WeeuoZJSUla8OG73Xddb/TpEmnacqUqdq3L1fr1n2rp5/+jyTpn/+8T05npV58cZ7Cw8P12GMP6y9/uVXPPvuCv+/W1qIw2+l0NtoVffDY5XIZwuymrg0PD5fL5Wr0vN99951mzZqlG264QWlpafrmm/qZNAf/gA99nZbMzU5IcLTZL669xcQc+270eDmUEBGn4qpSfy2/br8Gx/duhc4AtLXjWf8AOj4+A4DgxfoHghfrHzi8yMgwxcc7mjx3uBzwcNcfSXR0eJOPe/fdt3TllVfq9NNPkSRNmXKuPvtsqRYtek9//etf9eCD/5DD4VB4eLjy8vKUnByvmppqWSx1/ue76KIL1bVrkiTJ4QhTeHi4br75RlksFo0fP0aDBg1Sfn6u4uMdCguz+99DRESo0tPTddVVl0uSzj//bP3zn8kqKtqvqKhQffTRIj3xxBMaNKivJOnvf79XS5Z81OR7cTrrP2cuv/wypaTESZJ++curdeWVlysxMVEFBQUKCbEoKsohl6vc//ohITbFxztUVFSkZcuWavHixerTp34qxD33/E0ZGRnKzc3U0KFDW/w7b44WhdmRkZGNhqwfPHY4jL+QiIgIVVdXG2rV1dWNrnvzzTf1wAMP6KabbtKvf/1r/+scfO6D1x98naioqGb3W1zs7BQ7s2NiIlReXiWPx3vMz9MzuochzN6Yt0PD44a1QocA2kprrX8AHROfAUDwYv0DwYv1j2CxbdvuY36swxGlkpKmN7uuXv1NkzOdD3f9kVRUVDf5uOzsPVqzZq1effVVf83j8SgjY6xKSpzatGmbnnjiX9qzJ1tpaT2VltbT30NERP3zRUbG+J/b6axRXFy8Kiqq/etfssrlqlFJiVM1NW7/46uqahUXl2Doy2q1qaKiSllZuaqqqlJ09KHnLYqNjWvyvZSV1Wet4eHR/nP5+UWaM+dBrV//rVJSumjgwEHyer2qqKjyv35dnUclJU5t3rxDknTZZZcZntdms2nr1p3q3r3lm2ib848OLQqz+/fvr9LSUhUWFiopqf5fD3bu3KnU1FRFR0cbrh0wYIA2btxoqO3YscOfyns8Ht1zzz1asmSJnnrqKU2YMMF/Xe/evRUSEqIdO3boxBNP9L9OSEiI0tPTm92v1+uT12vuUPLW4vF45XYfR5gd1UPf5f/oP95dln1czweg/Rzv+gfQsfEZAAQv1j8QvFj/6Ozi4o5vpvLh1kdsbEKLrj+Sw63D5OQUnX32+Zox41f+2r59+xQWFqbq6lrNnn2zrr32el188WWyWCzasmWzPvpokeH5PB6f/+eD2eXBf8DyeLzy+eozTbfb6w/n3W6vvF6ffD5fo768Xp+io+MUFhamnJxcde9eH6BXVVWprKy0yffy0+v99Hz333+fYmJi9O67HyksLExer1crVpzm7+XQ109MTJYkzZv3lhITk/zPu3v3LnXr1r3NPsNaNDAmPT1do0eP1gMPPKDKykrt2bNHc+fO1aWXXtro2qlTp2rt2rVatGiR3G63Fi1apLVr1+rCC+uHnf/jH//Q559/rrffftsQZEs6MKD8XM2ZM0fFxcUqLi7WnDlzdMEFFyg8PPw43m7wangTyJyKXLm9bpO6AQAAAAAAAAJXaWmp8vP3G77cbremTr1Ib701X5s312/i3bJlk6655motXfqx6urqVF1drfDwcFksFu3bt09PP/24JKmurq5N+7Varbrgggv13HPPqLCwQNXV1Xr88Ufk8Xia/RxOZ6VCQ0Nls9nkcjn11FOPyel0+nsPDQ2Ty+WUz+dTUlKyJkyYqMcee1hlZaVyu9166aXn9Lvf/UKVlRVt9TZbFmZL0uOPPy63263Jkydr+vTpOvnkkzVz5kxJ0siRI/X+++9Lqr9h41NPPaVnnnlGY8aM0dy5c/XEE0+od+/eKi4u1rx581RYWKgLLrhAI0eO9H8dfPzf/vY3paena8qUKTrnnHPUo0cP3XXXXa341oNLz+gehmO3z6OcyjyTugEAAAAAAAAC11133a6LLz7f8JWTs1ennXaGfv/76/XAA/forLNO1Z13/knTp1+pSy+9XBEREbrjjr/pxRf/qzPPPEU33fR7ZWSMU0JConbt2tHmPV933Y3q1StdV111qa644mJ16dJFVqtVISEhzXr8rFm3avv2bTr33NN0xRWXyOVyaty4Cf7eTzrpZJWVlenssyepoqJCd955r6KiovTrX1+l88+frNWrV+nhh5807NRubRZfU4NkOomCgrb7V4D2YrdbFR/vUEmJ87i359/71UPa7yrwH18+YJpO6THhCI8AYKbWXP8AOh4+A4DgxfoHghfrHwherbH+v/tunfr27e8fB+1yOXXWWafqtdcW+Gd3B7Lk5OijXtPindnouHpGG0eNZJXvNakTAAAAAAAAAK1p/vxX9Nhjc1RTU62amhr997//Vs+evTpEkN1chNlBJL3B3OzMij0mdQIAAAAAAACgNc2efbuczkpddNH5uvDCs7V37x499NBjZrfVquxmN4D20/AmkPud+ap2Vyvczk01AQAAAAAAgI4sOTlF//jHw2a30abYmR1EekR1ldXy0x+5Tz5lV+SY2BEAAAAAAAAANA9hdhAJsYWoR1RXQy2rnFEjAAAAAAAAAAIfYXaQ6RnT8CaQhNkAAAAAAAAAAh9hdpBJj25wE0jCbAAAAAAAAAAdAGF2kGl4E8iSmlKV11aY1A0AAAAAAAAANA9hdpBJdaQo1BZqqDFqBAAAAAAAAECgI8wOMlaLVb2iexhqWeV7TeoGAAAAAAAAAJqHMDsI9YxpGGazMxsAAAAAAABAYCPMDkLpMT0Nx1nle+Tz+UzqBgAAAAAAAACOjjA7CDUcM+J0u1RUXWxSNwAAAAAAAABwdITZQSghPF5RIQ5DLZNRIwAAAAAAAAACGGF2ELJYLEqPSTPUmJsNAAAAAAAAIJARZgepnoTZAAAAAAAAADoQwuwg1XBn9p6KHHm8HpO6AQAAAAAAAIAjI8wOUr2ijWF2rbdO+1z5JnUDAAAAAAAAAEdGmB2kokIdSgxPMNQyy7NN6gYAAAAAAAAAjowwO4g1vgnkXpM6AQAAAAAAAIAjI8wOYj1jehiOuQkkAAAAAAAAgEBFmB3E0mN6Go5znftU66kzqRsAAAAAAAAAODzC7CCWFt1dFln8x16fV3src0zsCAAAAAAAAACaRpgdxMJsoerq6GKoMTcbAAAAAAAAQCAizA5yvRrcBDKzPNukTgAAAAAAAADg8Aizg1zjMJubQAIAAAAAAAAIPITZQS69QZhdWFWkitpKk7oBAAAAAAAAgKYRZge5bo5UhdpCDTVGjQAAAAAAAAAINITZQc5mtalXdA9DbVdZlkndAAAAAAAAAEDTCLOh3rG9DMe7CbMBAAAAAAAABBjCbKhPgzA7q3yPPF6PSd0AAAAAAAAAQGOE2VB6TE/Dca23TrnOfSZ1AwAAAAAAAACNEWZD0aFRSo5INNQYNQIAAAAAAAAgkBBmQ1Ljudm7yrJN6gQAAAAAAAAAGiPMhiSpd0yDm0CWszMbAAAAAAAAQOAgzIakxjuzC6uKVFFbaVI3AAAAAAAAAGBEmA1JUjdHF4XaQg015mYDAAAAAAAACBSE2ZAk2aw2pUenGWq7y5mbDQAAAAAAACAwEGbDr+GoEXZmAwAAAAAAAAgUhNnw6x3b03CcVb5HHq/HpG4AAAAAAAAA4CeE2fDrHWPcmV3rrVOOM8+kbgAAAAAAAADgJ4TZ8IsKdSglIslQ213G3GwAAAAAAAAA5iPMhgFzswEAAAAAAAAEIsJsGDScm02YDQAAAAAAACAQEGbDoOHc7MLqYlXUVprUDQAAAAAAAADUI8yGQbeoVIXZQg21XezOBgAAAAAAAGAywmwYWC1W9Yph1AgAAAAAAACAwEKYjUb6NAyzywmzAQAAAAAAAJiLMBuN9I41zs3OKt8rj9djUjcAAAAAAAAAQJiNJqTHGndm13nrtLcy16RuAAAAAAAAAIAwG02ICnEoJTLJUNtdlm1SNwAAAAAAAABAmI3D6B1jHDXC3GwAAAAAAAAAZiLMRpMazs3eXUaYDQAAAAAAAMA8hNloUp8GYXZRdYnKaipM6gYAAAAAAABAsCPMRpO6Oroo3BZmqDFqBAAAAAAAAIBZCLPRJKvFql4xaYYao0YAAAAAAAAAmIUwG4fF3GwAAAAAAAAAgYIwG4fVO6an4Ti7Yq/cXrdJ3QAAAAAAAAAIZoTZOKyGO7PrvG7lVOaZ1A0AAAAAAACAYEaYjcNyhESqS2SyobaLUSMAAAAAAAAATECYjSPqHcPcbAAAAAAAAADmI8zGEfWONc7N3l2ebVInAAAAAAAAAIIZYTaOqOHc7OLqEpXVlJvUDQAAAAAAAIBgRZiNI+rq6KJwW5ihxqgRAAAAAAAAAO2NMBtHZLVYlR5jHDWyq5wwGwAAAAAAAED7IszGUTWam13G3GwAAAAAAAAA7YswG0fVcG52dsVeub1uk7oBAAAAAAAAEIwIs3FUvRuMGXF73dpbmWtSNwAAAAAAAACCEWE2jioyJFJdIlMMNUaNAAAAAAAAAGhPhNlolsZzs7kJJAAAAAAAAID2Q5iNZukTY5ybvYswGwAAAAAAAEA7IsxGszS8CWRJTalKa8pM6gYAAAAAAABAsCHMRrOkOlIUbgs31JibDQAAAAAAAKC9EGajWawWq9Jj0gw15mYDAAAAAAAAaC+E2Wi2hqNGdpcTZgMAAAAAAABoH4TZaLY+DcLs7Iocub1uk7oBAAAAAAAAEEwIs9Fs6TE9Dcdur1t7KnJN6gYAAAAAAABAMCHMRrNFhkQo1dHFUGPUCAAAAAAAAID2QJiNFunTYHc2N4EEAAAAAAAA0B4Is9EijW4CWZZtUicAAAAAAAAAgglhNlqkYZhdUlOq0poyk7oBAAAAAAAAECwIs9EiXSKTFWGPMNR2MWoEAAAAAAAAQBsjzEaLWC1WpcekGWrMzQYAAAAAAADQ1giz0WINR43sLMs0pxEAAAAAAAAAQYMwGy3WNzbdcLynIkfV7mpzmgEAAAAAAAAQFAiz0WJ9YnvJavnpr47X52VuNgAAAAAAAIA2RZiNFgu1hapXtHFu9o7S3SZ1AwAAAAAAACAYEGbjmPSP72M43l66y6ROAAAAAAAAAAQDwmwck35xxjA7q3yPaj11JnUDAAAAAAAAoLMjzMYx6RPbSxZZ/Mcen0eZ5czNBgAAAAAAANA2CLNxTCLs4UqL7m6obS9h1AgAAAAAAACAtkGYjWPWv8GoEW4CCQAAAAAAAKCtEGbjmPWL62043l2epTqv26RuAAAAAAAAAHRmhNk4Zv3iehvmZtd53coq32NiRwAAAAAAAAA6K8JsHLPIkEh1i0o11Bg1AgAAAAAAAKAtEGbjuPRrNDebm0ACAAAAAAAAaH2E2TguDW8CubMsUx6vx6RuAAAAAAAAAHRWhNk4Lg1vAlnrqdWeyhyTugEAAAAAAADQWRFm47hEh0YpNTLFUNtewqgRAAAAAAAAAK2LMBvHrV98w7nZ3AQSAAAAAAAAQOsizMZx6x9rHDWys2y3vD6vSd0AAAAAAAAA6IwIs3HcGu7MrnJXK6dyn0ndAAAAAAAAAOiMCLNx3OLCYpUckWio7ShlbjYAAAAAAACA1kOYjVbRL864O3s7YTYAAAAAAACAVkSYjVbRP67hTSB3yefzmdQNAAAAAAAAgM6GMButol+c8SaQzjqX8pz7TeoGAAAAAAAAQGdDmI1WkRiRoPiwOEONudkAAAAAAAAAWgthNlpN//iGo0Z2m9QJAAAAAAAAgM6GMButpuGoke3MzQYAAAAAAADQSgiz0Woa3gSyvLZCBVWFJnUDAAAAAAAAoDMhzEarSY5IUmxotKG2nbnZAAAAAAAAAFoBYTZajcViUb8Gu7O3lzA3GwAAAAAAAMDxI8xGq2oYZu9gZzYAAAAAAACAVkCYjVbVP94YZpfUlKqoqtikbgAAAAAAAAB0FoTZaFWpkSmKCnEYajtKGTUCAAAAAAAA4PgQZqNV1c/N7m2ocRNIAAAAAAAAAMeLMButrtFNIAmzAQAAAAAAABwnwmy0uoZhdmFVkUprykzqBgAAAAAAAEBnQJiNVtc9KlUR9ghDbUcJu7MBAAAAAAAAHDvCbLQ6q8WqvrHphhqjRgAAAAAAAAAcD8JstIn+8cZRIztKd5vUCQAAAAAAAIDOgDAbbaJ/g7nZ+1z5qqitNKkbAAAAAAAAAB0dYTbaRI+obgqzhRpq7M4GAAAAAAAAcKxaHGYXFRVp5syZysjI0Lhx43T//ffL7XY3ee2KFSs0ZcoUjRgxQueee66WL1/e5HV///vfdfvttxtq33//vQYNGqSRI0f6v6666qqWtguT2Kw29WFuNgAAAAAAAIBW0uIwe9asWYqMjNTKlSv11ltvafXq1XrxxRcbXZeZmakbb7xRN998s7755hvdeOONmjVrlvbv3++/pqSkRLfccotefvnlRo/fsGGDxowZo/Xr1/u/5s2b19J2YaKGo0Z2EGYDAAAAAAAAOEYtCrOzsrK0du1a3XrrrYqIiFBaWppmzpzZZMj8zjvvKCMjQ2eccYbsdrvOO+88jRkzRq+//rokyel06pxzzlFMTIzOPvvsRo/fsGGDhg4deoxvC4GgX4MwO7dyn5x1LpO6AQAAAAAAANCR2Vty8fbt2xUXF6cuXbr4a3379lVubq7Ky8sVExPjr+/YsUMDBgwwPL5fv37asmWLJCksLEwLFy5UUlJSoxEjUn2YnZSUpLPOOkuVlZUaO3asbr/9dqWmpja7X6vVIqvV0pK3GHBsNqvhe0fSN6GnQqx21Xnrx9D45FNmRZZOTBlicmdAx9CR1z+A48dnABC8WP9A8GL9A8GL9d88LQqznU6nIiIiDLWDxy6XyxBmN3VteHi4XK76nbl2u11JSUlNvo7H41FKSoomTJigK664QnV1dbrvvvt07bXX6p133pHNZmtWvwkJDlksHTvMPigmJuLoFwWgAUl9tDF/m/8425WtSfFjTewI6Hg66voH0Dr4DACCF+sfCF6sfyB4sf6PrEVhdmRkpKqqqgy1g8cOh8NQj4iIUHV1taFWXV3d6Lqm2Gy2RnO4//rXv2r8+PHauXNnox3fh1Nc7OwUO7NjYiJUXl4lj8drdjst1js63RBmb9i3VSUlThM7AjqOjr7+ARwfPgOA4MX6B4IX6x8IXqx/KT7+6Llxi8Ls/v37q7S0VIWFhf5d1Tt37lRqaqqio6MN1w4YMEAbN2401Hbs2NGsOdh5eXl68cUXddNNN/nD79raWkn1u7uby+v1yev1Nfv6QObxeOV2d7y/yH1jehuOs8tzVFHtUoS9+X+OQLDrqOsfQOvgMwAIXqx/IHix/oHgxfo/shYNYUlPT9fo0aP1wAMPqLKyUnv27NHcuXN16aWXNrp26tSpWrt2rRYtWiS3261FixZp7dq1uvDCC4/6OvHx8Vq4cKEeffRR1dTUqLi4WPfcc4/Gjx+vnj17tqRlmCw9pqfslp/Gwvjk066yLBM7AgAAAAAAANARtXii+OOPPy63263Jkydr+vTpOvnkkzVz5kxJ0siRI/X+++9Lqr8x5FNPPaVnnnlGY8aM0dy5c/XEE0+od+/eR3p6SfW7r//73/9q586dmjhxos4++2xFRUXpX//6V0vbhclCbSHqFZNmqO0o3WVSNwAAAAAAAAA6KovP5+sccziaUFBQYXYLx81utyo+3qGSEmeH/U8MPtj5kT7KWuY/7h3TS7dkXG9iR0DH0BnWP4Bjx2cAELxY/0DwYv0DwYv1LyUnRx/1mhbvzAZaql9cH8NxVsUe1XpqTeoGAAAAAAAAQEdEmI021zu2l6yWn/6qeX1e5mYDAAAAAAAAaBHCbLS5cHuYekb3MNS2l+w0qRsAAAAAAAAAHRFhNtpF/wajRjYVbzWpEwAAAAAAAAAdEWE22sUJiQMNx9kVOaqorTSpGwAAAAAAAAAdDWE22kWf2F4Ks4UaapuLt5nUDQAAAAAAAICOhjAb7cJutWtgfH9DbVMRo0YAAAAAAAAANA9hNtpNw1Ejm4u3yevzmtQNAAAAAAAAgI6EMBvt5oSEAYbjyjqn9lTkmNQNAAAAAAAAgI6EMBvtJjEiQV0iUwy1TUXMzQYAAAAAAABwdITZaFcnJBp3Z28qZm42AAAAAAAAgKMjzEa7OiHBODd7d1mWXHUuk7oBAAAAAAAA0FEQZqNd9YvroxCr3X/sk09bSnaY2BEAAAAAAACAjoAwG+0q1Bai/nF9DbVNRYwaAQAAAAAAAHBkhNlodyckGkeNbCraKp/PZ1I3AAAAAAAAADoCwmy0uxMSjDeBLKstV65zn0ndAAAAAAAAAOgICLPR7lIik5UYnmCoMWoEAAAAAAAAwJEQZqPdWSyWxqNGireZ1A0AAAAAAACAjoAwG6ZoOGpkZ+luVbtrTOoGAAAAAAAAQKAjzIYpBsT3lc1i8x97fB5tK9lhYkcAAAAAAAAAAhlhNkwRbg9X39h0Q41RIwAAAAAAAAAOhzAbpmk0N7toi3w+n0ndAAAAAAAAAAhkhNkwTcMwu6i6RPlVhSZ1AwAAAAAAACCQEWbDNN0cqYoNjTbUNhVtNakbAAAAAAAAAIGMMBumsVgsGtxw1EgxYTYAAAAAAACAxgizYaohiYMMx9tLdqnWU2dSNwAAAAAAAAACFWE2TDUovp8ssviP67x12lm628SOAAAAAAAAAAQiwmyYKjIkUr1jexpqG4u3mNQNAAAAAAAAgEBFmA3TnZDQYG520TaTOgEAAAAAAAAQqAizYboTGtwEcr8rX0VVxSZ1AwAAAAAAACAQEWbDdGnR3RUV4jDUNhWzOxsAAAAAAADATwizYTqrxapBCf0Ntc1FW03qBgAAAAAAAEAgIsxGQGg4N3tryQ65vW6TugEAAAAAAAAQaAizERAGJw4wHFd7arS7LMukbgAAAAAAAAAEGsJsBISY0Gj1jO5uqDE3GwAAAAAAAMBBhNkIGA1HjWws2mJSJwAAAAAAAAACDWE2AsbgRGOYnVOZp9KaMpO6AQAAAAAAABBICLMRMHrH9FSEPdxQ21y83aRuAAAAAAAAAAQSwmwEDJvVpoHx/Q21zUVbTeoGAAAAAAAAQCAhzEZAOSFxgOF4c/E2eX1ek7oBAAAAAAAAECgIsxFQGt4E0uWuUlb5HpO6AQAAAAAAABAoCLMRUOLD49TV0cVQ28SoEQAAAAAAACDoEWYj4DTcnb2peJtJnQAAAAAAAAAIFITZCDgnJBrD7KzyPaqsdZrUDQAAAAAAAIBAQJiNgNM3rrdCrSH+Y5982sLubAAAAAAAACCoEWYj4IRY7RoQ389QY9QIAAAAAAAAENwIsxGQGo4a2VS8VV6f16RuAAAAAAAAAJiNMBsBqeFNICtqK7W3MtekbgAAAAAAAACYjTAbASk5MlHJEYmG2vf5P5rUDQAAAAAAAACzEWYjYA1PHmI4Xlfwg3w+n0ndAAAAAAAAADATYTYC1sjk4YbjfFehcp37TOoGAAAAAAAAgJkIsxGw0mPSFB8WZ6itz//BnGYAAAAAAAAAmIowGwHLYrFoZMowQ21d/gZGjQAAAAAAAABBiDAbAW1UinHUyH5XvvKc+03qBgAAAAAAAIBZCLMR0HrFpCkuLNZQY9QIAAAAAAAAEHwIsxHQrBZr41EjBRtM6gYAAAAAAACAWQizEfAajhrZ59zPqBEAAAAAAAAgyBBmI+Clx/Rk1AgAAAAAAAAQ5AizEfCsFqtGJA811NbnM2oEAAAAAAAACCaE2egQRjYYNZLr3Kd9znyTugEAAAAAAADQ3giz0SH0ie2l2NBoQ43d2QAAAAAAAEDwIMxGh2C1WDUiZZihtr6AudkAAAAAAABAsCDMRocxMtk4aiSnMk/7XQUmdQMAAAAAAACgPRFmo8PoG5eumEajRtidDQAAAAAAAAQDwmx0GFaLVSOShxpqzM0GAAAAAAAAggNhNjqUkSnGUSN7K3OV7yo0qRsAAAAAAAAA7YUwGx1Kv7jeig6JMtQYNQIAAAAAAAB0foTZ6FCsFqtOTGkwaqSAUSMAAAAAAABAZ0eYjQ5nVLJx1MieihwVVhWZ1A0AAAAAAACA9kCYjQ6nX1xvRYU4DLV1jBoBAAAAAAAAOjXCbHQ4NqtNI5IbjBrJZ9QIAAAAAAAA0JkRZqNDGpliHDWSXbFXhVXFJnUDAAAAAAAAoK0RZqND6h/Xp9GokfWMGgEAAAAAAAA6LcJsdEg2q00nJg8x1Bg1AgAAAAAAAHRehNnosEYmG0eNZFXsUVFViUndAAAAAAAAAGhLhNnosAbE95XDHmmorS9g1AgAAAAAAADQGRFmo8Ni1AgAAAAAAAAQPAiz0aGNTDGOGsksz1ZxNaNGAAAAAAAAgM6GMBsd2sD4foq0Rxhq37E7GwAAAAAAAOh0CLPRodmsNg1vMGpkHWE2AAAAAAAA0OkQZqPDG9Vg1Mju8iyVVJea0wwAAAAAAACANkGYjQ5vYHw/RTQcNVLwo0ndAAAAAAAAAGgLhNno8OxWu4YnnWCorcv/waRuAAAAAAAAALQFwmx0Cg1Hjewqy1RpTZlJ3QAAAAAAAABobYTZ6BQGJvRXuC3cUPsun1EjAAAAAAAAQGdBmI1OIcRq1/BkRo0AAAAAAAAAnRVhNjqNpkaNlFSXmtMMAAAAAAAAgFZFmI1OY1DCAMOoEZ98+jLvaxM7AgAAAAAAANBaCLPRaYRY7cpIHWGorc79Wl6f15yGAAAAAAAAALQawmx0Kid1G2s4Lqkp1ebibSZ1AwAAAAAAAKC1EGajU+kZ3UNp0d0NtVW5a03qBgAAAAAAAEBrIcxGp9Nwd/aGwk0qq6kwqRsAAAAAAAAArYEwG51ORpeRCrWG+I+9Pq/W7PvGxI4AAAAAAAAAHC/CbHQ6EfZwjUo50VD7MnetfD6fSR0BAAAAAAAAOF6E2eiUTupuHDVSUFWk7aU7TeoGAAAAAAAAwPEizEan1Duml1IdXQw1bgQJAAAAAAAAdFyE2eiULBZLoxtBfpe/QZV1TpM6AgAAAAAAAHA8CLPRaY1NHSW7xeY/dvs8WrtvnYkdAQAAAAAAADhWhNnotKJCHDoxeaihxo0gAQAAAAAAgI6JMBud2kndxhmO85z7tbs826RuAAAAAAAAABwrwmx0av3j+ygpItFQW5W7xqRuAAAAAAAAABwrwmx0alaLVSd1Nd4Ict3+71XlrjapIwAAAAAAAADHgjAbnd64rhmyWn76q17rrdM3+9eb2BEAAAAAAACAliLMRqcXGxatYUknGGqrctea1A0AAAAAAACAY0GYjaAwoesYw/GeihxlV+w1qRsAAAAAAAAALUWYjaBwQuJAxYfFGWpf5n5tTjMAAAAAAAAAWowwG0HBarFqfNcMQ+3rfetV46k1qSMAAAAAAAAALUGYjaAxvtsYWWTxH1d7qrUu/wcTOwIAAAAAAADQXITZCBoJ4fEanDjAUPsyd41J3QAAAAAAAABoCcJsBJWTuo0zHO8qy1Ju5T6TugEAAAAAAADQXITZCCrDEgcrOjTKUFudx40gAQAAAAAAgEBHmI2gYrPa9LNU440g1+z7VnVet0kdAQAAAAAAAGgOwmwEnQndxhqOnXUufV/wo0ndAAAAAAAAAGgOwmwEnZTIJA2I62uorcpda1I3AAAAAAAAAJqDMBtB6aQGu7O3lexQgavIpG4AAAAAAAAAHA1hNoLSiclD5bBHGmpf5rE7GwAAAAAAAAhUhNkISiG2EI1NHWWofZX3jTxej0kdAQAAAAAAADgSwmwErYY3giyvrdCPRZtN6gYAAAAAAADAkRBmI2h1i0pV75hehtoXuWtM6gYAAAAAAADAkRBmI6g1vBHkpqKtyq3cZ1I3AAAAAAAAAA6HMBtBbVSXExVpjzDUlmavMKkbAAAAAAAAAIdDmI2gFmYL1ak9TjLUvt6/XkVVJSZ1BAAAAAAAAKApLQ6zi4qKNHPmTGVkZGjcuHG6//775Xa7m7x2xYoVmjJlikaMGKFzzz1Xy5cvb/K6v//977r99tsNNZfLpT//+c8aN26cRo8erdtuu01Op7Ol7QJHNanHSQq1hviPvT6vPt3zuYkdAQAAAAAAAGioxWH2rFmzFBkZqZUrV+qtt97S6tWr9eKLLza6LjMzUzfeeKNuvvlmffPNN7rxxhs1a9Ys7d+/339NSUmJbrnlFr388suNHn/fffcpLy9PH3/8sZYsWaK8vDzNmTOnpe0CRxUV6tBJ3cYZal/mrlVFbaVJHQEAAAAAAABoqEVhdlZWltauXatbb71VERERSktL08yZMzVv3rxG177zzjvKyMjQGWecIbvdrvPOO09jxozR66+/LklyOp0655xzFBMTo7PPPtvw2KqqKn3wwQe66aabFBcXp8TERN1yyy1asGCBqqqqjuPtAk07vefJslp+Wg513jqt2LvKxI4AAAAAAAAAHMrekou3b9+uuLg4denSxV/r27evcnNzVV5erpiYGH99x44dGjBggOHx/fr105YtWyRJYWFhWrhwoZKSkhqNGMnKylJdXZ3h8X379lV1dbUyMzM1ePDgZvVrtVpktVpa8hYDjs1mNXxH20iJStS4rqO0Ovcbf23F3i91Tp/TFG4PN7EzBDPWPxDc+AwAghfrHwherH8geLH+m6dFYbbT6VRERIShdvDY5XIZwuymrg0PD5fL5ap/YbtdSUlJTb5OZWX9eIfIyMhGr9OSudkJCQ5ZLB07zD4oJibi6BfhuFw2/DxDmO1yV+mbovWaMugME7sCWP9AsOMzAAherH8geLH+geDF+j+yFoXZkZGRjcZ8HDx2OByGekREhKqrqw216urqRtcd7nUOPvfB6w++TlRUVLP7LS52doqd2TExESovr5LH4zW7nU7NoRidmDxE3xds9Nc+2PKJxiWPUYi1RUsFaBWsfyC48RkABC/WPxC8WP9A8GL9S/HxR8+NW5TQ9e/fX6WlpSosLPTvqt65c6dSU1MVHR1tuHbAgAHauHGjobZjxw4NHTr0qK/Tu3dvhYSEaMeOHTrxxBP9rxMSEqL09PRm9+v1+uT1+pp9fSDzeLxyu4PzL3J7OrPnJEOYXVpTrtV7v9GEbmNN7ArBjvUPBDc+A4DgxfoHghfrHwherP8ja9EQlvT0dI0ePVoPPPCAKisrtWfPHs2dO1eXXnppo2unTp2qtWvXatGiRXK73Vq0aJHWrl2rCy+88KivExERoXPPPVdz5sxRcXGxiouLNWfOHF1wwQUKD2d+MdpO79he6h/Xx1D7JPszeX18iAAAAAAAAABmavFE8ccff1xut1uTJ0/W9OnTdfLJJ2vmzJmSpJEjR+r999+XVH/DxqeeekrPPPOMxowZo7lz5+qJJ55Q7969m/U6f/vb35Senq4pU6bonHPOUY8ePXTXXXe1tF2gxc7qdZrhON9VaNitDQAAAAAAAKD9WXw+X+eYw9GEgoIKs1s4bna7VfHxDpWUOPlPDNqJz+fTP79+THsrc/21ntHddVvGTZ3mhqLoGFj/QHDjMwAIXqx/IHix/oHgxfqXkpOjj3pNi3dmA52dxWLRWb0mGWrZFTnaWrLDnIYAAAAAAAAAEGYDTRmRPExJEYmG2pKs5SZ1AwAAAAAAAIAwG2iCzWrTmT1PNdS2luxQVvkekzoCAAAAAAAAghthNnAY41JHKybUOKtnSdZn5jQDAAAAAAAABDnCbOAwQmwhOj3tZEPt+4Iftc+Zb1JHAAAAAAAAQPAizAaOYGL3nynCHu4/9smnpdkrTOwIAAAAAAAACE6E2cARRNjDdXL38Yba2n3rVFJdak5DAAAAAAAAQJAizAaO4rS0iQqx2v3HHp9Hy/asNLEjAAAAAAAAIPgQZgNHERMarfFdxxhqX+SukbPOZVJHAAAAAAAAQPAhzAaaYXLPU2W1/LRcaj21WrF3lYkdAQAAAAAAAMGFMBtohqSIBI1KGW6ofbZ3lWo8tSZ1BAAAAAAAAAQXwmygmc7qdZrh2Fnn0pe5a03qBgAAAAAAAAguhNlAM3WP6qohiYMMtU+zP5fH6zGpIwAAAAAAACB4EGYDLdBwd3ZJTam+3r/epG4AAAAAAACA4EGYDbRAv7je6hObbqgt2r1UdV63OQ0BAAAAAAAAQYIwG2ihs3pNMhwXVRfri5yvzGkGAAAAAAAACBKE2UALDU0crD6xvQy1xZlL5aqrMqkjAAAAAAAAoPMjzAZayGKx6KJ+5xtqzjqXPsn+zJyGAAAAAAAAgCBAmA0cgz6x6RqRPNRQW75npUqqS81pCAAAAAAAAOjkCLOBYzS177myWn5aQnVetz7cvcTEjgAAAAAAAIDOizAbOEZdIpN1UrdxhtqavG+VU5lnUkcAAAAAAABA50WYDRyH83qfoVBbqP/YJ5/e3bnIxI4AAAAAAACAzokwGzgOMaHROrPnqYbapqKt2lK83aSOAAAAAAAAgM6JMBs4TqennaKY0GhD7d2di+T1eU3qCAAAAAAAAOh8CLOB4xRuD9P5vc801PZU5Ojb/d+b1BEAAAAAAADQ+RBmA61gfNcx6hKZYqh9sOsj1XndJnUEAAAAAAAAdC6E2UArsFltmtb3XEOtqLpEK/d+aVJHAAAAAAAAQOdCmA20kmFJJ6hvbG9DbXHmp3LVuUzqCAAAAAAAAOg8CLOBVmKxWHRRv/MMNZe7SkuyPjOnIQAAAAAAAKATIcwGWlHv2F4amTzMUFu+9wsVV5eY1BEAAAAAAADQORBmA61sat9zZLX8tLTcXrc+3LXExI4AAAAAAACAjo8wG2hlKZHJOrn7zwy1tfvWaW9FrkkdAQAAAAAAAB0fYTbQBs5NP0PhtjD/sU8+vbtzkYkdAQAAAAAAAB0bYTbQBqJDo3Rmr0mG2ubibdpcvM2chgAAAAAAAIAOjjAbaCOnp52s2NAYQ+3dHYvk9XlN6ggAAAAAAADouAizgTYSagvVBX3OMtT2Vubqm/3fmdMQAAAAAAAA0IERZgNtaFzqaKU6uhhq7+/8SHWeOpM6AgAAAAAAADomwmygDdmsNk3re66hVlJTqqXZK0zqCAAAAAAAAOiYCLOBNjY0cbD6x/Ux1BZnfqrcyn0mdQQAAAAAAAB0PITZQBuzWCy6uP8Fslp+Wm4en0evbH5THq/HxM4AAAAAAACAjoMwG2gHPaN76IyepxpqWRV7tHzvFyZ1BAAAAAAAAHQshNlAOzkv/Qx1iUw21D7c9bH2uwpM6ggAAAAAAADoOAizgXYSYgvR1YMvk0UWf63O69a8zW/K6/Oa2BkAAAAAAAAQ+AizgXbUJzZdk9JOMtR2lmXq85zVJnUEAAAAAAAAdAyE2UA7m9LnHCWFJxhq7+1crKKqYpM6AgAAAAAAAAIfYTbQzsJsobpy0KWGWq2nVq9ueVs+n8+krgAAAAAAAIDARpgNmGBgQj9N7DbOUNtSsl2r8742qSMAAAAAAAAgsBFmAyaZ1u98xYXFGmpvb/9QpTVlJnUEAAAAAAAABC7CbMAkEfZwXTnoEkOt2lOt17YsYNwIAAAAAAAA0ABhNmCiIYmDNC51tKH2Y9Fmfb1/vUkdAQAAAAAAAIGJMBsw2SX9pyg6NMpQe2vb+yqvrTCpIwAAAAAAACDwEGYDJnOEROrnAy821Jxul97Y9p5JHQEAAAAAAACBhzAbCAAjkodqVMpwQ219/g9an7/BpI4AAAAAAACAwEKYDQSI6QOmyRESaai9vu0dVdY5TeoIAAAAAAAACByE2UCAiA6N0vT+FxpqFbWVenv7ByZ1BAAAAAAAAAQOwmwggIzuMkLDkgYbamv3rdOPhZtN6ggAAAAAAAAIDITZQACxWCz6+cCLFWEPN9Rf27pAzjqXSV0BAAAAAAAA5iPMBgJMXFisLu43xVArrSnT/zbNl9fnNakrAAAAAAAAwFyE2UAAGt81Q4Pi+xtqPxZt0ZKsz8xpCAAAAAAAADAZYTYQgCwWi2acMF3RIVGG+oe7PtbW4h0mdQUAAAAAAACYhzAbCFBxYbH69ZArZZHFX/PJpxc2vqrSmjITOwMAAAAAAADaH2E2EMAGJvTTBX3ONtQq6ir13I/z5PF6TOoKAAAAAAAAaH+E2UCAO6vXJA1NHGyo7SrL1Ls7F5nUEQAAAAAAAND+CLOBAGe1WPXLEy5XYni8ob5sz0qtz99gUlcAAAAAAABA+yLMBjqAyJBIXTN0huwWm6H+yuY3tN9VYFJXAAAAAAAAQPshzAY6iJ4xPXTZgAsNtWpPjf674WXVempN6goAAAAAAABoH4TZQAdyUrdxGpc62lDLde7T/K3vyOfzmdQVAAAAAAAA0PYIs4EOxGKx6OcDL1I3R6qhvmbft1qVu8akrgAAAAAAAIC2R5gNdDChtlBdM2yGwm1hhvqb295Tdvlek7oCAAAAAAAA2hZhNtABdYlM1ozB0w01t8+j//74spx1LpO6AgAAAAAAANoOYTbQQY1IGabT00421IqqS/S/TfPl9XlN6goAAAAAAABoG4TZQAc2re956hubbqj9WLRFS7I+M6UfAAAAAAAAoK0QZgMdmM1q02+GXqXokChD/cNdH2tL8XaTugIAAAAAAABaH2E20MHFhcXqN0OvlEUWf80nn57fOE/7nfkmdgYAAAAAAAC0HsJsoBMYEN9PU/qcbag561x64rv/qrSmzKSuAAAAAAAAgNZDmA10Emf2mqRhSYMNtZKaUj313XNy1blM6goAAAAAAABoHYTZQCdhtVj1qxOuVK/oNEM917lP//7hRdV66kzqDAAAAAAAADh+hNlAJxJuD9MfTvy1UiKTDPWdZZl6fuM8ebwekzoDAAAAAAAAjg9hNtDJRIdG6YYTf6fY0BhDfUPhJs3fukA+n8+kzgAAAAAAAIBjR5gNdEKJEfG6fsRvFWEPN9S/zPtaH+z62KSuAAAAAAAAgGNHmA10Ut2juuq64b9WiNVuqH+ctUzL93xhUlcAAAAAAADAsSHMBjqxfnG99eshV8kii6H+9vYP9M3+78xpCgAAAAAAADgGhNlAJ3di8hBdMehiQ80nn/636XVtLt5mUlcAAAAAAABAyxBmA0HgpG7jNKXP2Yaax+fRfzb8T1nle0zqCgAAAAAAAGg+wmwgSJzd63Sd2uMkQ63GU6u53z+vfFeBSV0BAAAAAAAAzUOYDQQJi8WiS/tP0aiU4YZ6ZZ1TT373X5XVlJvUGQAAAAAAAHB0hNlAELFarPrFCT/XwPh+hnpRdYme+v45VbmrTOoMAAAAAAAAODLCbCDIhFjtunbYL9QzuruhnlOZpye/e06uOpdJnQEAAAAAAACHR5gNBKFwe7hmnvhbJUckGuqZ5dn61/pnVF5bYVJnAAAAAAAAQNMIs4EgFR0apRtGXKOY0GhDPacyT4+ue1rF1SUmdQYAAAAAAAA0RpgNBLGkiETdPPJaxYXFGur5rkI98u3TyncVmNQZAAAAAAAAYESYDQS5VEcX/d+oPygpPMFQL6kp1SPrnlZOZZ5JnQEAAAAAAAA/IcwGoKSIBP3f6D8o1dHFUK+ordS/1v1bu8uyTeoMAAAAAAAAqEeYDUCSFBcWq/8beZ16Rnc31F3uKj3x3bPaVrLTpM4AAAAAAAAAwmwAh4gKdeimkdeqb2y6oV7jqdXc75/Tj4WbzWkMAAAAAAAAQY8wG4BBhD1CN4y4RoMTBhjqdV63ntnwkr7d/505jQEAAAAAACCoEWYDaCTUFqrfD/+VRiQPM9S9Pq9e2Piavsxda1JnAAAAAAAACFaE2QCaFGK16zdDrtS41NGGuk8+zdvylpbtWWlSZwAAAAAAAAhGhNkADstmtenqwZfp1B4nNTr39vYPtHD3J/L5fCZ0BgAAAAAAgGBDmA3giKwWqy7rP1Xn9Dq90blFuz/RvC1vqc5TZ0JnAAAAAAAACCaE2QCOymKxaErfczSt73mNzq3O+1qPrvu3SqpL278xAAAAAAAABA3CbADNdmavSbp8wEWyyGKoZ1Xs0T+/fkzbSnaa1BkAAAAAAAA6O8JsAC1ySo/x+s3QqxRqCzXUK+uceuK7/2hZ9ufM0QYAAAAAAECrI8wG0GKjUobr1tE3KDki0VD3+rx6e8eHenHTa6r11JrUHQAAAAAAADojwmwAx6RbVKpuy7hJQxMHNzr3zf7vNOfbp1RYVWRCZwAAAAAAAOiMCLMBHLPIkAj9fvgvdV76GY3O5VTm6cGvH9fGoq0mdAYAAAAAAIDOhjAbwHGxWqw6v89Zum74rxRuCzecc7mr9PT3z+ujzGXM0QYAAAAAAMBxIcwG0CqGJZ2g28bcqFRHF0PdJ58+2PWR/vPjy6pyV5vUHQAAAAAAADo6wmwAraZLZLJuHX29RiYPa3Tu+4If9dA3T2qfM9+EzgAAAAAAANDREWYDaFXh9nD9dujVurDvubLIYji335Wv//fN4/oydy1jRwAAAAAAANAihNkAWp3FYtFZvU7T9SN+K4c90nCuxlOreVve0tM/vKDSmjKTOgQAAAAAAEBHQ5gNoM0MThig28bcpB5R3Rqd21i0RfeveUTf7P+OXdoAAAAAAAA4KsJsAG0qKSJBs0fP1ISuYxqdc7mr9MLGV/XcxnmqrHWa0B0AAAAAAAA6CsJsAG0u1BaqqwZfpuuG/0oxodGNzq/P/0F/X/OwfijYaEJ3AAAAAAAA6AgIswG0m2FJJ+gv4/6o0SknNjpXUVepZza8pP9tel2uuioTugMAAAAAAEAgI8wG0K6iQhz6zdCr9JshV8kREtno/Jp93+r+tY9oc/E2E7oDAAAAAABAoCLMBmCK0V1O1F/GztbQxMGNzpXWlOnJ7/6r+VvfUbW7xoTuAAAAAAAAEGgIswGYJjYsWtcN/5WuHjxd4bbwRudX5qzWP77+l7aX7DKhOwAAAAAAAAQSwmwAprJYLBrfNUN/Gfd/GhDfr9H5wqoi/Wv9v/XSpvkqqyk3oUMAAAAAAAAEAsJsAAEhITxeN464RtMHTFOINaTR+bX71unerx7Sp9mfy+P1mNAhAAAAAAAAzESYDSBgWC1Wndpjgu4YO0t9Yns1Ol/tqdGCHR/qgbWPakvxdhM6BAAAAAAAgFkIswEEnJTIZP3fqD/o0v5Tm5ylvc+Vrye++4/+u+FlFVeXmNAhAAAAAAAA2pvd7AYAoClWi1WnpU3U6C4n6r0di/XVvm8aXbO+YIN+LNqic9JP1+S0UxRiazyeBAAAAAAAAJ1Di3dmFxUVaebMmcrIyNC4ceN0//33y+12N3ntihUrNGXKFI0YMULnnnuuli9fbjj/n//8R6eccopGjBihGTNmaNeuXf5z33//vQYNGqSRI0f6v6666qqWtgugg4sJjdaME6Zr9ujrlRbdvdH5Om+dPtj1sf6+9hH9WLjZhA4BAAAAAADQHlocZs+aNUuRkZFauXKl3nrrLa1evVovvvhio+syMzN144036uabb9Y333yjG2+8UbNmzdL+/fslSe+8845efvllPffcc1qzZo2GDBmim266ST6fT5K0YcMGjRkzRuvXr/d/zZs37/jeLYAOq09sL92WcaOuGHixHPbIRucLq4r09A8v6Onvn1e+q9CEDgEAAAAAANCWWhRmZ2Vlae3atbr11lsVERGhtLQ0zZw5s8mQ+Z133lFGRobOOOMM2e12nXfeeRozZoxef/11SdIbb7yhK6+8Uv3791dYWJhmz56t3NxcrVmzRlJ9mD106NBWeIsAOgurxaqJ3X+mu8bfqpO7j5dFlkbX/Fi0RfeveVgLtn+oitpKE7oEAAAAAABAW2jRzOzt27crLi5OXbp08df69u2r3NxclZeXKyYmxl/fsWOHBgwYYHh8v379tGXLFv/53/3ud/5zISEhSk9P15YtW/Szn/1MGzZsUFJSks466yxVVlZq7Nixuv3225Wamtrsfq1Wi6zWxmFXR2KzWQ3fAUhx9mhdPeQSnZI2TvO3vKudpZmG826fR5/u+Vwrc1ZrUs+TdFb6JEWHRpnT7HFg/QPBjc8AIHix/oHgxfoHghfrv3laFGY7nU5FREQYagePXS6XIcxu6trw8HC5XK6jnvd4PEpJSdGECRN0xRVXqK6uTvfdd5+uvfZavfPOO7LZbM3qNyHBIYulY4fZB8XERBz9IiDIxMcP1PCet2ll1lq9/P0ClVWXG87Xeuu0JPMzrdi7Wuf0O1VTBp2pmLCOF2qz/oHgxmcAELxY/0DwYv0DwYv1f2QtCrMjIyNVVVVlqB08djgchnpERISqq6sNterqav91Rzpvs9kazeH+61//qvHjx2vnzp2NdnwfTnGxs1PszI6JiVB5eZU8Hq/Z7QABaVjsUN0zoZ8+3LlEy7K/kNdnXCs17hq9t2WJPtq+Qqf1PEln9jpVUaGOwzxb4GD9A8GNzwAgeLH+geDF+geCF+tfio8/elbTojC7f//+Ki0tVWFhoZKSkiRJO3fuVGpqqqKjow3XDhgwQBs3bjTUduzY4Z+D3b9/f23fvl2nnXaaJKmurk6ZmZkaMGCA8vLy9OKLL+qmm27yh9+1tbWS6ndvN5fX65PX62vJWwxYHo9Xbndw/kUGmiNEobqo7wU6udt4Lc78VGv3rWscantq9NHuZfose5Um9ThJp/c8RY6QxjeTDDSsfyC48RkABC/WPxC8WP9A8GL9H1mLhrCkp6dr9OjReuCBB1RZWak9e/Zo7ty5uvTSSxtdO3XqVK1du1aLFi2S2+3WokWLtHbtWl144YWSpEsuuUSvvPKKtmzZopqaGj388MNKSkpSRkaG4uPjtXDhQj366KOqqalRcXGx7rnnHo0fP149e/ZsnXcOoFNKikjUjMHT9ddxt2hc6ugmbxJZ7anRR1nLdNeX/9SHu5bIVVfVxDMBAAAAAAAgkFh8Pl+Lti4XFhbq3nvv1Zo1a2S1WjVt2jTdcsststlsGjlypO655x5NnTpVkrRy5UrNmTNH2dnZ6t69u2699VadeuqpkiSfz6cXXnhB8+bNU3FxsYYNG6Z77rlHvXv3liRt2bJFDz74oH788UdJ0qRJk/SXv/xFcXFxze61oKCiJW8tINntVsXHO1RS4uRfZYBjkO8q0OLMT/X1vvXyqemPuwh7uE5LO1mndp8QUONHWP9AcOMzAAherH8geLH+geDF+peSk6OPek2Lw+yOhDAbwEH7nflanPmpvtn/3WFDbbvVrowuIzSpx0lKi+7ezh020Q/rHwhqfAYAwYv1DwQv1j8QvFj/zQuzWzQzGwA6qi6OFP1qyBU6J32yFmcu1bf7v28Uaru9bn2V942+yvtGfWPTdWqPkzQieahsVptJXQMAAAAAAOAgwmwAQSXVkaJfD7lS56RP1qLdn2h9/oYmd2rvLMvUzrJMxYXFamK3n2li93GKDo0yoWMAAAAAAABIhNkAglRXRxf9dujVyq3cpyVZn2ld/vfy+DyNriutKdOHuz/WR5lLNbrLCJ3aY4J6xaSZ0DEAAAAAAEBwI8wGENS6RaXqV0N+rov7n69VOWu0Mme1ymobz9t3+zxas+9brdn3rXrH9NSpPU7SyJRhslv5GAUAAAAAAGgPpDAAICkmNFrn9j5DZ/aapO8KftSKvau0qyyryWt3l2dr96Zsvb3jA41NHaWfpWaoW1RqO3cMAAAAAAAQXAizAeAQdqtdGV1GKKPLCGWX79Vne1fp2/3fyd3ECJKK2kp9mv25Ps3+XL2i0/SzrqM1ussIOUIiTegcAAAAAACgc7P4fL7Gdz7rJAoKGo8K6Gjsdqvi4x0qKXHK7faa3Q4QlCpqK7Uqd61W5qxWaU3ZEa+1W2wanjxEP+uaoUHx/WWz2o75dVn/QHDjMwAIXqx/IHix/oHgxfqXkpOjj3oNO7MB4CiiQ6N0TvrpOrPnqfq+cKM+27NKO8t2N3mt2+fRuvwftC7/B8WGRmts6mj9rOtopTq6tHPXAAAAAAAAnQthNgA0k81q06iU4RqVMlx5zv1ak1d/Q8jyJm4YKUlltRX6JPszfZL9mdJjetaPIUk5UZGMIQEAAAAAAGgxwmwAOAZdHV00rd95mtLnbG0u3qav9n2rDQUbm5ytLUmZ5dnKLM/Wm9ve16CE/hqZMlwnJp1AsA0AAAAAANBMhNkAcBxsVpuGJg3W0KTBcta59M3+7/RV3jfKrtjb5PUen0cbi7ZoY9EWvWaxaWBCP41KHq4Tk4cQbAMAAAAAABwBYTYAtBJHSKRO7TFBp/aYoNzKffpq3zdau2+dKmorm7ze4/NoU9FWbSraqle3vq1B8Qd2bCcPkYNgGwAAAAAAwMDi8/l8ZjfRVgoKmp5j25FwJ1OgY/N4PdpUvFVf5X2jHws3H3YMyaGsFqsGxfdXRtcTNan/WNW5xPoHghD/GwAIXqx/IHix/oHgxfqXkpOjj3oNO7MBoA3ZrDYNSzpBw5JOUJW7WhsKN2l9/gZtKt4qt9fd5GO8Pq82FW/VpuKtmrfpLfWNS9fA+AEanNBfadHdZbVY2/ldAAAAAAAAmI8wGwDaSYQ9XGNTR2ls6ihVuav1Y+Fmrc//QRuPEGx7fF5tK9mlbSW79MGuj+QIidSg+P4alFAfbseHx7XvmwAAAAAAADAJYTYAmCDCHq4xqSM1JnWkqg8E2+sKNmhT0RbVHSbYliRnnUvf5n+vb/O/lySlRqZocMIADUror/7xfRVmC22vtwAAAAAAANCuCLMBwGTh9nBlpI5UxsFgu2hL/Y7towTbkrTPla99rnwt3/uFbBab+sama1BCfw2I76ue0T1ks9ra6V0AAAAAAAC0LcJsAAgg4fZwZXQZoYwuI+RWnXLrcrQ26wdtKtyq/a6CIz7W4/NoW+lObSvdKUkKtYaoT2y6+sX1Vr+4PkqPSVOILaQ93gYAAAAAAECrI8wGgAAVbg/T6ORh6hPRR263V0VVJdpSsk2bi7dra/F2udxVR3x8rbdOW0q2a0vJdkmS3WpXekya+sf1Ub+4Puod24uxJAAAAAAAoMMgzAaADiIxIl4nRYzTSd3GyevzKrtirzYXbdfm4m3aXZ4lr897xMe7vW7tKN2tHaW7JX0qq8WqXtE91C+uj/rF9VZ6bE9FhTja580AAAAAAAC0EGE2AHRAVotV6TE9lR7TU+f2nqwqd7W2l+zU5uLt2l66U3nO/Ud9Dq/Pq93l2dpdnq1Psj+TJKVEJCk9tqd6x/RUemxPdXd0Ze42AAAAAAAICITZANAJRNjDNTx5iIYnD5EkVdRWamdZpnaU7tKOkl3aW5knn3xHfZ78qkLlVxVq7b51kqQQa4h6xfRQ75heSo/tqfSYNMWFxbbpewEAAAAAAGgKYTYAdELRoVEakTxUI5KHSpJcdVXaVZap7aW7tKN0t7Ir9h51LIkk1XnrDhlNUi8+LM4fbHeP6qoeUd0UHRrVZu8FAAAAAABAIswGgKAQGRKhoUmDNTRpsCSp2l2j3eVZ2lGyS9sPhNt13rpmPVdJTalK8ku1Pv8Hfy0mNNofbHeP6qruUV3VJTKZESUAAAAAAKDVEGYDQBAKt4dpcMIADU4YIEnyeD3KceYps6x+hnZmWbbyqwqb/XzltRUqL67Q5uJt/prdalc3Rxd1PxBw94jqqm5RXeUIiWz19wMAAAAAADo/wmwAgGxWm3pG91DP6B46RRMkSZW1TmWWZyuzPFu7y7KVWb5H1Z7qZj+n2+tWdkWOsityDPXo0Ch1jeyiVEcXdXWkHPjeRVEhDlksllZ9XwAAAAAAoPMgzAYANCkq1GEYTeL1ebXfVXAg2M7W3spc5Vbua/Z4koMqaitVUVupbaU7DXVHSKRSI40Bd6ojRbGhMYTcAAAAAACAMBsA0DxWi1VdD4TME7qNkVQfcBe4CrW3Mk97K3OVU5mnnMo8ldaUtfj5nXUu7SzbrZ1luw31cFuYukSmqIsjWV0iU5QamawujhQlRSQqxMr/GQMAAAAAIFiQAgAAjpnVYlUXR4q6OFI0usuJ/nplnVM5FXnKqczV3gMBd55zvzw+T4tfo9pTo6yKPcqq2GOoW2RRUkSCP+hOjUyp/zkyWY6QSHZzAwAAAADQyRBmAwBaXVSIQwMT+mlgQj9/zeP1qLCqSHmufO1z7leec7/2OfO135WvOq+7xa/hk08FVUUqqCrSj0WbDefCbeFKikhQUkSCEiMSlBSe6D9OCI+XnR3dAAAAAAB0OPx/8wCAdmGz2vy7uJU81F/3+rwqqirRPtdPAXeec7/2ufJV66k9pteq9lRrb2Wu9lbmNjpnkUVxYbEHwu1EJYYnKDEiXgnh8UoMj1dsWIysFusxv08AAAAAANA2CLMBAKayWqxKjkxUcmSihiWd4K97fV6VVJcp31Wgfa78A98LtN+Zr7La8mN+PZ98KqkpVUlNqbaX7mqyn/iwWMWHxykhvD7kTgg7+HOc4sPjFWoLOebXBwAAAAAAx4YwGwAQkKwWqxIj4pUYEa/BiQMM56rc1fXhtjNf+10F2u/K1z5XgQpchcc0l/tQXp9XRdUlKqoukbS7yWuiQhxKCI9TbFis4sJiFRcWc+B7/c+xYbGKsIcfVx8AAAAAAMCIMBsA0OFE2MPVKyZNvWLSDHWP16OSmjIVVhWpqKpYhdXFKqwqUmFVsYqqiuV0u1rl9SvrnKqsc0oVOYe9JtwWdiDsrg+6Y8NiFBsWo5jQaMNXuD2sVXoCAAAAAKCzI8wGAHQaNqvNf6PHplS5q1RYVaKiqqIDQXd92F1cXaLi6pJjuhHl4VR7alTtqr/B5ZGE2kIPCbejjGF3WLSiQ6MUHVL/nfEmAAAAAIBgRpgNAAgaEfYIpUVHKC26W6NzPp9PlXXOA8F2qT/gLq4uVcmB7621s/tQtZ7aA7vHi456bbgtrD7cPhB8R4dGKzrEYTwOdSgqJEoR9nBZLJZW7xcAAAAAALMQZgMAIMlisRwIiqMajS85qNpd7Q+6y2rKVVJTprKaMpXWlKu0pkylNWVyuavarMdqT42qq2pU0Izg22qxyhESqagQh6JCHHKEOBQVEqmo0KgDxw3OhToUZgtts94BAAAAADhehNkAADRTuD1c3aJS1S0q9bDX1HpqVVpTrrKasgNh909Bd3ltpcprK1ReU65ab12b9ur1eVVRW6mK2spmPybEGqKoEIeiQw+G31H+n6MPBN4Hf3aEOhRpj5DVYm3DdwEAAAAAwE8IswEAaEWhtlClRCYpJTLpiNdVu2vqg+1DvipqKn4KvA/Waivl8Xnapfc6b51KakpVUlParOstsigyJMK/u/vgbu9DvzesR9ojZLPa2vaNAAAAAAA6JcJsAABMEG4PU7g97Kiht8/nU5W7SuUHdllX1FX6Q+6K2gpV1DpVUVtx4HxFm+/4NvQmn5x1LjnrXJIKmv24CHu4Iu2RB8Lun76OVIuwh7MLHAAAAACCHGE2AAABzGKxKDIkUpEhkUp1pBz1+hpPrZx1TlXWOeWsdany4M91TlXWuQ7Unf56ZZ1TXp+3Hd7JT6rc1apyV6uourjZj7HIUh+Ch0TKYY9UZEjEIWF3xGHq7AQHAAAAgM6EMBsAgE4kzBaqMFuoEsLjm3W9z+dTtadaFQcD7trKA6F3pSoPDb1rnaqsq1RFnVO1nto2fhdN9CmfXO4qudxVKtTRb4B5qHBbuBwNQm7HgX8gcNgj5AhxEIIDAAAAQAdAmA0AQBCzWCyKsEcowh6hFB155MlBtZ46/05v54Fd386Du74P+X5ovcaEAPygak+1qj3VKqouadHjwm1h/sA78kDAHRnyU9h9aCAeechYlFBbSBu9EwAAAAAIboTZAACgRUJtIQq1xSk+PK7Zj6nzuuU6MF/bWeeSy/3Tz00fV8lZ51Sd1912b+Qoqj01qvbUqFgtC8HtVvshAfjBXeARB8agRB7YJe74KRA/cE2YLVQWi6WN3g0AAAAAdHyE2QAAoM2FWO2KDYtRbFhMix5X66nzB92uOpec7ip/KF4feDdVd5m6E9ztdaustkJltRUtepzNYvOH3o6QSEWFOpQQFasQX6gcdoccIQ5FhzgUFepQVEj9V5gtjAAcAAAAQNAgzAYAAAGrfhd4rOLCYlv0uPqd4FWGHd/1oXfD4/od4AevNTME9/g8qqitVEVt5U/FgiM/xm61KyrEIUdIpKJDohQV6jjwPUoxoVGKDo1SVMhPP4faQtv2TQAAAABAGyLMBgAAnU79TvBoxYZFt+hxbq+7/kaTdS45DwnDm9oV7qqrktNdf67KXS2ffG30bo7cb2lNmUprypp1fagtVDEhB0Lu0ChFHwi663921H8/8OWwR3ITTAAAAAABhTAbAADgALvVrpjQaMWEtiwE9/q8qnJXHzL/u0pVTY5FcRpDcneVvD5vG72bxmo9tSr0FKuwuvio11pkUWRIhKIPht8hDmMIHhZ94HcVpZjQaHZ9AwAAAGhzhNkAAADHyWqxyhFSP+u6JXw+n6o91fUB94ExKAfD7ypPleqstSqqKFV5TaUq65z+L3c73BjTJ59/JMs+V/5Rrw+zhfr/ISA6NNr/c0zoT8F3bFiMokOi2PENAAAA4JgQZgMAAJjEYrEowh6hCHuEFJFgOGe3WxUf71BJiVNu90+7t30+n2o8Naqsc6myrlKVtT+F3JW1zvq523WVqqitUEWtUxW1FXL7PG3+Xmo8tSqoKlJBVdERr7PIoqhQh+JCYxQTFqPY0JgDI2EO/hxD6A0AAACgSYTZAAAAHYjFYlG4PVzh9nAlNQjAm3Jw93f9zSXrw+36sPunr8q6n0JwZ52rTfv3yffTjS4rcw97nT/0DotVbGiM4sJi6n8Oi1FsWKziDoTeDnukLBZLm/YMAAAAIDAQZgMAAHRih+7+TolMPur1Hq9HTrerUdBd6d/xXX9cXluh8toK1Xnr2qTvQ0PvPco57HV2q92/o/tg4B0fHqf4sDjFh8cqPixO0aFRslqsbdInAAD4/+3dd5wkdZ038E+Frs5x8mzOC8iyCwso4qMS5NAFQVB84JT1QU8JcqvA4omeAUHQFe5Bz3DgCYc+ShKRcIJnQFBJkgQFNgAbJufpXOn5o0J3T3fPzvTM7PTMft4v26r6/ap/VT1L18x8+jffIiI6cBhmz0MDI1l09k9uVpUJE/b/YJomDBPWtrMOE6YJGKYJwF6a1i/IoihAFADRWRcFa11A6ba9dBRPohIEAe6mULRwdrKPa5+KtbTP1X0NhVWY9vmWvEaz6LW6gxXGrPg1qfD8YkLx6xYEd1so+pqUrNv7VFJ1TlnxuII1U01wvrYC3DGdryFnpxER0VRIojThm2A6JU+sYLsQcI/mSredx0zc7FIzNPRnB9A/zk0tJUGyQ+6oHXLHELdD75g3hoQvhoDs5/dQIiIiIqI6xzB7nnngT2/g53/YNdunQbPICbSdwNsJukU7CC9uKw7JgUJg73xY4Xy44X4QYK8XfbRQw/mVnlN5WF8e0Feyv6OLQnHoL5R88OCul+0z5lyE0nMta4d1wvbLsF6dUPggxn09RfvBHsfepfAax7ZDgCQJCPgV5HLWrEdJFN0PiSTR+qBEKvoQSSr64Kjk3734dWDs6yr9GkjFYxUtnXGlMcdyxiGig1NxyZP9zfo2TAMpNY3h3AiG8yMYzo1iODeCkfyI3eZsj0Kf5hrfuqnvN/BWJAVxO9h2gu64L464N4qEHXorkmdaz4uIiIiIiCaHYfY888s/vjHbp0CzzAmgYQK6UVvgTDQZzgcATsBe8pcZxX+9IQoQRRFyUSAuSVYgL0mite30SYX9ij84EMd8aCAIxR8oFP46QZIEdzxnLPcYdp8sioX9JBGyZLXJcmFdkgR4JLHQL4mQJatdZIhPNCmiICKshBBWQliI9qr7jQ29h3LDGM6NYChnh965YQzlR5DMp8r+imoq8noe3ekedKd7qu4T8gStsNsXR6MvgUZ/Axr9CTT6E0j44pBF/mhNRERERDST+BP3PBMNKugfyc72aRDRQcQ0Ad00D7oPT5xg3Am7PZJgL8Vx22U7GLdC9qJ1p08s3ZackF0q31cSC+PJRcfzyNb+nDVPc9FEQ2/d0DGSHy2E3fkRDGWHMZgbwmB2GEO5IQzmhqe1tElSTSGpprB7tLyGtwABMW8UTXbA3VAUdDf6G3ijSiIiIiKiacAwe5658Iy34J5Hd6KjP1W9BnMVhdIJpWUUBLuGgmgPOLYshWGaMAyzaIkx2yYMo7Cfo6SGdWkZ6/2fq/1/7vm5r8HtddvLa3ELY7ZRsn+FI1XZss7dMJza4oX1gyvSIzo46YYV4OfV6a8BPB0EAB5ZdB+KLBWti/B4JHgkEYrH2cfaLt5Hdp5f1O5sKx4JikeCVxahKBK8sgSPR+SMdTpgJFGyS4HEqu5jmAZG8qMYtEPuoawVcA9mhzBgb4/kk9Myw9uEaQXpuSG8NrSzrN8n+dDkT6Ax0OgG3k1+az3qjfAGlUREREREE8Awe55Z3h7BFf97w2yfxrQwi276ONdubOiE22ZRsF98A83Cfvsfy61fbd+M061bbZowMGbbWRpObevCcZ1zKXu+fX5u6Ybikg6AVfsZKKmNLLqFoat9Aao1F45dfA4VX1/RvmUfKow5dvmHDIXxnNdc/GGDMeZrVfZ1NUvP0xh7bkX7AHDP13od9rk7r8veofhGqsWlYNy65Pb+zn8jzsuWZQm5nAZdN6A752sUzlsv/uDIXtcN0x2r4n83Y77OxR82HWyzq+czE0BeM5DXDmzYrshW0O312IG3XFj3yEXLooDdCtQlK2Qfs49PkeFTJPthrYvi3Pl+QLNLFETEvFHEvFEsw+KK+2iGhuHcCAayQ/as7kLQPWCH3xktM+VzyepZ7El2YE+yo6xPFmW3bEmTvwGNAWvZ5G9Agy8BSZSmfHwiIiIiovmAYTbVLSc4nYuRhSBYdYABAPz9k2okyyLi8SAGB1PQDmAgWRxsFwfkTmBe3O5sV/wrDecvNczS5zrP1wyj0KY7fUbJfsXBPIDChwyA1Y4xHwoUHU/TjbKxdd06bqHNhK4b0IraNWdbr88Z1/XOCdCTU8/+qvJ6pELA7ZXhLwq6fd7y8Lts3d7Hr0gsx0KQRRkN/gQa/Imq+2S1LAZzw1a4nR1Ef3YQ/ZkB9Gb60Z8ZQEpLT+kcNENDV7oHXRXqdYuCiAZfHE2BRjT5G9Hsb0RTwFomfDEG3URERER0UGGYTUREJURBgCgJkA/yfMQ0C0G7ZlgBtxV8G1DtdVW3AnBraUDVrNC8OBDXNAOaYdrLQntxcK4XtbttzviGWXLskv0146Asa5RTdeRUHcOpqY8liYIVbHtl+BQZAa8dkDsPO/wOOAG421dYD3hlyBJLRMxnPtmHNtmHtmBLxf60mkF/thBu92X60WcvB3JDU6rbbZgGejP96M30A3i1pE8SJDT442Uhd3OgEXFfjKVLiIiIiGjeYZhNRERUgSAI9o0VAW8d/4mFMwvdmV2uaQZU+5HXDKia7q7n7XVVM5BXrbBc1XTkVTukL3quWnFbt8ZUC/1znW6YSGU1pLLalMbxyGJJuB3wFgffdptPRtDvQcjvQdDnQdAvI+jzIOCTWWt8jgt4/Ah4FmBReEFZn27oGMwNoTfTjz47lO5L97vbeUOt+bi6qaMn3YeedB9eHtMnCxIai8JtN+wONCKqRPgXCUREREQ0JzHMJiIimsNEUYAiSlA8B/7YhmEir+nIqQbyqo68WljPqVbw7aznVB2qWhqq59XSsF0d05fXdGTz+pwIza3QP4+RVH7SzxUAN+gO+uyw2y8jHFDQGA9AhAm/IiHg8yBkh9/WvpwRPhdIooRGfwMa/Q1lfaZpYiQ/6gbbTtjtBN5TKV+imTq6Ut3oSnWX9SmS4obbLf5GtASb0RpoRkuwGV5JqfmYREREREQzjWE2ERER1UQUBbsW9cweR9MN5FQdmZyGbN4KuLP2eiavlWxbj3HWc3rdlWYxgaLZ4ZMrNu71SAj6ZQS8HoTsmd7hgAfhgIJI0H4EPIgEFYQDCoI+mTNy64ggCIh6I4h6I1gZW1bWn1LT6M1YM697033oyfShN92PnkzflG5Kmdfz2JvswN4KN6OMe2NoLQq3WwPNaA02I6yEaj4eEREREdF0YZhNREREdU2WRMiSiKBv6tPPTdNEXjVKQu5MTrNC8ZwVjmdyGjJF69mcvY+9X8be1o3Zj8WdWe8DyE1of0kUEA54EAkoCBcF3VborSAacpZehP0eiCKD79kU9AQQ9CzG0sjiknbTNEuD7pJlP7J6tuZjDuaGMJgbwt8HXis7l5ZAIdxuDbagNdCMuC/K2txEREREdMAwzCYiIqKDhiAI8CoSvIqE6BTGMU0Tec1wQ+50rigEz2lIZ8e2a/bsaxXJjIpURoWmH/gwXDdMDCXzGEruvxyKIMCa4R1QEA16EAl6EbWDb2fpPBh8H1iCICCkBBFSglgWXVLSZ5omRtWkW0vbCrh77fV+qDXW6E6paewafgO7ht8oaVckBa2BJrQEWtAWLATdjb4EJLF+7zdARERERHMTw2wiIiKiSRIEAV6PBK9HQizknfTznTA8lVGRympuwJ3KWtupjIp0TkNeNzE0krX67b5cXp+BV1TpHIGRlFUHfG/v+PsKAhD2e0oC7mjxjO+gNds7FlIQ8ntY6mQGCYKAiBJGRAmXlS4xTAPDuREr6M702oF3L7pSPejPDsKsoQhPXs9j9+g+7B7dV9IuCxKaAo3uDO62YDPagq1oCTQx5CYiIiKimjHMJiIiIjrAisPwRKTyPrIsIh4PYnAwBa3oJpiabiBtz/J2g++shmTWCsRH0ipGU3kMp/MYTeUxklaRyWkz+npMExhJW8dGb2rcfWVJQDToRSysIBby2g8F8bC3aNsLv1di6D3NREFE3BdD3BfDGqws6cvrqhVsp3vQnepBV7oHXake9GT6oBmT/+9HM3V0prrROeYGlJIgoSXQhLZgC9pDrWgLtqAt2IpGf4LlSoiIiIhovxhmExEREc0hsiS6s58nStV0jKZVjKTz9mzronV7OWzPwk6m1Rm9Saamm+gfyaJ/ZPy6zopHRDzsQyLsRUPEh0TEi4SzDFtLn8IfZaeLInmwMNyOheH2knbDNNCXGUC3HW47YXdnqqem2ty6qaMj1YWOVBf+0vOC2+4RPe7sbSfobg+2IuaN8kMNIiIiInLxNwAiIiKiec4jS0hEJCQivv3uqxuGFXynSkPukmV65oPvvGqgeyCN7oF01X2CPhnxsA8NRUF3Q8SHhqgPDREfYiEva3lPkSiIaA40ojnQiMMbD3XbTdPEcH7ECrhTPehMd9shdzeS6viz8ytRDbViuRK/7Ed7sAXtobaiZSsCHv+UXxsRERERzT0Ms4mIiIjIJYmiW+pjf3TDQDKtVgy6C2G4NQt8NJWf9uDbuqlmEnt7k1Vei4B4uDTgLl164ZFZv7kWgiAg5o0i5o1ibWJVSV8yn7LLlHRbQXeqG13pHgzlhid9nIyWwc7hN7BzzI0nY94o2oOt7gzu9lAbWoPN8Ij89YaIiIhoPuNPe0RERERUE0kUEQ15EZ1g8D2SUjE4msNQsugxmi/aziOZUaft/HTDRN9wFn3DWWBP5X0iQQWNUZ/98KMx5kNT1I/GqA+JiA8emXWcJyukBLFSWVZ2A8q0mkZHqhudqS50JO1lqgsptfrs+2qGcsMYyg3jbwOvum2iIKLZ34gFoTa0h1qtZbANCV+MpUqIiIiI5gmG2UREREQ04yRRRDzsRTw8fvCtajqGknkMjuYwMJrF4EgO/SNZDIzkMDCSxcBobloDb2cG+a6OkbI+AUAs7C0E3VGfG3Y3xfyIh1nGZDICngBWxkpDbtM0Maom0ZHssm8YWQi6s3puUuMbpmHNCE/3lNTj9kk+N9xeEGpFe9AKu/3y/svuEBEREVF9YZhNRERERHXDI0toillhcTU5VXeD7YFha2kF3ln0D2fRP5KDphtTPhcTwOBoDoOjOWzfW14iQxIFNEZ97vlaj8K238sftfdHEARElDAiiXBJuRLTNDGQHUJHqhMdSWsGd0eyC93pXuimPqljZPUsdg2/gV1jSpUkfHEsCLViQbAwk7vJ3whJZOkZIiIionrFn7CJiIiIaE7xeiS0NQTR1hCs2G+aJkbSqh1sZysu0zltyuehGya6BzPoHsxU7A/5PSXhdnPcj5Z4AM1xP6JBhaUvxiEIAhr8cTT44yU3ntQMDT3pPnQkO7Ev1WXP5O5Cf3Zw0scYyA5iIDuIv/b93W2TRRltgWa0h9rsmdxW0B1RwtPyuoiIiIhoahhmExEREdG8IggCokEF0aCC5e2Rivtkchr6R6x62v3DWfQOZazlsLVMZacediczKpIZFa93jpb1eT0SmuN+99ESD6Al7kdzPIBoSIHIoLsiWZStmz6GWrGxqD2jZdGZ6rZC7mQX9iU70ZHqQkar/EFDNZqhYU+yA3uSHSXtYU+oqA63dfy2YAsUSZmGV0VEREREE8Uwm4iIiIgOOn6vjIVNISxsClXsT2c19A1n3BtI9g056xn0DmWRUydX6mKsnKpjT08Se3qSZX2KLKIp7kdrPIDWhgDaGgJoawiiNRFg6ZIq/LIPy6NLsDy6xG0zTRNDuWEr2E52YZ9dsqQr3QPDnFwZmlE1iVcHd+DVwR1umwABTf4GK1wPtqI91Ib2YAuaAo0QBd44lIiIiGgm8KdhIiIiIqIxAj4Zi31hLG4pLy9hmiZGMyp6hzL2w5rZ3WdvD4zkYE7h2HnNwL7eFPb1psr6YiHFDbbdoDsRRDzi5WzuMQRBQNwXQ9wXw1saD3HbVUNDd6rHnb3dYc/kHs6X3wR0PCZM9GT60JPpw/O9L7ntHlFGa7DFncG9INiGtlALokqEpWWIiIiIpohhNhERERHRJAiCgEhAQSSgYEV7tKxf1QwMjGTdsLtnKIOeQfsxlIGq1X5zyqFkHkPJPP7+ZmmNaMUjWgG387BD7paEHz6FP/IX84gyFobbsTDcXtKeVFNusO3W5E52IW+okxpfNTTsGd2HPaP7StoDsh9tdsDdHmxBW7AFbaFWhDyVa78TERERUTn+ZEtERERENI08soiWRAAtiUBZn2GaGBrNoXswg57BtL201nsGM8jXGHTnVQO7u5PY3V1etiQe9pYF3a2JABoiPogiZwo7Qp4gVsdXYHV8hdtmmAb6Mv1uyL0v1YWOZCf6MgMwJzn/Pq1lsHP4dewcfr2kPaKE0R5stcNta0b3wmgb4mDITURERDSWYJrmVP4Ksq719pbfbGeukWUR8XgQg4MpaFOYxUNEcw/f/0QHN14DDj6GaWI4mUfPYBpdA9ajsz+Nrv40eoczmO6f2mVJRGvCj/bGINobgmhvDGJBUxBNMT9kiTWfx5PT8+hKdWNfsgudTqmSVCdG8+UfJtSqMZBAi78JLYFmtAVb0RZsRmuwGX7ZP23HIKL6w+//RAcvvv+BpqbyEn9jcWY2EREREVEdEAUB8bAX8bAXaxbHS/pUzUDPoBVudw5YAXfXQAqd/Wlk87XdjFLTDeztTWHvmNrckiigtSGA9oYgFjRaIXd7YxDNcYbcDq+kYElkEZZEFpW0j+aT6Eh22bW4O9GR6kZHqgt5PT/pY/SlB9CXHsDL/a+WtMe8UbQFW9AabEZbwJrN3RpoRsBT/pcARERERPMNZ2bXOX4qQ3Tw4vuf6ODGawBNhGmaGErmrZnc/Skr6LbD7v7h7JRuRDmWJApoTQTccNua0W2VU2HIXZ1hGhjIDrkzuDvtgLs71QPNrO2DiEqiShgtQSvYbgk2oTVgzeTmjSeJ5hZ+/yc6ePH9z5nZRERERETzmlA0m/uQJaWzufOqjp7BjFWuxJ3NbT0yOW3Sx9INE/v6UtjXVzqTWxQENMftciWNAbdkSWsiAMUjTen1zQeiIKLRn0CjP4HDGw9123VDR2+m3w23O5Nd6Eh1ozfTB8Oc/C+ww/lRDOdH8drgjpJ2n+RFS1HA3RJsRmugGU3+Bkgi/32IiIhobuHM7DrHT2WIDl58/xMd3HgNoJlimiZG0qo1k7s/jQ47oO7oS2E4NflyGNUIABpjvpJ63AsaQ2hrYMg9HtXQ0J/rwwiGsKN7N/aNdqEz1VNzyF2NKIho8jegOdCIJn8jmvwNaLLXE74YRIGz7YlmA7//Ex28+P7nzGwiIiIiIhpDEAREgwqiQaWsNncyo6KjL4WO/hQ6eu2Quz+F4eTkQ24TQO9QFr1DWbyws7/o+EBzPICFTsDdFMIC1uR2eUQZC8PtiMdX4dDwoe4vs5qhoSfdh85UN7pS3ehM96Az1Y2edG9NIbdhGuhO96I73VvWJwkSGvxxNPutcLsx0OCuJ3wxzugmIiKiWcMwm4iIiIiIAAAhvwerF8WwelGspD2ZUdHZX5jB3dmXQkd/GoOjuUkfwzSB7oE0ugfS+MtrhSBVlgS0JoJY2GSF3O2N1g0oG2N+iKz5DFmU0R5qRXuotaTdKlfSh85UD7pSPehO96Ar3YPuVA/yhlrTsXRTR0+6Dz3pvrI+URDR4Itbs7kDDSWzuht8ccgif8UkIiKimcOfNIiIiIiIaFwhvwerFsawamGspD2d1dA54ATcaWtGd18KfcPZSR9D003s7U1ib2+ypF2RRbTZwfaCxkLJkoaIjzc2BCCJElqDLWgNtpS0G6aB4dwIulJWuO0E3F3pHozmk1VG2z/DNNCb6Udvph8YKO0TICDhixeVLGlAk78Bjf4GNPgT8EpKzcclIiIiAhhmExERERFRjQI+GSvao1jRHi1pz+V1dA2U1uPe25usKeTOawbe7BrFm12l98PxKhLaGwoB90K7ZEkspDDkhjWDOu6LIe6L4ZCG1SV9aTWN7nQvutK96Mv0ozfdh95MH3rS/cjqk/83cpgw0Z8dQH92AK8Mbi/rD3mCSPhiSPgSaPDFkfDF0eC3lglfHH7ZV/OxiYiI6ODAMJuIiIiIiKaVV5GwpDWMJa2lN/HJ5jV09KWxrzeJfX0p7OtNYm9fbTW5c3kdr3eO4PXOkZL2oE/GwqYQFjaFsKA5iEVNISxoCsKn8FcfR8ATwLLoEiyLLilpN00TSTVlzby2A25rvR89mT5ktMyUjptUU0iqKewe3Vf5vGS/FXD74oj7Yoh5o4WlN4qoN8IyJkRERAc5/iRAREREREQHhE+Rsbw9guXtkZJ258aTTri9r8cKu1NZbdLHSGU1vLpnCK/uGSppb4z6rJC7OYSFTUEsbAqhJeGHJPKmkw5BEBBWQggrISyvEHSn1LRdYqQQcvfZ60k1NeXjp7UM0skM9iY7qu4TVkKIe6OIe2OI+aKIeaNu2B3xRhBRwvBJXs7OJyIimqcYZhMRERER0ayqdONJ0zQxksrbM7gL5Ur29aWQyU0+5O4bzqJvOIvndxRuaihLItoaAljQGCypy90U80MUGYYWEwQBISWIkBLEsujisv60mrFKlhQF3b2ZPvRk+qZUo3us0XwSo/lk1dndAOARZUSUMMJK2F6GEFHCiNjLcFGbV2JZGiIiormEYTYREREREdUdQRAQDXkRDXlx6NKE226aJgZHc26wXRx051R9UsfQdAN7epLY01MatnpkEW2JANqbgoW63E1BNEUZclcT8Pix2LMQiyMLy/ryeh4D2SEMZAfRnx3EgP3ozwxiIDuA4fxohRFrpxoa+u1j7Y9HlBHyhKyg3hNEyBNC2FlXxmx7QvDLvPEoERHRbGKYTUREREREc4YgCEhEfEhEfHjL8ga33TBN9A1nsa8nib29SezptcqWdA2kYZqTO4aqGdjdk8TuSiF3QwDt9gzu9gbr5pOcyT0+RVLQGmxGa7C5Yr+qqxjMDWEgO4T+7AAGMoMYzA1jyH4MZoeQN9QZOTfV0DCYG8JgbmhC+4uCiKAcQNATQMBjLcu27baAJ4CQ3a6IHobgRERE04BhNhERERERzXmiIKA55kdzzI8Nq5vcdlXT0dGXxt7eJPb1prCn1wq7a7nppKoZ2N2dxO7u0pDbKVfS3miF21bIHUBznDW5J8IjedAcaEJzoKliv2mayGiZQsCdHbYDaGvdCb2zem7Gz9UwDYyqSYyqkyudIgkS/LIPAY8fftmPgP3wewrrxdt+2Qef5IVX9sIn+VgOhYiIyMYwm4iIiIiI5i2PLGFJaxhLWsMl7cmMin291o0m9/Wl0GkvR9OTnwFcrVyJLAloTVghd5s9i7utIYCWeAAemSH3RAmCgIA9w3lBqK3qfnk9j5F8EiP5UYzmRzGSHy3aTmIkV2ifqZne1eimjqSaqvlGmQIEeCUFXskLX1HQ7XcDby8USYEieuCRPFBEBR5Jtpceq130QJGcpQKP6IFHlOERZciizLCciIjmBIbZRERERER00An5PVizOI41i+Ml7SPpPDrG3HCyoy+FZKaWkNvE3t4U9vaWBpiiIKAp7kd7Q8ANuNsbg2hLBOFVpCm9roOZIilo9CfQ6E/sd9+slsNIftQKmPNJN2hO5q3lqJp015P55AEPv8cyYSKr55DVcxjOj8zIMWRBgix6IIsSPCVLK+x2Qm9REN2H5KzDXop2G0r3kUQJkiAVloIEeUybLEoQBQmy3eYczzovufRh70NERAcfhtlERERERES2SEBBZImCtUvGhNypvBtsu4/+2mZyG6aJ7oE0ugfSeG57X0lfQ8SLBU0hLF0QRSKooCnmR2sigFiIZSamk0/2wid70YzGCe2f1/MYzaeQUlNIaWmk1DTSqrWsvp2BiUkWbJ9FmqlD03VgcvdRnTUChKKAW7Jnmjuzza0Z6u660+7OUrfarfDdCdUL66IbwBf3W9vFwX4hXLfaGLATEc08htlERERERET7EQkqiAQVHDI25E7n0TlmFndHXwojNYTcANA/kkP/SA4v7uwvafcpEloSAbQlAmhNBNBqlytpTQQ4m/sAUCQFDX4FDf74/ne2GaaBrJZFWssgrWaspZZBpmh97LazntWy0Mw5kirPEhMmVEOFOsuz5os5AbunZDa7B4odpBeXeimUe5GLysN44PUoiA4Gkc1oEEzBDdJFoShcF8vXRUGCKAgQIEAURAiCABH2UhAgQLSWgggRgt3OckdENPcwzCYiIiIiIqpRJKAgslgpK1cyms6jsz/t1uPu6E+hsz+NwdHablKYzet4s2sUb3aNlvXFw14r4E4E0BL3o9leNsX8kCWGVbNFFES31jf8k3++ZmjI6jnkNKu8SNZe5tz1bKFPz0HVVeT1PFRDRV5XkbeDXqtNQ17PI2+o0Axt+l8sAajPgH08oh2Iy6IEWbBmljtlXSS7TRYlSCXlXgrlZ5wZ7rJolajxFAX4hdnyVrCvSM5zitpEj7svg3UimiiG2URERERERNMsHFAQDihYvShW0p7OaugcSKGzL20F3HbQ3TeUrbkgxeBoDoOjOfz9zcGSdkEAGqM+NMetcLslHkBLwlo2RH0MuuucLMoIiTJCnuC0jmuYhhtua4YGzdChGio0U4NmaFB1DZqpQTXsbXvpPAzTgGEa0O1l8cNq0+1t093WTR2aYS11Z2ka0A0NumlAMzXohuH2a6buHm8ulWqZa5x/t3oI352a7R5JdoN1SXBKuzjlX4prrZeWgHHqswt2zXbJmZ1eVONdhODOYHdrvo8pJ1NcYkas0l66XZgZX3JO9vNZHopo+jHMJiIiIiIiOkACPhkr2qNY0R4tac+rOroGrIC7eyCD/tEcdneNoKs/jbxm1HQs0wR6h7LoHcri5ddL+yRRQEPUh6aY33740BQtrAd8nlpfItU5URDhlRR4JWW2T2VCxobbJeG6aYXxVrsK1dCg6mrF9byh2tuaO3vaCdadIL4QthuF9jHhu2a30/RyarZn51l1HVmQ4Jf98Ht8CMgB+GUfArIffo/fWjrbsh8Bu00WZfe/e/cDH0Mr+VDIWmrutggBiqRAsd/b7lIsb+MseJrrGGYTERERERHNMsUjYXFLGItbwpBlEfF4EIODKeRVHYMjOXQNpK1Hfxpdg9ZyYKT22dy6YaJnMIOewUzF/qBPRmPUDrntwLvRXk+EffDIDEPowJBECRKkugrfDTtcHDt7fewsds3UkHcDdStMz9thulP+xQnc3bDdVAHBRE5ToRk6jJIw3VkvbHPmen3TTB2jahKjanK2T8UlizK8oh1qC4AIAbDrqAuwZpI764K1Ye0jwJrZDqGkTnvJ7PcKj+JZ7sXbhZnvznZhJr5X9sIneeGVvPDJPmspWTfu9UpeeER5zs96N00T+/btxQsvPI8XXngOL7zwHIaGBvHss3+Z7VOrewyziYiIiIiI6pQoWDOoG6I+HLYsUdKXV3V0D2bskDuFnsEMugcz6B5MY7TGG1A6UlkNqewo3uwur9EtAIiGFDREfWiM+tEQ8aHRPsfGqA8NER8UD29KSfOXKIjuLNjpVvxhljaBv8pwZo8bMGHa5V1MmDBMo7A0zbJ20zRKZvlq9qxzZ/a5Zmj27GCtaL9CaG8F92rRulYyQ97ZzutqUdCv8samdcD5t5nLrL8wsQNv2QtJEN3yRmZRGSQTJnRTt98DVr9hWu8XAXDL0jihvHPjVCtct26Y6pSvkexj+mWf/fBXWHdm2/vcEN758Gvfvr148YXn8dcXX8TLL/0Vf3vxJQwNDpa9tqe3P4flDSsP/Bd1DmGYTURERERENAcpHgmLmkNY1Bwq60tnNXQPptE9mEbPgBVy9wym0T2YQTIztaDbBDCUzGMomcfOfSMV94kEPGiI+q0gPuJFPORFPOJDPORFLKwgFvKyZjfRNBAFEeIcei8ZdojuzFYvhONqSfjtlnpxym2YeqGuelFtdWNMWRjDNGHAgGEY1rKkrntp0Gk4pWXGlJUpbjfGzIgvrhdPs8cwDWS0DDJaBqjtvsoH1LP/+Rh2PvLyhPa97EdfxEknnoxPHr6ZJWGqYJhNREREREQ0zwR8Mpa1RbCsLVLWl8qq6B6wwu3e4Sx6hzLoG8qgdyiLgdEszGmoWjCSVjGSVvF6Z+WwG7AC71jYi0TYh1jYi3hIsZZhL2Ih6xH0zf0/JSeiAmtWuwhFmtt1+Z2ZvnpJ0F644alTe101VGS0LNJaBhk1g7RmPTJaFmk1Xehz2tUMNFOHbN/YsvhmmLIoQRJlq8/tt5a6aSCv55HX88g5SyMPVVdZimYG5ZNZDO8ewPDuAeRGMzjsg0dX3C/SHp/wmIOv9+KlvlfQne5FW7Bluk51XmGYTUREREREdBAJ+jxY3u7B8vbyoFvTDfSPZNE3ZIXc7mM4i97BDNK56fvTdCfw3t1dvZasLAmIBhVEQ15Eg9aMbmvbaouFFESDXkSCHkgiZ7AR0YEhCIIVKGN6SyqZ9qeJ0/Uhnmma1g1QnZDbKATezuxy0y5BYwIwnTaYdru9Za+XznQvnv1ulvdVmAVfaCud+V68v2qoyOo55LSctdRzyGq5WQ3ldVVHsnMIw7sHMLS7H8O7+zG8ewCZgZS7jyCJOOTMIyHK5f9NxJc3Vh3bE1QQX9ZkPVY0oXF1K7ySgoAcmJHXMh8wzCYiIiIiIiIAgCyJaIkH0BKv/Et0OqtZYfdwBn3DWfTbj74RaznVEiZjabqJ/pEc+kfG/ztyAUAo4EEkoCAc8CASVKz1oIKI026vhwMKfIrEGd9EVHem+7okCAIUyQNF8iCE4LSOfSBZobwVcme1QsCd063A2zCN0htPYuyNKQUIECGJIgS7RjaAkiDehIF0Oo2dr+3AGzt34c1dr+PNna9j96430bm3A4Y+fr13Uzdg9KjwLfQhb5R+L4wuaQAEAR6/xwqtlzehcUUzGle0ItqagCzJ7iz8lnAD3r3gHYh6wzP29ZzrGGYTERERERHRhAR8MgK+ynW6ASCb16yAeySLvmHrMTiaw+BoDkOjOQwmc1AncFO7yTIBjKbVCd/4UpFFhAMehPwKQn4ZoYCCkM+DoF9GOKAg6JcR8nsQ9hfWvR4G4EREs8EK5a2brkaUqYW8481+/93zv8GF52yueeyTvMfhg+/6MHRDR0bLIqfnIYkiJEHCFU9ehEULl0AWJfdmk2NN9gawByuG2URERERERDQtfIqMBU0hLGiqHHabpolUVisE3MlcxfXpnuE9Vl4zJjTju5gsiQj6ZQR9HgS8shXse2X47WXAXVbu5w0viYgODNM00dPTjddf34Vdu3aWLF9/fRd+9avfYs2atWXPW716zaSPpSgKVq9ei0MPPQwLFiwEAEiihJASLJkNH166qvYXRCUYZhMREREREdEBIQgCQn4PQn5P1dndgFW7eySVx1Ayj+FkDkMpe2lvD6fy1iOZhzEdd6ycAE03MJy0jlkLRRbhLw657XW/t3zb75XhVyT4FBk+RbIeXhmKXHk2HxHRwUbTNHR2dmDPnt1uSF0cWKfTqarPff31XRXD7Pb2BQgGQ0ilKt/LYcGChTj00MNw6KFvcZfLl6+AxzO3b2g61zDMJiIiIiIioroiSyISER8SEd+4+xmmiWRaxVAyh9G0ipF0HqOpvH1zycL6aDqPkVQe+Vn8s+28ZiCvWSF8rQQBbsDt9xYF3YoMr0eCV5Hg9YjweiQoHsleWtvuQ3H6Cvt55EINWSKiemAYBgzDgCyXR5e9vb044og10LTabkq8a9fOiu2CIGDt2rUYGBjA6tVrsGrVGnu5GqtWrUYkEq3peDS9GGYTERERERHRnCQKgnWzx6Cy331N00RO1a1wO5XHSDqPZEZFKqNhNJNHKqMimdGQTOeRzGpIZlQk0+oBm/k9EaYJZHIaMjmrVMt0UmTRDbkVjwRFLgThih2KK7IERRbhKVqXZdF6rmyF4opHhMfZT3bGKt1PFBmcEx3sUqkUOjr2Yd++vSXLvXv3YPfuN7Fv317cfPNtOPXU95U9t6GhAZIk1Rxmv/76rqp9Dz74PxBFloWqZwyziYiIiIiIaN4TBMGe1SyjOeaf0HNM00QmpyOZtYLtVFZFJqchndWQLlmqSOc0ZMa0z8TNLmeKNXPcQDIz88eSRMEKvSU7+PbYwbcdiHtkp0+EXLSUZcFt90hWQC4XbxftI7vbIjySULTu9Aks2UI0w3Rdxz333GmH1fvQ0bHXXQ4NDe33+bt3v1GxXRRFLFy4CDt37hj3+T6fD0uXLsPSpcuxfPkKLFtmLcerjc0gu/4xzCYiIiIiIiKqQBAEq8a1b+IBeDFV05HO6UhnVWRyOtI5FemsNbM6bc+wdoJvNwjPacjmdGTzGrJ5HbpRPzPDp4tuWB8SZKADmNmbfY5HlgRIkhVwS5JgL62g2wm83TZRcMNzd120nieXPKd4LKtNEkuXsiRAsp/r9kkCZFGEV5EgemSksxpM04QkWvsweKd6kE6n0dXVie7uLnR3d6GrqxNdXV049dRNOPbYt5btL4oirrhiCzKZ2j4l27Nnd9W+xYuXYOfOHQgEAliyZCmWLbPCaiewXrZsOdra2hlOz0MMs4mIiIiIiIhmgEeWEJUlRCdQBqUS0zSh6QYyeR3ZvI5szgq4naDbacvkdeRU65F3141Cm73M5a12TZ87M8Znkqab0HQdOeizfSr7JYkCRDvYlkQrhJfGbIuC4AbkoihAtpfWuuiuO88Zu168j1xpX0mEKMBtF0XBOmbRvqIglPUX7ye4+6FKu/Mc68MkZx+aeX/5y9Po6elBb2/xo7dkfWRkuOJzGxubKobZgiCgvX3BfmdQV7N7d/Uwe9u2/wu/P4CGhgZ+2HOQYZhNREREREREVIcEQYBHluCRJUQC0zeubhjIq4YVcmvW0t1WdavkiBuCO/tZ+6ia1a+qVlkSZzuvGlB1e1s1oGoG8pqOOio5PqfphgndMGdxHvvscgJuURDskLt4vTQcFwUrHB/b5j5HFCDC6hMEaz9nHAFF60674ByrcMzivuJtoWhfYcxzSsZxj136HKfdOg8UrTtjAgJKxy/eHwCymRRGhocwPDyA0eEBjAwPYnjQWh5z3Dux4ci3us9F0djnfPgsjAwP1fTvs+vNPejsT7nnAgH2UkBTc9u4YXY0GkNb2wK0tbejrX0B2toWYPGixVi4aAmWLl2GTE6r8DoFtLUvBGDdS8Cs9UJjnyfD8LmFYTYRERERERHRQUQSRfi9IvzemY0ETNMKYN2g2w7KnaDbWVft8Fy163ZrutXmLNUx25rdphX1qboJzd5H051+s65u4Em1M0wThg4A/Pd8/bkHkEsNQc0mkc+OWsvMiPsw9OofefxlxzBWvear2GdIIQBDNZ3Tb5/4G4ZufrJiX7/RiqYlG+ALN8JvP6z1BvjCjZA9hfMZATCiA6++AeCNLPDY3wH8vaZzqkUhiC8O+61GwQnoS3a29i1vK2oa++FEcYBuf5ABwP1gpDkRwClHL8Lhyxum/wXOEwyziYiIiIiIiGjaCYLg1pKeLYZhWsF2UfitOcG3MWZds8q6uOuGYQfkJnTDCs11w95HN93gXNfL2zTDLHmubi+tbWs/3TCh6wzc5yvTNKGrWai5JNRsCmouBS1nLZ1t62H1a0XbK485G0vWnVJx3J1P34tssr+mc8plRqr2eQMxJAf2TmgcyeODL5SANxiHL5hAYsEhVfdd8/bzJn2es8W0/8+0/m9szwHRN5zFjr1DuP6Tb0M05D1gx51LGGYTERERERER0bwkigK8ogSvR5rtU6nKME03EIcAhMN+9A+kkFN1GIYJ3Qm+7fBbN6ywXC8Kyg17Frzh7Oes6wZ0s6jd2Vcv2sc0YTiBu7Ov02+WHt+w+w0DhX7DhFnU74zrjG0Wnc9ciO1NQ4emZqHlM+5DVwvrai4NLZ+GlkujZcUxVYPcR773UajZ0ZrOIZcarNrn8YVqDrPz44TZ/kgT/OEmKIEYvMEovIFY4RGMQQlE4Qsl4AsmICuTvyEuTVxeNZDKagyzq2CYTUREREREREQ0S0RBgCgL8MgiZFlEPOIDdB2aNv9u1OmE4WZx6G2iJAA3zKLQ3ARMu800UdRuLbPZLDKZDNKZFFKpNDLpFNKpFDKZNNLpNDKZFNLpNA7f8FYsXLISZtGxTHvML245B6nkCLKZFDKZFPK57IRfz1vWLME7jzzJrdts2EvTBB4PBjFUY5gdkPNY0R6BicJ4pj1jOByOYrRv/2OIkge+QATeQBRee9m0+DCEA56SOtOmPQn5rad/tmxmcvHxC/sW+mhmbFjdiLaGabxRwjzDMJuIiIiIiIiIiKbMNE3k83nkcllkszlksxlks1lksxlkMllkMml32wmiC/0ZnHnm2TjkkEPLxlVVFSeeeDzS6QzS6RQymQwymTQMY2KB/7Zt/xfvOO1/Vey7sHsPRkaGa3q9q9p8+Mf3rKnYd9s3Ehjq76pp3BXNMq766MaKfcG+92H79pWIxeKIx+OIxRKIx+NobGxEItGAhoZGNDQ0IBgMzfiNDU3TLAnAi0Nv2Nvlgbhpt415flE7ip5TEqzbgxeOWfvNH51zcw5YaUz3XIpeW/Hznb5CY8mi5OtSeD3Fr9VqMOyDC6KAJe0xxAMydJ0fF1TDMJuIiIiIiIiIaI7SdR35fB75fA75vGov81BVFYsWLYbfX14SYnR0BA8+eL/7vFzOWWaL1iu15ZDNZvG1r12HDRuOKht3cHAAa9cuq/m1HHLIoRXDbFmW8dprr044vB4rnU5V7QuFQjWH2aOj1ct2hMORku1IJIpoNIpIJIpYLOZuR6Mxe+n0xbFkydKq415xxb/UdK4zwbm5YdH/0RTIsoh4PIjBwRQ49706htlEREREREREdFAwDAOapkHTNBiGDlVVoWk6dF1z2611vWjd2VZx7LFvgySV19/u6NiH3/3uN/Z4KjRNg6pq0DTVbtPcPlVVoaoaVDXv9l177TfR1NRUNu7f/vYyPvnJjyGfz0PTNDukziOfV93njxfwPvLI77F+/ZFl7X19fbj00gtr/joODFSu2ez1+moeEwAymUzFdkEQ4PcHkEolaxo3lRo/zB6P1+tFKBRCMBhGJBJBOGwtQ6Ew1q1bX/V53/nODwAAsVgM4XCk4n83RDR5kw6z+/v78cUvfhFPPfUUJEnC6aefjiuvvBKyXD7Uo48+im3btmHPnj1oa2vD1q1b8e53v9vtv/nmm3H77bdjZGQEhx9+OL7yla9g+fLlAIB0Oo2rr74av/3tb6FpGk488UR86UtfQjAYnMLLJSIiIiIiIpoZpn0jveKls24Fjta6JMlVf7cdGOhHOp0eM47h1vh1xho7pmEYCAZDWL26csmDV199Bfv27Sl6bvFYesmYuq67x9J1HdFoFKeffmbFcf/0p8fx5JN/hq7r9vMM64aBuu4+3zD0om3D3Q6Hw7jmmm9UHPfBB+/Hrbfe4o5hBcvOWIZ9PM09rqZZfZqmQRAEvPDCKxXHveeeO3Hxxf+033/Lanbu3Fs24xYAXnnl7/jMZy6pedx/+ZcvVAyzNU3Fq69Wfi0TkcvlK7Z7vVO7sVw2m5uRcauF2QDg9/v3G2Z7vV4EAgEEgyF7GUQgEMSCBQurPufLX/4acrk8gsEgQqEQQqGwvQwhGAzB4/HU9FqWLq19hjoRVTfpMHvLli1oaWnBY489hr6+Plx44YW49dZb8fGPf7xkvzfeeAOf/vSnccMNN+Bd73oXHnnkEWzZsgWPPPIIWlpacO+99+L222/HD3/4QyxevBg33ngjLr30Utx///0QBAFXX301Ojs78fDDD0PXdWzZsgXbtm3Dl770pWl78fPZXXf9rObnbthwFFauXFWx7/77f4FsduI3Qyh2yCGH4S1vObxi38MP/3fNf9azfPkKHHXU0RX7fv/736Knp7umcRcuXITjjju+Yt+f//xH7Nmzu6ZxGxubcMIJJ1Xse/bZZ7Bjx/aaxg2HIzj11PdV7Hv55Zfw8st/rWlcr9eL97//AxX7duzYjmeffaamcQHggx/8cMUaXnv27Maf//zHmsc97bQzKv4pXU9PD37/+9/UPO4pp5yKaDRW1j4yMoxf/eqhmsd95ztPQEtLS1l7NpvF7bf/HKlUDro++T+ne+tbj8PixUsq9t15508nPZ5jw4ajsGrV6op9v/zlvVO6Rhx++LqKfb/61UMYHh6qadwVK1Zi48ZjKvb97ne/mdI14u1vf0fFvj/96fGarxFNTU044YSTK/b95S9PY/v212oaNxKJ4r3v3VSx76WX/oqXXnqxpnG9Xi/OPPPsin07dmzHM888VdO4AHDOOedWvEbs3v0m/vSnxys+ZyI1884446yK14ju7m789re/nvyJ2k499X2IxeJl7cPDQ3jwwfsnPZ4gAMGgF2972zvR1FT5GnHPPXeWtE2mZuDxx/+vqr9k3X77reM+d7zjHH30sRX/PBgA7rjj/034GjH2GEccsb7inzMDwC9+cQ+Ghoaqntd457t27SFVv98/+OD96OrqmNR4jqVLl+Gkk06p2Peb3zyCnTt3jDtmtWO0tbVX/b78hz/8Hi+9VP79frzzdfoSiQTOPfcjFfd54ok/46mnnih+VtXxxx7L5/PhU5+qHO48//yz+M1vCu+5auNUW9+69fMVrxF///vf8Itf3G3vX36elcYrbvvsZ7dWnKX3+uu7cOutP6zwnNLtasstWy5HS0tr2bjd3V3Ytu36kv0FwYTX60E2q7phqNM3dtxPf/ozFb8vj46OYOvWz9r7lT6/tB7p2D5r+6KLLsUxxxxbNi4AnHfeB8d9buFmZYWvkdP/iU9cWPX70Yc//AEMDw+VjFeoM1r+KB73Yx/7BP7P//lExXE/9KEzsGvXzpJwufjhtDvjFfYDzj33I/jKV66pOO5ZZ52GP/3p8bLXPxGnn34mbrnltop9l132z3jwwV9OeKxixx//v/Dznz9Qse+WW36A2277YU3jrl17SNUw+w9/+D1uuKFyIL0/TU3NVcPsjo69ePTR39U07nh1gitNxJsMVVUrttcaeBbG1aqMq0xx3Gph9tRmUOeq3CRRlmXIsgxNK309Ho8HPp8fPp8Pfn8Afr/P3bbarPVly6oHwDfe+B2YpolAIAC/3w+/P+CuO6F1Lf++1b5fE1F9mtS7/M0338RTTz2FP/zhD/D7/Vi0aBEuuugifPOb3ywLs++9915s3LgRJ51khXbvfe978fOf/xx33HEHLr30Utx5550499xzsWqVFZpedtlluPPOO/Hkk0/iiCOOwP3334//+q//QiwWAwBcfvnl+OhHP4qtW7dW/AWUSl1yySdrLoL/9a9/s2qYfeWVl6Gvr7emcT/72a1Vw+xrrvkyXnnl7zWNe/75F1QNs2+66QY8/vgfahp306b3V/3l9j//82bcd9/Paxr3rW89rmqYfeedP8V//ufNNY27atXqqmH2f//3A/jGN66tadxEIlH1l+bHHnsUV1752ZrGBYCzzz6n4g+azz//LC655JM1j/uud51Y8Tqxc+f2KY376KNPVAyzu7q6pjTuPffcXzHMTqWS+OhHP1rzuP/xHz+qGmZ/+tOfmtI1olqY/bnPXT6la0S1MPvaa78ypWtEtTD729++cUrXiGph9o9+dMsUrxGVw+y77vrZlK4R1cKDX/3qwSldI6qF2VO9RnzoQ/+74jXihReem9KfxZ544nsqXiN27dqBf/7ni2oed/36JyqG2d3d3diy5eKax73vvgcrhtmpVGpKM8D+4z9+VDXMvuyyS2se9+tf/2bVMPsrX/lizdeIyy67smqYfcMN35jSNaLa9/tbbvk+/vjHx2oad9Om91f95fhnP/t/NV8j3va2t1f9vvzQQ/fXfI1YvXpN1TD7scd+j29+8+s1jZtIJKqG2c8++xdcf33lkHAitm79fMX2HTtew403bqt53Asv/HTFMLuzswPf+963ax73/PMvqBhmDw0N1RwwAta1stL35VwuX/aB12SceeZZVft+/euHax5306b3V+174YXn0N9fuXTB/ox3bens7MDu3W/WNG4mk67a58wUrsV4P4eJoljTmADGLTchirXXsx1/3Kmcb/WvnyTVHjo7H0pUOreplnrQtMrnXK8heT5fOcz2+Xw4/PAjoCgKfD4fFEWB1+uFonjh9XrtdQWK4nX7FcULv98Hr9dXsXSJ4/HHn4bX64XP54fX64Xf75+WEhunnHLqlMcgorlvUlfb7du3IxaLlYQuK1asQEdHB0ZGRhCJFP7UZseOHVi9uvSHqpUrV+KVV15x+z/xicIn5x6PB0uXLsUrr7yCWCwGVVVLnr9ixQpks1m88cYbOOSQQyZ0vqIoTOkbdj2QJLFkeSCIogBZrny8qdwId7xxp0IUMSPnKwgHftyp/vd6oMeVpKmPW+kHzKmfb+X/1qZ6vpJ0oMed2vtl5t5zvEY4z+U1YvxxeY2YnnFFUaw4rixPdVxeI6xxeY1wzMVrRKUPvKZ6vpJU+Zyn/l6emXGrXXs8nnr9OWK898ZUvhbmjIw73ntuaj+rVT/fqYxrmkbVcWW59jBxpsbV9erjKsrUwmFBqPw1VpSJh8PW92AZHo8HkiRDkqSq4yYScRx11NHweGR7drLHnaXs8Xjchyx7oCiFdWspo6WlGbIslmUACxa045ZbbrXDZKXo+UpRm+yGzV6v4q5b51353ycaDePRR2v/a9jxrF5deXIcEY1vNjLAuWhS3x1SqVTZTCZnO51Ol4TZlfb1+XxIp9P77U8mrRpIgUCg7DjjFe0fK5EITvEHovoRiRy42eiBgBfxeOX6bVP5evr9StVxp/JG9Xo9Vcedyg9WiiJXHXcqP1jJslR1XK+39k/dJUmsOq7fX/ufpgmCUHXcQGBq9dDi8WDFoCoUmtqfvMViwYrnHA5P7X0UjQYqjhuNBirsPXHhsL/iuLpevV7cRIRCvqr/dlPBa4SF1wgLrxEFM3WNCAYrv+d4jSjgNcIy164RweDU3svxeOWf9efaNSIWm9r7sNp7earXiGrXnqka7708lRm+Pt+Bfy97PFN5L1d/z03kvSwIAgRBgChaH/wKggBJkhAMVv7vDABaW5uxcOFC9zmiKEKSpKrbxetLliypOu6GDetwxhlnlDyn0rLSIxis/H4DgJNOejduuOEGd19ZlkueW23bWTY2Rir+N3XGGZvw2muvufsWB87F2865T9Txxx87pfJqYzkZQDwexAUXnD9t4xJR/TuQGeBcNKnvvoFAoKwYv7M99uYVfr+/rCZiNpt19xuv3wmxM5mMu79znP3dZbbYwEBqXszMjkT8GBnJTKpm7rp1R9RcQsDvD2NwsPKHBocccihaW9tqGjcaTVQdd8WKVTX/GVljY0vVcRcvXop1646oady2toVVx21rW1jzuIsXL6s6bkND8xTGXVx13HA4XvO40Wi06rg+X6jmcQFgcDBV5U//fFMaN5nMQVHKz9kwpCmNm8sZFb8WuZwxpXENQ6o4biqVx4YNG2qqlw1YX8dq/3aVrhETDZnGv0Ychra29smdqG28a8TKlashy7WFNE1NreNeI444Yn1N47a3Lxr3GlFp3Il8jZcuXVF13KamVqxfv2FS5+lYtKj6NSISSdQ8biQSqTqu3x/Ghg3V/wR1f6pdI2TZhyOPrFxuYiKqXSNMU8aRR26sedx83qx6jTjyyI2TDnIFwfo5QBA8FcdNJnNVS+hMhKIEq/7bHXPMW/f7c0S11xMKxaqOu2HDURgaGprwORYfoqGh+vf7ww8/ApFIdNzzqmbBgiVVx1279rCqf06+v+MsX75qnJ97VuOd73xXpVH3e4xDDz2s6rhLliwvK2VWaYxKbQsWLKg67oIFi8v+tHvsGMXbxeuhUGica1o7Nm06fULjVNoeHExVfC3RaCPOOuuD4445Xls2q1c8Z78/4pZiGfucwnK843grjisIXnziE58s2rZCSq/Xg3xec+tbF49ZvB6JNFS59pjYsuWyoucUj1M61tixBUHAokXLq/7bfelLXy0Zq9oYglB+3hs2HFN13Kuv/joymcyYMcZ7wD3+2rVrq4577bXfQCqVKjrPwhhOEOyca2HbWm9urn7t+cY3bkQ6nbb3tcZ1nl88jhM0O+2AgEDAX3Xcb33r2/jGN/6tJKwufhR/vSupNu5nPnMlPvOZK6s+b3+qjXvyye/DySdXLnc4lXEXLlyOzZtrv1Hj8HC1D3RENDZW/nlV0wBNMwBULs1xINSaARDR3Mf3Pyb0QbpgTiLxfOONN3DKKafgj3/8IxobGwEADz30EK6//no8+uijJfveeOONePnll3HLLbe4bR//+Mfxlre8BVu2bMGHP/xhnHDCCfinf7K+OamqimOPPRb//u//jvXr1+Poo4/GT37yExxxhBUQvfDCCzjvvPPwzDPPwOeb2GyL3t7Rib60uuV8Yj84mLK/qRLRwYLvf6KDG68BRAcvvv+JDl58/xMdvPj+B5qawvvdZ1J/x7V06VIcddRRuPbaa5FMJrFnzx5897vfxdlnl9/46fTTT8dTTz2Fhx56CJqm4aGHHsJTTz2F97/fuuHHWWedhR//+Md45ZVXkMvl8K1vfQuNjY3YuHEj/H4/Tj31VGzbtg0DAwMYGBjAtm3bsGnTpgkH2UREREREREREREQ0f0y6KNlNN90ETdNw4okn4kMf+hDe8Y534KKLLgIAbNiwAb/85S8BWDds/Pd//3f84Ac/wNFHH43vfve7+Pa3v41ly5YBAM4++2xs3rwZF198Md761rfib3/7G37wgx+4d+r90pe+hKVLl+K0007DP/zDP2DhwoX413/91+l63UREREREREREREQ0h0yqzMhcwzIjRDSX8f1PdHDjNYDo4MX3P9HBi+9/ooMX3/8zUGaEiIiIiIiIiIiIiGg2MMwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIioronmKZpzvZJEBERERERERERERGNhzOziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcwmIiIiIiIiIiIiorrHMJuIiIiIiIiIiIiI6h7DbCIiIiIiIiIiIiKqewyziYiIiIiIiIiIiKjuMcyuY/39/bjooouwceNGHHvssbjmmmugadpsnxYRzYBXXnkFH/vYx3DMMcfg7W9/O7Zu3YqBgQEAwAsvvIAPfvCD2LBhA0444QTcdddds3y2RDQTdF3HRz7yEXzuc59z2/j+J5r/hoaGsHXrVhx77LE4+uijcdFFF6GnpwcArwFE893LL7+M8847Dxs3bsTxxx+Pr33ta8jn8wD4/iearwYGBnDyySfjySefdNv2936/9957cfLJJ2P9+vX4wAc+gOeee+5An3ZdYZhdx7Zs2YJAIIDHHnsMd999N/785z/j1ltvne3TIqJpls1m8fGPfxwbNmzA448/jgceeABDQ0P4/Oc/j+HhYfzTP/0TzjjjDDz99NO45ppr8PWvfx0vvvjibJ82EU2z73znO3jmmWfcbb7/iQ4On/70p5FOp/HrX/8av/vd7yBJEr74xS/yGkA0zxmGgU9+8pM45ZRT8NRTT+Huu+/G448/jptvvpnvf6J56i9/+QvOOecc7N69223b3/v9ySefxNVXX43rrrsOTz/9NE4//XRceOGFyGQys/UyZh3D7Dr15ptv4qmnnsIVV1wBv9+PRYsW4aKLLsJPfvKT2T41IppmHR0dWLt2LS6++GIoioJ4PI5zzjkHTz/9NB555BHEYjGcd955kGUZb3vb23DaaafxWkA0z/z5z3/GI488gve85z1uG9//RPPfSy+9hBdeeAHXXXcdIpEIQqEQrr76alx++eW8BhDNc8PDw+jt7YVhGDBNEwAgiiL8fj/f/0Tz0L333ovLL78cn/nMZ0ra9/d+v+uuu/C+970PRx11FDweDzZv3ox4PI6HHnpoNl5GXWCYXae2b9+OWCyGlpYWt23FihXo6OjAyMjILJ4ZEU235cuX45ZbboEkSW7bww8/jMMOOwzbt2/H6tWrS/ZfuXIlXnnllQN9mkQ0Q/r7+3HVVVfhW9/6Fvx+v9vO9z/R/Pfiiy9i5cqVuPPOO3HyySfj+OOPx/XXX4+mpiZeA4jmuXg8js2bN+P666/H4Ycfjne+851YunQpNm/ezPc/0Tx0/PHH49e//jXe+973lrTv7/2+Y8cOXg/GYJhdp1KpVMkvtADc7XQ6PRunREQHgGmauPHGG/G73/0OV111VcVrgc/n43WAaJ4wDANXXHEFPvaxj2Ht2rUlfXz/E81/w8PDePXVV/HGG2/g3nvvxS9+8Qt0d3fjyiuv5DWAaJ4zDAM+nw9f/OIX8fzzz+OBBx7Azp07cdNNN/H9TzQPNTU1QZblsvb9vd95PSjHMLtOBQKBsvo3znYwGJyNUyKiGZZMJnHppZfi/vvvx49//GOsWbMGfr8f2Wy2ZL9sNsvrANE88YMf/ACKouAjH/lIWR/f/0Tzn6IoAICrrroKoVAIjY2N2LJlCx599FGYpslrANE89utf/xoPP/wwzj33XCiKglWrVuHiiy/GT3/6U/4MQHQQ2d/7ndeDcgyz69SqVaswNDSEvr4+t23nzp1obW1FOByexTMjopmwe/dunHXWWUgmk7j77ruxZs0aAMDq1auxffv2kn137NiBVatWzcZpEtE0u++++/DUU09h48aN2LhxIx544AE88MAD2LhxI9//RAeBlStXwjAMqKrqthmGAQA45JBDeA0gmsc6OzuRz+dL2mRZhsfj4c8ARAeR/b3fV61axevBGAyz69TSpUtx1FFH4dprr0UymcSePXvw3e9+F2efffZsnxoRTbPh4WGcf/75OPLII/HDH/4QiUTC7Tv55JPR19eHW2+9Faqq4oknnsD999+Ps846axbPmIimy69+9Ss8++yzeOaZZ/DMM89g06ZN2LRpE5555hm+/4kOAscddxwWLVqEz3/+80ilUhgYGMCNN96Ik046CZs2beI1gGgeO/7449Hb24vvf//70HUde/bswfe+9z2cdtpp/BmA6CCyv/f72Wefjfvvvx9PPPEEVFXFrbfeiv7+fpx88smzfOazRzCd2+ZS3enr68NXv/pVPPnkkxBFEWeccQYuv/zykpvEEdHc96Mf/QjXXXcd/H4/BEEo6Xvuuefw17/+Fddccw1ee+01JBIJXHTRRfjABz4wS2dLRDPpc5/7HADguuuuAwC+/4kOAt3d3bjuuuvw9NNPI5fL4YQTTsBVV12FSCTCawDRPPenP/0J//Zv/4Zdu3YhHA7j9NNPx8UXXwxFUfj+J5rH1qxZg//6r//CscceC2D/P/Pfd999+N73vofu7m6sXLkSX/jCF3DEEUfM1unPOobZRERERERERERERFT3WGaEiIiIiIiIiIiIiOoew2wiIiIiIiIiIiIiqnsMs4mIiIiIiIiIiIio7jHMJiIiIiIiIiIiIqK6xzCbiIiIiIiIiIiIiOoew2wiIiIiIiIiIiIiqnsMs4mIiIiIiIiIiIio7jHMJiIiIiIiIiIiIqK6xzCbiIiIiGgafPvb38aaNWv2+7jmmmtm+1Tdc33yySdn+1SIiIiIiCZMnu0TICIiIiKaT8455xwcddRRVftXrFhxAM+GiIiIiGj+YJhNRERERDSN1q9fj/e///2zfRpERERERPMOy4wQERERERERERERUd1jmE1ERERENAtOOOEEbN68GY8//jjOPPNMrFu3Du9+97uxbds2ZLPZsv1/+ctf4pxzzsH69euxfv16nHPOObjvvvvK9jNNEz/96U/xgQ98AOvXr8dxxx2HT33qU3j55ZfL9h0cHMS//uu/4rjjjsO6detwxhln4IEHHpiR10tERERENFUMs4mIiIiIplE6ncbAwEDVh2ma7r47d+7Epz71KaxcuRJXXnkl1q1bh5tvvhkXXHABDMNw97v66qtxxRVXQFVVXHLJJbjkkkuQz+exdetWfO1rXys5/pVXXokvf/nL8Pv92LJlCzZv3oyXX34Z//iP/4i//e1vJfv+y7/8C1577TVcfPHFuOSSS9DX14fLLrsMjz322Mx+kYiIiIiIasCa2URERERE0+jqq6/G1VdfXbX/6aefRiQSAQD09PRgy5YtuPDCCwEA5513Hq699lrcdtttuO+++3DmmWfimWeewY9//GO87W1vw8033wyPxwMAOP/883HBBRfg9ttvx3ve8x4cc8wxeOKJJ3Dfffdh06ZN2LZtGwRBAACceOKJ2LRpE77//e/jpptucs9lw4YNuOWWWyCK1hyXdevW4fzzz8dDDz2Ed7zjHTPy9SEiIiIiqhXDbCIiIiKiaXTBBRfg+OOPr9ofCATc9XA4jAsuuKCk/1Of+hRuu+02PPzwwzjzzDPx3//93wCASy65xA2yAcDj8eDSSy/Feeedh4ceegjHHHMM/ud//gcA8PGPf9wNsgFgxYoVuPvuu9Hc3FxyrDPOOMMNsgHr5pWAFbITEREREdUbhtlERERERNNo5cqVOO644ya075IlS6AoSklbIpFANBrFnj17AAC7d+8GAKxatars+atXrwYA7N27t2S5YsWKsn0PO+ywsrampqaSbZ/PBwDI5/MTOn8iIiIiogOJNbOJiIiIiGbJ2CDboes6JEkCgJIa25X2Kx5HVdVJHb94VjYRERERUb3jT69ERERERLNk9+7dZWF1d3c3kskkli5dCgBYvHgxAGD79u1lz9+xYwcAoL29HQCwcOFCAMDrr79etu8NN9yAa6+9dtrOnYiIiIjoQGOYTUREREQ0S/r6+nDfffeVtH33u98FAGzatAkAcMoppwAAvvOd70DTNHc/TdPwne98p2Sfk046CQBw2223lYy5e/du3HrrrW7pEiIiIiKiuYg1s4mIiIiIptHzzz/vlgipJBgMuqGzx+PBF77wBbz44otYuXIlHn/8cfzmN7/BySefjPe85z0AgGOPPRbnnHMO7rjjDnzoQx/C+973PgDAgw8+iJdffhnnnnsujj76aADAO97xDmzatAn33HMPurq6cMIJJyCZTOInP/kJvF4vrrjiihl+9UREREREM4dhNhERERHRNLrjjjtwxx13VO1fsGCBG2Y3NzfjK1/5Cq6//nrcddddWLBgAbZu3YrNmzeXPOerX/0q1q1bh5/97Ge46aabIEkS1q5di23btuG0004r2feb3/wm1q1bh7vvvhvXX389otEoNm7ciH/+53/GsmXLpv31EhEREREdKII53h1liIiIiIhoRpxwwgkAgN/+9rezfCZERERERHMDa2YTERERERERERERUd1jmE1EREREREREREREdY9hNhERERERERERERHVPdbMJiIiIiIiIiIiIqK6x5nZRERERERERERERFT3GGYTERERERERERERUd1jmE1EREREREREREREdY9hNhERERERERERERHVPYbZRERERERERERERFT3GGYTERERERERERERUd1jmE1EREREREREREREdY9hNhERERERERERERHVvf8PbIJ/ZRrCxysAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (18, 8)\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "last_epoch = len(history.history['loss'])\n",
    "\n",
    "plt.plot(np.arange(1, last_epoch+1), history.history['loss'], label='Loss', lw=3)\n",
    "plt.plot(np.arange(1, last_epoch+1), history.history['val_loss'], label='val_loss', lw=3)\n",
    "plt.plot(np.arange(1, last_epoch+1), history.history['lr'], label='Learning rate', color='#000', lw=3, linestyle='--')\n",
    "plt.title('Evaluation metrics', size=20)\n",
    "plt.xlabel('Epoch', size=14)\n",
    "plt.legend()\n",
    "plt.savefig('eval_vs_lr.jpg', dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edb9337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Batch_Norm_1 (BatchNormali  (None, 5, 96)             384       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " LSTM_1 (LSTM)               (None, 5, 512)            1247232   \n",
      "                                                                 \n",
      " LSTM_2 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " Returns (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3480714 (13.28 MB)\n",
      "Trainable params: 3480522 (13.28 MB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d563c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'RMSE Loss: Training and Validation')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.title('Huber Loss: Training and Validation')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['root_mean_squared_error'])\n",
    "plt.plot(history.history['val_root_mean_squared_error'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.title('RMSE Loss: Training and Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdd49756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 9.4795e-04 - root_mean_squared_error: 0.0435 - mae: 0.0346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0009479468571953475, 0.043541859835386276, 0.03462986648082733]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(my_window.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b563725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdIAAALPCAYAAABmGTJsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAACvFUlEQVR4nOzdeZxN9R/H8fe9szCLYexLyjbWlDWUXUqKyh7JFj/7VoiSiIqIbFlKCdmXyC5LQkiSyG4Y2c1m9u3+/pCb0wxmmJkzc+/r+Xjcx9z5nGU+d/K58Z4z32Ox2Ww2AQAAAAAAAACAJFnNbgAAAAAAAAAAgIyMIB0AAAAAAAAAgHsgSAcAAAAAAAAA4B4I0gEAAAAAAAAAuAeCdAAAAAAAAAAA7oEgHQAAAAAAAACAeyBIBwAAAAAAAADgHgjSAQAAAAAAAAC4B4J0AAAAAGkmPj7e7BYAAACAh0aQDgAAgPsqVaqUSpUqpXfeecfsVjKUCxcu2L83S5cuNbudDCUsLEyjR4/WmjVrzG4lw2rfvr1KlSql1157zexWAAAAcB8E6QAAAABS3Ysvvqh58+YpISHB7FYAAACAh+ZqdgMAAABAZuXm5qZHH31UkuTt7W1yNxnL5cuXzW4BAAAASDUE6QAAAMADypcvnzZv3mx2GwAAAADSGEu7AAAAAAAAAABwD1yRDgAAgHQRFBSkuXPnatu2bQoICFB8fLwKFCigWrVqqXPnzipQoMBdjw0MDNSiRYu0a9cunT17VqGhocqSJYvy5cunGjVqqH379ipSpEii40qVKiVJ+vrrrxUcHKzJkyfrwoULypkzp1588UUNGTJE77zzjlauXKkmTZpo/PjxWrdunRYtWqTjx48rMjJShQoVUsOGDdWlSxdlz57dcP4LFy6oQYMGkqTRo0erZcuW9m3169fX33//rdGjR+vll1/WN998o3Xr1uncuXOyWCzy8/PTK6+8opYtW8rVNem/lgcHB2v+/PnavHmzzp8/L3d3dz355JPq0qWLqlWrpvLlyysmJkbffvutqlWrlqz/DitWrNDQoUOVL18+bdq0SePGjdMPP/ygmJgYFS5cWEOHDtXTTz8tSUpISNCGDRu0ceNGHT58WIGBgUpISFCOHDlUvnx5vfzyy2rYsKEsFkui133b0KFDNXToUD311FOaN2+eoZeLFy/qm2++0c6dO3Xp0iVZLBYVLlxY9erVU8eOHeXr65us1yRJV69eVd26dRUfH6/+/furR48ed923Q4cO+uWXXxL19Mcff2jBggXav3+/rl69qixZsqhgwYJ65pln9Prrr+uRRx5Jdj+pITo6WsuXL9f69et14sQJhYeHy9fXVxUrVlTz5s1Vp06dux67a9cuLV68WAcPHlRQUJA8PT1VuHBh1a5dW+3bt1fOnDmTPG79+vVatWqVDh8+rJCQEHl7e6tYsWKqX7++XnvtNZYwAgAATosgHQAAAGnul19+Ud++fRUSEmKonz17VmfPntWSJUs0btw4Pf/884mO3bFjh/r376+IiAhDPTY2VmFhYTp9+rSWLl2qqVOnqnbt2kl+/U2bNmnhwoX2z69cuaIcOXIY9rHZbPZQ/U5nzpzRzJkztWbNGi1cuFD58+dPyUtXaGioWrZsqWPHjhnqv//+u37//Xdt2bJFs2bNkouLi2H76dOn1blzZ8Na4xEREdqxY4d++uknDR48OEV9/JfNZtPAgQP1448/2munTp1SsWLFJN364UX37t116NChRMdeuXJFV65c0ZYtW/Tqq6/qk08+SfHXX7t2rYYOHaro6GhD/fjx4zp+/LgWLVqkadOmqUqVKsk6X968eVWjRg39/PPPWrdu3V2D9KtXr2rfvn2SpKZNm9rrS5cu1fvvv2+4OWpsbKxOnDihEydOaMGCBfr8889Vv379lL7UB+Lv769evXrp1KlTifrfuHGjNm7cqMaNG+uTTz5RlixZDPt8/vnnmj59uqEWEhKikJAQ/fnnn5o/f76++uorPfHEE4Z9Bg8erO+//95QCw4O1m+//abffvtNCxYs0Lfffmu/LwAAAIAzYWkXAAAApKkTJ07of//7n0JCQvTII49o3Lhx+umnn7R7927NmjVL5cuXV1RUlAYOHKgDBw4Yjr148aL69euniIgIFSlSRBMnTtSWLVu0Z88eLV++XG+88YZcXV0VHR2tESNGyGazJdnDwoUL5efnp4ULF+rnn3/W+PHj9eqrrxr2+fHHH7Vy5UrVrVtX3333nX755Rd9//33aty4sb2XiRMnpvj1T5kyRSdOnFCnTp30ww8/6JdfftE333yjsmXLSpJ+/vlnrVq1ynBMRESEunTposuXL8vLy0vvvvuuPUAfOXKksmfPrrFjxyomJibF/dx29epV/fjjj3rttde0bds2bdiwQR999JH9BwVDhw7VoUOH5OLiot69e2vNmjX65ZdftH79en300UcqWLCgJGnlypXas2eP/bxr167Vb7/9Zv985MiR+u233zR79mx7bffu3Xr77bcVHR2t0qVLa9q0adq9e7d27typiRMnqkiRIgoODla3bt3k7++f7Nf08ssvS7r1Z+7kyZNJ7rNu3TolJCQoS5YsatSokSTp77//1siRI5WQkKBatWppwYIF2rVrl7Zv364JEyYob968iomJ0dChQxUWFpbsfh5UUFCQunTpolOnTsnNzU3du3fXunXrtHfvXi1atMj+A6d169Zp6NChhmMPHjxoD9GbNGmiZcuWac+ePdqyZYtGjhwpb29vhYaGasiQIYYfGvzwww/2EL1Dhw5avXq1fvnlF23atElvvfWWXF1ddenSJY0cOTLNXz8AAEBGxBXpAAAASFOjRo1SVFSUHnnkES1btsywXEedOnVUo0YNvf766zp06JBGjhyp1atX27fPnz9fkZGRcnNz05dffqnChQvbt+XMmVOPP/64LBaL5s6dq4sXL+r06dMqUaJEoh6sVqumTp1qX/6lSZMmifaJjIzU888/r8mTJ9trvr6+mjhxos6fP68///xTW7Zskc1mMyxlcj+RkZF677331L59e3utRo0a+vLLL1W/fn1FRUVp8+bNat68uX37l19+qUuXLsnFxUWzZs0yXJXdpk0bVahQQa1atUp0NXdKValSRR988IH986JFi0q6dWX69u3bJUl9+vQxXN3t6+urYsWKqVy5cvbgeufOnapRo4YkycPDw/A13N3d5eXlZf88Pj5ew4cPV0JCgp544gnNnz/fcEV148aN9cwzz6hZs2a6cOGCxo4dqy+++CJZr6dhw4by9PRURESE1q5dq/79+yfa54cffpB0awmabNmySZK2b9+u2NhYeXp6avr06XJ3d7fv/9JLLyl//vxq166dgoODtXv3bj333HPJ6udBzZo1SxcuXJAkTZo0Sc8++6x9W8WKFVWxYkV9+OGHmj9/vtauXauXX37ZvszLpk2bJEmPPfaYPv30U/uf1Zw5c6pNmzbKmjWrhgwZojNnzuivv/5SuXLlDMfVqFFDw4YNs389X19fdevWTbGxsZo8ebJ27dqloKCgFC27AwAA4Ai4Ih0AAABp5tSpU9q/f78kqWfPnkmGb+7u7howYICkW8t63LmUSMmSJdW6dWt17drVEKLf6amnnrI/DwoKSnKfUqVKJbmG+n917do1yfrtkDIsLOyuX+NusmfPrjZt2iSq58qVS+XLl5cke2h62+0r1Bs3bpzk0ialS5dWu3btUtRHUu4WCMfHx6tz5856/vnn9dprryW5T+nSpeXj4yPp7t/3pPz888/21/vWW28lWpZEuvU96969uyRp27Ztunr1arLO7eHhYb9ae/369Ym2nzt3TocPH5b079Xrkuw/kIiLi0vytVSpUkXTpk3TypUrVbNmzWT18qASEhK0fPlySdKzzz5rCNHvNGTIEPs653cuW3T7txQiIiISLYckSc8//7ymT5+uNWvWyM/PL9FxISEhiouLS3Tca6+9plmzZmnt2rX2H0AAAAA4E4J0AAAApJm9e/fan5csWVLh4eFJPkqVKmVfI/zO5V1eeeUVjRo1Sv369Uvy/JcuXdLRo0ftnycVAEpSmTJl7turm5ubfbmV/8qVK5f9eVRU1H3P9d+v7ebmluS220FoZGSkvXbmzBn7zTrvtR53UuvJp9Tdvi+lSpXSkCFDNHny5ERryUtSeHi4du3aJav11j8n7vZ9T0py/0w8/vjjkm6t5X7w4MFkn/92QO7v768///zTsG3NmjWSbn3fa9WqZa/f/mFFTEyMWrRooWnTpunIkSOGpYKeffZZlS1bVp6ensnu5UEcP37cfi+Be1357u7ubv/zsX//fnuvt1/LtWvX1KxZM82ZM0enT5+2H+fh4aEGDRqoZMmShivvbx939OhRtWrVSt99953hBzw5c+ZUnTp1VLx48bveHBcAAMCR8TcgAAAApJmAgAD78xYtWiTrmEuXLiWqxcbGavfu3Tp27Jj8/f0VEBCgU6dOJbp6+G5rpCcVBv+Xj49Poht+3nZn4HjnutLJca8lMG6f986+b4foku55Ff3tm4I+jOR8X06cOKH9+/fr7NmzCggIkL+/v86fP2/4Ptzt+56UO/9M3F4O5n6S+jNxN9WqVVP+/Pl1+fJlrV271h7IS7fWb5ekF1980RAGP/HEE2rbtq2+++47Xb16VZMnT9bkyZOVK1cuPfPMM6pXr57q1q2b5iG6ZHytxYsXv+e+t7eHhYXp5s2b8vHx0XPPPad69epp27Zt8vf319ixYzV27FgVLFhQNWvWVL169VSzZk3Dn2lJateunTZs2KDDhw/ryJEjOnLkiKRby/3UrFlTDRo0ULVq1ew/PAEAAHA2BOkAAABIMw9yY8b/HrN06VJNmzYtUZhqtVpVpkwZFSlSJMllPO7039AwKWl1lW1KzxscHGx/njVr1rvulxqhblLLqtx24sQJffzxx9q9e3eibXny5NEzzzyjbdu22a+eTq7U+DNxL1arVU2aNNHs2bO1YcMGDR48WBaLRUePHtWZM2ckSU2bNk103IgRI1StWjXNnz9fBw4cUEJCgm7cuKHVq1dr9erV8vLyUo8ePe66/E9qufO13u+/8Z3r0UdERNh/GDR9+nQtW7ZMixYtsgfiFy9e1JIlS7RkyRLlzJlTgwcPNtxw18PDQ999952+/fZbLV++3P69Onv2rM6ePat58+apUKFCGjFihH2pIwAAAGdCkA4AAIA0c2cQ/Mcff9wzuE3KvHnzNHr0aElSvnz59Nxzz6lMmTIqUaKE/Pz85Onpqd27d983SM9M7gxH71zy5b/ute1hXbx4Ua+//rpCQkLk5uam+vXrq2LFivLz85Ofn5/y5csnSapdu3aKg/Tbfyby5Mmjn3/+OdV7l24t7zJ79mxdvHhRBw8eVKVKlezLuhQtWlRPPPFEksc1atRIjRo1UmBgoHbt2qXdu3dr165dunLlisLDwzV+/Hi5ubmpY8eOadK3ZAzPk1rj/E53hu53/rmxWq1q1aqVWrVqpcuXL2vnzp3as2ePdu3apeDgYAUGBuqdd96Rt7e3GjZsaD/O3d1db775pt588035+/vbvwd79uxReHi4/v77b/Xq1UvffffdXb+HAAAAjoogHQAAAGmmYMGC9ucXLly451IVNptNFovF/nlUVJQ+//xzSVL58uU1b948Q1h4W0pv/pnRPfroo/bn586dU7ly5ZLc79y5c2nWw4wZMxQSEiIXFxfNnz9fFSpUSLSPzWZLcYgu/ftnIjAwUBEREWmyXIqfn5/Kli2ro0ePatOmTapYsaI2bNggyXiT0bvJmTOnmjRpoiZNmshms2nXrl0aOHCgQkJC9O2336ZpkF6oUCH789OnTxuWpvmv21eNe3l5KXv27Enukz9/frVs2VItW7ZUfHy81q9fr6FDhyomJkbffvutIUi/U5EiRVSkSBG1a9dOMTExWrhwoT7++GPFxsZq4cKFBOkAAMDpsMAdAAAA0sztGxhK0o8//njX/X777Tc9+eSTev755+1Xl586dUo3b96UJL366qtJhuiStGfPHvvzlK5fnhGVKFHCfnPTn3766a77bd++Pc16uH1zzzJlyiQZoku3/pvdvvFqStZIr1y5siQpPj7+nq9hzZo1qlixol588UX9+uuvyT7/bbcD8x9//FG///67Ll68KIvFoiZNmiTad/To0Xr++ec1cODARNssFotq1qxpXw7m6tWrKe4lJUqWLCkfHx9J0saNG++6X0xMjLZt2yZJqlixor3er18/NWjQQJ9++mmiY1xcXPTSSy+pZs2akv59LZGRkerSpYvq1KmjBQsWJDrO3d1dHTp0UMmSJSVJV65cecBXBwAAkHkRpAMAACDNPPHEEypdurQkafbs2fL390+0T1RUlMaOHavo6Gj9/fff9itd77zx56lTp5I8/65du7RixQr757GxsanYvTmsVqv9xqw//PCDDh8+nGifgIAAffPNN2nag3Trxqe3w/I7hYSEaNSoUfbPk/q+314b/r/bGjRooNy5c0uSxo8fr8DAwETHBgYGavLkyYqIiND169dVpkyZFL+Gl156Sa6urjp//rxmzpwp6dYPdh555JFE+yYkJMjf31/btm1L8kp/m82m48ePSzL+xkBacHFxUfPmzSXd+iHAli1bktzv008/tf82RsuWLe31qKgoXbhwQatXr07yexsTE6PTp09L+ve1eHh46MqVK7p8+bIWL16s6OjoRMeFhITo4sWLhuMAAACcCUE6AAAAks3f319Lly697+PkyZP2Y0aMGCFXV1eFhoaqdevWmj9/vi5cuKAbN27o559/VseOHfX7779Lkrp06WJf2qJkyZLKkyePJGnRokWaPn26zp07p8DAQP3xxx8aPXq0unXrpvj4ePvXut+a0plFt27dlC9fPsXGxqpz585asGCBLl++rGvXrmnVqlVq27at/Wp9SYYlcVLDM888I+nWsjk9evTQwYMHFRgYKH9/f3333Xd69dVXdezYMfv+4eHhic6RI0cOSbfC4CtXrthD3yxZsujdd9+VdCuob9GihVatWqUrV67oypUr2rx5s9q3b6/z589Lkt566y15eXml+DXkzp1bTz/9tCTZr9y+27Iu7du3l7u7uyIiItSpUyetXLlS58+fV2BgoA4dOqS33npL+/btkyS9/vrrhmM3b95sX1t98+bNKe4zKT169LDPQf/+/TVx4kSdPn1aISEhOnTokPr3769vv/1WkvT888+rUaNG9mO7dOki6dbV5h06dNCmTZv0999/6/r169q3b5+6d+9u/2HBna/l9nHHjx9Xp06dtHPnTvt/kx07dqhz5866efOmXFxc9Nprr6XK6wQAAMhMWCMdAAAAyXbw4EH7sh/3MnToUPn5+UmSKlWqpMmTJ+vtt99WcHCwPvzwQ3344YeJjmnZsqX69u1r/9zFxUUffvihevfurbi4OH3++ef2NdNvs1qt6tatm7799ltFRUUlecV7ZuTt7a1Zs2apY8eOCgoK0qhRowxXgFutVrVu3VqLFy+WZLx6PzX873//0/bt23X69Gnt3r1bu3fvTrRPhQoVlCNHDm3fvj3Jq7irVaumtWvXaseOHapdu7YKFSqkrVu3SpIaN26s0NBQjR49Wn///beGDBmS6HiLxaJevXqpVatWD/w6XnnlFfvyOFmyZDEEzncqWrSoxowZo2HDhunvv//WO++8k+R+bdq0Udu2bQ21mzdv6uzZs/bnqSF79uyaM2eOunfvrrNnz2rGjBmaMWNGov2aNm2qkSNHGmpPPfWU3nrrLX322Wc6ceKE+vTpk+g4q9Wqfv36qU6dOvbaq6++qkOHDmnhwoU6cOCA3nzzzUTHubm56cMPP1SpUqVS4VUCAABkLgTpAAAASHMNGjTQ5s2bNW/ePO3YsUMBAQGKjo6Wr6+vKlasqNatW9uvgr5TvXr1tHjxYn355Zf69ddfFRQUJHd3d+XPn1+VKlVS27ZtVa5cOR05ckS7du3S5s2b1atXLxNeYeorXbq01q1bp9mzZ2vr1q26dOmSsmbNqkqVKql79+5KSEiwB+lZsmRJ1a+dPXt2LVmyRLNnz9bmzZsVEBAgm82mHDlyqFSpUmrSpIleeuklrV+/Xtu3b1dAQID++usvwxIs77//vlxdXbVjxw77bwpER0fbe23Tpo2eeeYZzZ07V3v27NHFixcVGxurvHnzqkqVKnr99dcf+oaWDRo0kLe3t8LCwlSvXj1ly5btrvs2bdpUZcqU0bx587Rv3z5dunRJ8fHxyp07typVqqSWLVuqRo0aD9VPShQpUkSrV6/WkiVLtGHDBp08eVIRERHKly+fnnjiiXv2061bN1WtWlXfffedfvvtN129elUWi0V58+bVU089pbZt2yZ5E9MPPvhA9erV07Jly/THH3/oxo0bcnNzU758+fT000/rjTfeUJEiRdL4lQMAAGRMFltK7gwEAAAAIEP46aef1LVrV0m3bjxaoEABkzsCAAAAHBdXpAMAAAAZzPDhw+Xt7a3nnntOFStWTHKfP//8U5Lk6empfPnypWd7AAAAgNMhSAcAAAAyGH9/f+3bt0+HDx/W/PnzE20PDAzUwoULJUlPP/20rFZrercIAAAAOBX+xg0AAABkME2aNJEk7d+/X/369dPBgwcVGBiogIAArV27Vu3atdPVq1fl7u6ugQMHmtwtAAAA4PhYIx0AAADIYBISEjRkyBCtXr36rvtky5ZN48aNU/369dOxMwAAAMA5EaQDAAAAGdTWrVu1bNky/fHHHwoODpaXl5cKFCigevXqqXXr1sqfP7/ZLQIAAABOgSAdAAAAAAAAAIB7YI10AAAAAAAAAADuwdXsBhzVtWs3zW7BFFarRTlzeikwMFwJCfyyA+BseA8AnBfzDzgv5h9wXsw/4Lwcbf7z5Ml23324Ih2pymq1yGKxyGq1mN0KABPwHgA4L+YfcF7MP+C8mH/AeTnj/BOkAwAAAAAAAABwDwTpAAAAAAAAAADcA0E6AAAAAAAAAAD3QJAOAAAAAAAAAMA9EKQDAAAAAAAAAHAPBOkAAAAAAAAAANwDQToAAAAAAAAAAPdAkA4AAAAAAAAAwD0QpAMAAAAAAAAAcA8E6QAAAAAAAAAA3ANBOgAAAAAAAAAA90CQDgAAAAAAAADAPRCkAwAAAAAAAABwDwTpAAAAAAAAAADcA0E6AAAAAAAAAAD3QJAOAAAAAAAAAMA9EKQDAAAAAAAAAHAPBOkAAAAAAAAAANwDQToAAAAAAAAAAPdAkA4AAAAAAAAAwD0QpAMAAAAAAAAAcA8E6QAAAAAAAAAA3ANBOgAAAAAAAAAA90CQDgAAAAAAAADAPbia3QAcT0JCguLj4xUXFy+bzSZJ9/0YExOtmzdv2h9hYaH256Ghobp5M1RhYTfv2CdUWbNmVYECBVWwYCH7x4IFCyp//oLy9PQ04ZUDAAAAAAAAcEQE6Ug1V69eVY8enbVr189KSEgwtRdfX18VKHArWL/9sWDBQvL2ziY3Nze5u7vJzc1dbm5udzzc5e7uJldXN7m7u8vV1U1ubq6KjY1TbGyMYmL+fcTGxig6+tbHO5/f3h4XF6fY2FjFxcUqLi5ecXGx/3weZ98WHx9nr8XHxyshIcHwsNkSZLPZlJBgS1RPSEhQ1qweyps3r/Lmzac8efLan+fNm0+5c+eRu7u7qf8NAAAAAAAAAEdBkI5U88knH2rnzp/MbkOSFBQUpKCgIB09+qfZrZjG19fXHqznyZNHefLkk6enhywWq6zWfx8Wi+Wfj/+tWeTi4iJv72zy9c2pHDl8lTPnrY85cuSQm5ub2S8RAAAAAAAASBcE6Ug1ERERZreAO9z+YcLx48fS5Py3Anbff4J1X/tzX19f+frmVM6ctx+5lDNnTvn65lT27DlktT74rRliYmIUERGu8PBwxcbGSpIsFst9H9Ktj56eHvLy8v6nBgAAAAAAACQPQTpSzdtvv6M9e3bp0qWLD3UeLy9vZcuWTdmyZZOPj4+8vbMpWzYfe83b+9YjIiJcly5d1MWLf//z8aJCQ0NS6dXgfsLCbios7KYCAs4n+xir1XpH0J7LHrR7enoqIiJC4eHh9qA8qee3w/OHYbVa5ePjIx+fHPLx8VH27Nnl45P9n48+dzzPrmzZfOTi4vLPkjq2O5bb+ffjnc9vy5Ur1z/L7bDMDgAAAAAAgCMgSEeqKVHCT4cPH1Ng4GUFBt5UfLztnyuCb23/9ypgi+Fzi8UiV1dX+fj4yMvLWy4uLg/cQ1jYTV26dOmOcP1vXbx4URcvXtClS5cUFRVpX/M8Njb2n8e/z5PDxcVF7u7ucnfP8s966+53PLLIzc31n/XV3eTi4io3N9c7nrvJ1dVVrq6uhu0uLi6yWl2SWHLFkmgpltvbwsLCdPXqFV29ekXXrl375+NVxcXFPfD3L60lJCToxo0bunHjhqSTpvUQHBys4ODgdPuavr6+ypMnr30t+3+f3152J698fXPK19dX3t7ZuGIeAAAAAAAggyFIR6qyWq3y8/NTUFC44uLS/4aj3t7Z5OeXTX5+JVN8rM1mU1xc3D83C41VTMytm4W6uropSxb3f25G6v5QQX9aS0hIUFBQkK5du2oP2a9evWr/PDY2xn7z0n+vrE5IdGX1rYdN8fFxCg0NVXDwrWViwsPDzH6JmdLtZXZOnDh+333d3NwMS+T8+zGnoebjk10eHp7y8PD4z8NTWbNmJYwHAAAAAABIRQTpwD8sFovc3Nwy9U00rVarcuXKpVy5cql06TKpfv6YmBgFBwcrKChQQUFBCg4OsofswcGB9sA4MDBQQUGBCgy8oaCgQEVGRqZ6L44qNjZW167d+uHHw7gdrGfN+m/A7unpKS8vL3l5ef/z8f7P8+bNp3z58mfoHyABAAAAAACkNYJ0AMnm7u6uvHlvLU+SEhEREf8E6/8G7IGBxqD9doDr6ektLy9Pe5B7K/xN/NzNzf2fdctthvXLk37c+o2DiIhwhYSE6ObNUIWEhCgkJPiO5//WQ0P//dxms/2zzI7V/vH2c4vF+p9tFsXFxevGjeumL7MTGRmZaj/AcHNzU6FCj6hw4cf06KOPqnDh249bnxO0AwAAAAAAR0eQDiDNeXreuhq6UKFHTO4kd7p9pVtrsQcZ1q//73r2t5/fuHE9Q69tHxsbK3//s/L3P5vk9sRBe0Flzeqt7NlzKEcOX+XMmdOwXE2WLFnS+RUAAAAAAAA8HIJ0AEgDVqtVOXPmUs6cuVSqVOl77muz2RQWdvOfpXFuXbkfHBxk/3h7KZ1bH29tDwsL++eq8wjFx8en06tK2v2C9v/y9PRUjhy+9pA9Z85cKlz4UT32WBH745FHCsvd3T2NOwcAAAAAAEgegnQAMJnFYlG2bD7Kls1Hjz76WIqPj42NVWRkhH05l9sBe1RUlCIjIxQREfnPxwiFh4crPDzsn4/G5xERxvrNmzcVGxub6q83IuJWLxcv/n3XfaxWqwoVekRFihQ1BOy3H76+ObmhKgAAAAAASDcE6QCQyd26SW52+fhkT9XzJiQk6Nq1awoIOKcLFwJ0/vx5BQScV0DAuX8+nldUVFSqfs07v/btr7Fz545E2729s6lgwYIqUKCgChYspAIFbj8vqAIFCqlgwYKE7QAAAAAAINUQpAMAkmS1WpUvXz7ly5dPVao8lWi7zWbT9evX7cH6+fPndfny3woPv6nLl6/esSRNkEJCglO1t7Cwmzpx4rhOnDh+132yZs2q/PkLqGDBQsqfv4AeffQx+fmVVMmSpVSiREl5eXmlak8AAAAAAMBxEaQDAB6IxWJRnjx5lCdPHlWqVEWS5Opqla+vl4KCwhUXl2DfNz4+XiEhwf9Z+/3Wuu9XrlzR+fPndO7crXXWg4ODU6W/qKioe67d/sgjhe3BesmSpeXnV0olS5ZUzpy5UuXrAwAAAAAAx0GQDgBIcy4uLvabrxYrdu99g4OD/gnW/XX27FmdO+f/z+OsLlwISLWbq164EKALFwK0bduPhnru3Lnl51dKfn6llD9/fnl6esnL69Yj6efe8vT0lIeHB0vJAAAAAADgoAjSAQAZSo4cvsqRw1dPPFEh0ba4uDhduBCgv/++oIsX/9alS5d06dLfunjxov3jtWtXZbPZHvjrX79+XdevX9eePbtSdJzFYpG3dzY99lgRlSlTVmXKlFPZsmVVtuzjypcvPyE7AAAAAACZGEE6ACDTcHV1VZEiRVWkSNG77hMbG6srVy7bw/VLly7q77//1pkzp3TixHGdP3/uoYL2u7HZbLp5M1R//vmH/vzzD8M2X19flSlTzh6w3/pYVt7e2VK9DwAAAAAAkPoI0gEADsXNzU2PPFJYjzxSOMntkZGROnXqpE6evHWz0pMnT+jkyeM6ffqUYmNj06SnoKAg7d79s3bv/tlQf/TRx1SmTFk9+WRFVapURZUqVVaOHL5p0gMAAAAAAHhwBOkAAKfi4eGh8uWfUPnyTxjqcXFxOnfurE6c+DdYDwkJUUREuMLDbz3++/xhnT9/TufPn9PGjevttWLFiqtSpSqqXLmKKlWqonLlysvd3f2hvxYAAAAAAHhwFlta/H47dO3aTbNbMIWrq1W+vl4KCgpXXFyC2e0ASGfO9B6QkJCgyMhIQ8AeGHhDx4//pb/+OqqjR4/o2LG/FB4e9lBfx93dXeXLP/HPFeu3HkWKFGXNdWQ4zjT/AIyYf8B5Mf+A83K0+c+T5/5Lr3JFOgAAD8BqtcrLy0teXl6Geq1adezPExISFBBwXn/9dVR//XXkn8dRnTp1UvHx8cn6OjExMTpw4FcdOPCrvZYzZ05VrlxVVao8pSpVnlLFipXl7e2dOi8MAAAAAAAkwhXpaYQr0h3jp1EAUob3gOSJjo7WiRPH9ddfR3T48CEdOPCrDh8+pOjo6Ac6n9VqVZky5f4J1quqatWnVLRoca5aR7pi/gHnxfwDzov5B5yXo81/cq5IJ0hPIwTpjjFEAFKG94AHFxMTo7/+OqIDB37Vb7/9qoMHD+jkyRMPfL5cuXIZrlqvUKESV60jTTH/gPNi/gHnxfwDzsvR5p8g3UQE6Y4xRABShveA1BUSEqyDB3/Tb7/9an9cv379gc5ltVpVqlRpVaxYWRUqVFKlSpVVpkw5ubm5pXLXcFbMP+C8mH/AeTH/gPNytPknSDcRQbpjDBGAlOE9IG3ZbDYFBJzXr7/usz/+/POw4uLiHuh8WbJk0eOPl78jXK+iYsWKy2q1pnLncAbMP+C8mH/AeTH/gPNytPnnZqMAADgQi8WiRx99TI8++piaNWspSYqMjNShQwe1f/+/4fq1a1eTdb7o6OhENzLNls1HFSpUtIfrFStWUsGChVhvHQAAAADg1LgiPY1wRbpj/DQKQMrwHmC+1L5qXZLy5MmrihUr2YP1ChUqK1euXKnYNRwB8w84L+YfcF7MP+C8HG3+WdrFRATpjjFEAFKG94CMKTIyUocP/6Hffz+ggwd/08GDB3TmzOmHOuejjxYxhOtPPPGkvL3v/xcPOC7mH3BezD/gvJh/wHk52vwTpJuIIN0xhghAyvAekHkEBwfp998P6vfff7OH65cvX3rg81ksFpUsWUo1ajyj555rpJo16yhr1qyp2DEyOuYfcF7MP+C8mH/AeTna/BOkm4gg3TGGCEDK8B6QuV26dFEHD/6m33//Tb/9dkCHDh1USEjwA53L09NTtWvX1XPPvaCGDZ9Xvnz5U7dZZDjMP+C8mH/AeTH/gPNytPknSDcRQbpjDBGAlOE9wLHYbDadPXvaHq4fPPibDh8+pMjIyBSfq0KFinruuRf03HONVL78k9y81AEx/4DzYv4B58X8A87L0eafIN1EBOmOMUQAUob3AMcXFxenY8f+sgfrv//+m/7660iKbmZaoEBBNWzYSM8997xq1aorDw+PNOwY6YX5B5wX8w84L+YfcF6ONv8E6SYiSHeMIQKQMrwHOKfIyEgdOXJYu3fv0ubNG7R//14lJCTvv7+Hh4dq1qyt+vWfVb16z6pYseJp3C3SCvMPOC/mH3BezD/gvBxt/gnSTUSQ7hhDBCBleA+AJN24cUNbt27Wpk0btHXrFt28GZrsYx97rIjq139W9es31DPP1JK3t3cadorUxPwDzov5B5wX8w84L0ebf4J0ExGkO8YQAUgZ3gPwXzExMdq7d482bVqvjRvXy9//bLKPdXNzU/XqT6tevWdVv/6zKlOmLGurZ2DMP+C8mH/AeTH/gPNytPknSDcRQbpjDBGAlOE9APdis9l06tRJbdy4Xps3b9DevXuSvQSMJOXPX0D16jVQ/frPqkGD57haPYNh/gHnxfwDzov5B5yXo80/QbqJCNIdY4gApAzvAUiJoKBA7dixTVu3btHWrVt09eqVZB/r6empRo1eVIsWrVSnTn25ubmlYadIDuYfcF7MP+C8mH/AeTna/BOkm4gg3TGGCEDK8B6AB2Wz2XTkyJ/aunWLtm3bor179yguLi5Zx+bOnVuvvNJcLVq0VsWKlVn+xSTMP+C8mH/AeTH/gPNytPknSDcRQbpjDBGAlOE9AKklLOymfv55p7Zu3aytW7fo/PlzyTquaNFiatGitZo3b6VixYqncZe4E/MPOC/mH3BezD/gvBxt/gnSTUSQ7hhDBCBleA9AWrDZbDpz5pS2bt2iLVs26aeftis+Pv6+x1WuXEUtWrTWyy83V+7cudOhU+fG/APOi/kHnBfzDzgvR5t/gnQTEaQ7xhABSBneA5Aerl+/ru+/X65ly5bowIH9993fxcVF9eo10IsvNlXDho2UN2/edOjS+TD/gPNi/gHnxfwDzsvR5p8g3UQE6Y4xRABShvcApLczZ05r+fIlWr58ic6cOX3f/S0WiypXrqpGjV7UCy+8qBIl/FhTPZUw/4DzYv4B58X8A87L0eafIN1EBOmOMUQAUob3AJjFZrPp4MEDWrZssVatWq7r168n67hixYqrUaMX1ajRi6pa9Sm5uLikcaeOi/kHnBfzDzgv5h9wXo42/wTpJiJId4whApAyvAcgI4iNjdVPP23T0qWLtX79D4qMjEzWcbly5VLDho3UqNGLqlOnnry8vNK4U8fC/APOi/kHnBfzDzgvR5t/gnQTEaQ7xhABSBneA5DRhIWFaePGdVq/fq1+/HGzwsPDknVc1qxZVbduA3Xt2l01a9Zm+ZdkYP4B58X8A86L+Qecl6PNP0G6iQjSHWOIAKQM7wHIyKKjo7Vr10/asGGdNm5cr0uXLibruCeeqKDevfvppZdelquraxp3mXkx/4DzYv4B58X8A87L0eafIN1EBOmOMUQAUob3AGQWNptNhw4d1IYNa7Vhw3odPfrnfY959NHH1L17L732WnuWfUkC8w84L+YfcF7MP+C8HG3+kxOkW9Ohj2S7ceOGevbsqSpVqqhatWoaM2aM4uLiktx3x44datKkiSpUqKAXXnhB27ZtM2yfPXu2ateurQoVKqh9+/Y6c+aMfduFCxfUu3dvVa9eXdWqVVPPnj0VEBBg3z5r1iyVK1dOFStWtD8mTpyYNi8aAACkO4vFogoVKumdd4Zr+/bd2r//D40e/Ylq1qx91xuOnj9/TsOGDValSmX1ySejde3atXTuGgAAAABglgwVpPfv31+enp7auXOnli1bpj179uibb75JtJ+/v7/69Omjfv366ddff1WfPn3Uv39/XblyRZK0cuVKzZs3T1999ZX27t2rcuXKqW/fvrp98X2vXr2UPXt2bd26VVu3blWOHDnUs2dP+/n//PNP9ejRQwcPHrQ/BgwYkC7fAwAAkP4ee6yIunXrqRUrftBff53Rxx+P12OPFUly36CgIH322ThVrlxOgwYN0Jkzp9O3WQAAAABAusswQfq5c+e0b98+DRo0SB4eHipcuLB69uypBQsWJNp35cqVqlKlip599lm5urqqcePGqlq1qhYvXixJWrJkidq2bSs/Pz9lyZJFb731li5evKi9e/cqJCREuXPnVr9+/eTp6SkvLy+98cYbOnHihEJCQiRJhw8f1uOPP56urx8AAGQMOXL4qkuXbvrll4P68su5qlixUpL7RUVFae7cr1SjRiV16vS6DhzYn86dAgAAAADSS4a5Y9bJkyeVI0cO5cuXz14rXry4Ll68qNDQUPn4+Njrp06dUsmSJQ3HlyhRQseOHbNv79q1q32bm5ubihQpomPHjql69er66quvDMdu3LhRhQoVUvbs2XXjxg1dvHhRS5Ys0XvvvSd3d3c1atRI/fr1U5YsWZL9eqxWi6xWS4q+B47AxcVq+AjAufAeAEfi6mpVs2bN9eqrzbRr105NmfK5Nm/emGg/m82mtWtXa+3a1apR42n16tVXzz//wl2XiHFUzD/gvJh/wHkx/4Dzcsb5zzBBenh4uDw8PAy1259HREQYgvSk9s2aNasiIiKStf1OCxcu1Jw5c/TFF19Ikq5du6YqVaqoWbNmmjRpkgICAtS/f39FRkZqxIgRyX49OXN6yWJxviD9Nh8fj/vvBMBh8R4AR9OkyQtq0uQFHTlyROPHj9eCBQsUGxubaL89e3Zrz57dKlGihAYMGKCOHTvK09PThI7Nw/wDzov5B5wX8w84L2ea/wwTpHt6eioyMtJQu/25l5eXoe7h4aGoqChDLSoqyr7f/bZLUkxMjD7++GOtW7dOM2fOVPXq1SVJpUuXNiwnU7x4cfXs2VMffPBBioL0wMBwp70i3cfHQ6GhkYqPz/x37AWQMrwHwNEVLFhEn302VW+/PUwzZ07XN9/M0c2boYn2O3XqlHr16qX33huuzp27qEuX/yl//vwmdJx+mH/AeTH/gPNi/gHn5Wjz7+vrdd99MkyQ7ufnp+DgYF2/fl25c+eWJJ0+fVr58+dXtmzZDPuWLFlSR44cMdROnTplX9fcz89PJ0+eVL169SRJsbGx8vf3ty8HExgYqB49eigmJkbLli1T4cKF7efZt2+fDh48qP/973/2WkxMjLJmzZqi15OQYFNCgi1FxziS+PgExcVl/iEC8GB4D4Cjy5s3v4YPH6V+/d7St99+o1mzpuvy5UuJ9gsKCtSECZ9qypTP1axZS3Xv3ltly5YzoeP0w/wDzov5B5wX8w84L2ea/wyziE2RIkVUuXJlffTRRwoLC1NAQICmT5+uFi1aJNq3adOm2rdvn9atW6e4uDitW7dO+/bt08svvyxJat68uebPn69jx44pOjpaEyZMUO7cuVWlShXFxsbqzTfflLe3txYuXGgI0aVbV7NPmTJFa9asUUJCgk6ePKnp06erdevW6fJ9AAAAmYePT3b17t1Pv/56WFOnzlS5cuWT3C8mJkaLFi1Q3bo11LLly9q6dYtsNuf9gTsAAAAAZDYWWwb6V9z169c1atQo7d27V1arVa+88orefvttubi4qGLFiho5cqSaNm0qSdq5c6fGjx+v8+fPq1ChQho0aJDq1Kkj6dZNv77++mstWLBAgYGBKl++vEaOHKmiRYtq06ZN6tOnj7JkyZLoJmBr165VwYIFtWnTJk2bNk3nz59XtmzZ1KpVK/Xs2VNWa/J/7nDt2s3U+8ZkIq6uVvn6eikoKNxpfhoF4F+8B8DZ2Ww2/fzzT/riiynasmXTPfctXbqMunfvrebNW6XohuYZFfMPOC/mH3BezD/gvBxt/vPkyXbffTJUkO5ICNIdY4gApAzvAcC/jh8/plmzpmvJkoWKjo6+63758uVXnz791b59p0Q3S89MmH/AeTH/gPNi/gHn5Wjzn5wgPcMs7QIAAOBISpUqrQkTJuu3345q0KCh9nvA/NeVK5f13nvvqEqV8vrii6mKiIhI504BAAAAAPdDkA4AAJCG8uTJo0GDhurAgSOaMGGy/PxKJrnftWtXNWLEMFWpUl5Tp36usLCwdO4UAAAAAHA3BOkAAADpwMPDQ+3bd9TOnfv03XdLVbNm7ST3u379mkaNGq6qVctr8uTPFBbmnMvFAQAAAEBGQpAOAACQjqxWq5599nmtWPGD1qzZpLp16ye5340bNzR69AeqXPlxTZz4qUJDQ9K5UwAAAADAbQTpAAAAJqlWrbqWLFmldeu2qEGDhknuExQUpI8//lCVK5fX+PGfKCQkOH2bBAAAAAAQpAMAAJitSpWntHDhcm3cuE3PPdcoyX1CQoI1btxHqlTpcY0b95Fu3gxN5y4BAAAAwHkRpAMAAGQQFStW1vz5S7Rly0964YWXktzn5s1QjR//if2mpBEREencJQAAAAA4H4J0AACADOaJJypo7tzv9OOPP+ull15Ocp+goCCNGjVcTz31pL76apaio6PTuUsAAAAAcB4E6QAAABlU+fJPaM6cedq+fY9efrmZLBZLon2uXr2ioUPf1tNPV9bChfMVFxdnQqcAAAAA4NgI0gEAADK4smXLafbsb7R9+x41btwkyX0CAs6rX7+eql27mlatWq6EhIR07hIAAAAAHBdBOgAAQCZRpkxZffPNAm3atF316jVIcp9Tp06qW7dOatCgljZuXC+bzZbOXQIAAACA4yFIBwAAyGQqVKikxYtX6vvv16tatRpJ7nPkyGG1b99ajRs/q59+2p6+DQIAAACAgyFIBwAAyKRq1HhGq1dv0KJFy/XkkxWT3OfAgf1q0aKpXn+9lQICzqdzhwAAAADgGAjSAQAAMjGLxaL69Rtq06btmjNnvkqVKp3kfps2bVCtWtX0xRdTuSEpAAAAAKQQQToAAIADsFgseumlptq+fY+mTZulxx4rkmifiIhwjRgxTI0a1dehQwfTv0kAAAAAyKQI0gEAAByIi4uLWrZso927D2j8+M+VJ0/eRPv88cfvev75eho+/B2FhYWZ0CUAAAAAZC4E6QAAAA7Izc1Nb7zRSbt27dcbb3ROtD0hIUEzZ05XrVpPaePG9SZ0CAAAAACZB0E6AACAA8uRw1fjx0/SmjWbklw//e+/L6h9+9bq3Lm9Ll++ZEKHAAAAAJDxEaQDAAA4gWrVquvHH3/W0KHDlSVLlkTbf/jhez39dBV99dUsxcfHm9AhAAAAAGRcBOkAAABOwt3dXQMGDNKOHXtUq1adRNvDwm5q6NC39dJLDXXkyJ8mdAgAAAAAGRNBOgAAgJMpVqyEli1brSlTZihnzpyJth848KsaNqytDz54T2FhN03oEAAAAAAyFoJ0AAAAJ2SxWNS6dVvt2nVArVu3TbQ9Li5O06dP1jPPVNX336+QzWYzoUsAAAAAyBgI0gEAAJxYrly5NGXKDK1Y8YOKFSueaPulSxfVtWtHtWz5ik6dOmlChwAAAABgPoJ0AAAAqGbN2tq+fY8GDhwsd3f3RNt/+mmb6tSprjFjRio8PNyEDgEAAADAPATpAAAAkCRlzZpV77zznnbs2KM6deol2h4bG6vPP5+gWrWe0tq1a1juBQAAAIDTIEgHAACAQfHiflqyZJW++upbFSxYKNH2CxcC1KlTO7Vt20Jnzpw2oUMAAAAASF8E6QAAAEjEYrGoSZNX9PPP+9W7d3+5urom2ufHHzerTp3qGjt2jCIjI03oEgAAAADSB0E6AAAA7srb21vvvz9K27fvUc2atRNtj46O1oQJY/X001X1ww8/mNAhAAAAAKQ9gnQAAADcV8mSpbR8+RrNmPGV8uXLn2j7uXP+atKkiQYM6KOoqCgTOgQAAACAtEOQDgAAgGSxWCxq1qyldu/+Vf/7Xy+5uLgk2mfu3K/VpMnzOnfOP/0bBAAAAIA0QpAOAACAFMmWzUcffvixfvzxZ1Wv/nSi7YcOHVTDhrW1efMGE7oDAAAAgNRHkA4AAIAHUrZsOX3//XpNmjRNHh4ehm3BwcFq166VPv54lOLj403qEAAAAABSB0E6AAAAHpjFYlHbtu21efN2lSxZMtH2iRPHq1WrV3Tt2jUTugMAAACA1EGQDgAAgIdWtmw57d+/X02bvpJo286dO9SgQU3t3ftL+jcGAAAAAKmAIB0AAACpwsfHR19/PU+jR38iV1dXw7bLly/p1Vcba8aMqbLZbCZ1CAAAAAAPhiAdAAAAqcZisahbt55atWq9ChQoaNgWFxen998fpjff7KCbN0NN6hAAAAAAUo4gHQAAAKnuqaeqacuWnapVq26ibWvWrNJzz9XV0aNH0r8xAAAAAHgABOkAAABIE3ny5NGSJSs1cOCgRNtOnz6lF16oryVLFprQGQAAAACkDEE6AAAA0oyLi4veeWe4vvtuqXLkyGHYFhkZqd69/6d33x2s2NhYcxoEAAAAgGQgSAcAAECae/bZ57Vly05VqFAx0bbZs2eoVatXdP36dRM6AwAAAID7I0gHAABAunj00ce0Zs0mdejQJdG2Xbt26vnn6+rw4UMmdAYAAAAA90aQDgAAgHSTJUsWffrpRE2e/IWyZMli2BYQcF4vvfScVqxYalJ3AAAAAJA0gnQAAACkuzZt2mn16g0qUKCgoR4ZGanu3bto1Kj3FR8fb1J3AAAAAGBEkA4AAABTVKxYWZs27dBTT1VPtG3q1El67bXmCg4OMqEzAAAAADAiSAcAAIBp8uXLpxUrfkhy3fTt27fquefq6tixv0zoDAAAAAD+RZAOAAAAU7m7u+vTTydq/PjP5ebmZtjm739WjRrV19q1a0zqDgAAAAAI0gEAAJBBvPFGJ61YsVZ58uQ11CMiwtWpUzuNHTtGCQkJJnUHAAAAwJkRpAMAACDDqFaturZs+UkVK1ZKtG3ChLHq0OE13bwZakJnAAAAAJwZQToAAAAylAIFCur77zeodeu2ibZt3LhejRrV18mTJ0zoDAAAAICzIkgHAABAhpM1a1ZNnvyFxowZKxcXF8O2kydP6Pnn62n9+rUmdQcAAADA2RCkAwAAIEOyWCzq2rWHli79Xjlz5jRsCwu7qQ4dXtMnn4xm3XQAAAAAaY4gHQAAABlazZq1tWnTDj3++BOJtn322Ti9/norhYQEp39jAAAAAJwGQToAAAAyvEcffUw//LBJLVq0TrRty5ZNeu65uvrrr6MmdAYAAADAGRCkAwAAIFPw9PTUtGmzklw3/ezZM3rhhQb6/vsVJnUHAAAAwJERpAMAACDTuL1u+vLla5Q7d27DtoiIcHXt2lGjRr2vuLg4kzoEAAAA4IgI0gEAAJDpPP10TW3e/JMqVqyUaNvUqZPUpk1zBQbeMKEzAAAAAI6IIB0AAACZUqFCj+j77zeoXbs3Em376adteu65ujp8+JAJnQEAAABwNATpAAAAyLSyZs2qzz6bok8/nSQ3NzfDtvPnz+nFFxtq6dJFJnUHAAAAwFEQpAMAACBTs1gs6tChs1atWqd8+fIbtkVFRalXr256//1hSkhIMKlDAAAAAJkdQToAAAAcQtWq1bRly0+qWrVaom0zZkxV9+6dFR0dbUJnAAAAADI7gnQAAAA4jHz58mvlyrXq1OnNRNtWrVqhtm1b6ObNUBM6AwAAAJCZEaQDAADAobi7u2vs2M80adK0ROum79y5Qy+/3FhXrlwxqTsAAAAAmRFBOgAAABxS27bttWDBUnl5eRvqf/75h158saFOnz5pUmcAAAAAMhuCdAAAADisunXra9WqtcqdO4+hfv68v1566TkdPHjApM4AAAAAZCYE6QAAAHBoTz5ZUWvXblaRIkUN9Rs3bujVV1/U1q2bTeoMAAAAQGZBkA4AAACHV7RoMf3ww2Y9+WRFQz0iIkKvv95aixd/Z1JnAAAAADIDgnQAAAA4hbx582rlyh9Up049Qz0uLk59+nTXlCmTZLPZTOoOAAAAQEZGkA4AAACn4e2dTQsWLFXz5q0Sbfvww/c1fPg7SkhIMKEzAAAAABkZQToAAACciru7u6ZNm6UePfok2jZr1hfq3r2zoqOjTegMAAAAQEZFkA4AAACnY7VaNXLkGH3wwZhE21atWqG2bVvo5s1QEzoDAAAAkBERpAMAAMBp9ezZR1988aXc3NwM9Z07d6hFi6YKDQ0xqTMAAAAAGQlBOgAAAJxa8+attGDBUnl5eRvqBw/+ptatm3FlOgAAAACCdAAAAKBu3fpatWqtcufOY6gfOLBfbdo0V1jYTZM6AwAAAJAREKQDAAAAkp58sqJWr96gvHnzGer79+/Va6+1UFhYmEmdAQAAADAbQToAAADwjxIl/LRixQ/Kkyevob537x61a9dS4eHhJnUGAAAAwEwE6QAAAMAdSpYspeXL1yh37tyG+p49u9S+fWtFRESY1BkAAAAAsxCkAwAAAP9RunQZLVu2Rrly5TLUf/75J73xxmuKjIw0qTMAAAAAZiBIBwAAAJJQtmw5LV26Wr6+vob6Tz9tU4cOrykqKsqkzgAAAACkN4J0AAAA4C4ef7y8li1brRw5chjq27dvVadO7RQdHW1OYwAAAADSFUE6AAAAcA/lyz+ppUu/l49PdkP9xx83q3Pn1wnTAQAAACdAkA4AAADcx5NPVtTSpauULZuPob5580Z17dpBMTExJnUGAAAAID0QpAMAAADJULFiZS1evELe3tkM9Q0b1qlbt06KjY01qTMAAAAAaY0gHQAAAEimKlWe0qJFK+Tl5W2or1u3Rt27dyFMBwAAABwUQToAAACQAk89VU0LFy6Xp6eXob5mzSr16dNdCQkJJnUGAAAAIK0QpAMAAAApVL16DX333VJ5enoa6itWLNWQIW/JZrOZ1BkAAACAtECQDgAAADyAp5+uqfnzl8jDw8NQnzv3K40a9T5hOgAAAOBACNIBAACAB1SzZm19/fUCubm5GerTpn2uSZPGm9QVAAAAgNRGkA4AAAA8hPr1n9WMGXNktRr/av3xxx/qyy9nmNQVAAAAgNREkA4AAAA8pCZNXtakSdMS1YcNG6xFixaY0BEAAACA1ESQDgAAAKSCNm3a6aOPxiWq9+/fS2vWfG9CRwAAAABSC0E6AAAAkErefLO7hg4dbqglJCSoe/fO2rp1i0ldAQAAAHhYBOkAAABAKurf/2316tXPUIuNjVWnTu30yy+7TeoKAAAAwMMgSAcAAABSkcVi0fvvj1KHDl0M9cjISLVr10qHDh00qTMAAAAAD4ogHQAAAEhlFotFY8dOULNmLQ31mzdD1br1qzp+/JhJnQEAAAB4EATpAAAAQBqwWq2aMmWGGjVqbKgHBgaqZcuXde6cvzmNAQAAAEgxgnQAAAAgjbi5uWnWrG9Uq1ZdQ/3y5Utq3rypLl26aE5jAAAAAFKEIB0AAABIQ1mzZtXcud+pcuWqhvr58/5q2fJl3bhxw6TOAAAAACQXQToAAACQxry9vbVw4TKVLfu4oX7ixHG1b99aERERJnUGAAAAIDkI0gEAAIB0kCOHr5YsWaXixUsY6r/+uk/du3dWXFycSZ0BAAAAuB+CdAAAACCd5M2bV0uXfq+CBQsZ6hs2rNM777wtm81mUmcAAAAA7oUgHQAAAEhHjzxSWIsWrVD27DkM9W+/naNJk8ab0xQAAACAeyJIBwAAANJZ6dJl9O23C+Xu7m6of/zxh1q0aIFJXQEAAAC4G4J0AAAAwAQ1ajyj6dNny2KxGOoDBvTWjz9uMqkrAAAAAEkhSAcAAABM0rTpqxo9+hNDLT4+Xl26dNDvv/9mUlcAAAAA/osgHQAAADBR16491KtXP0MtIiJcbdu21NmzZ0zqCgAAAMCdCNIBAAAAkw0fPlLNmrU01K5fv6Y2bZrp+vXrJnUFAAAA4DaCdAAAAMBkVqtVkyd/oVq16hrqZ8+e0euvt1R4eLg5jQEAAACQlMGC9Bs3bqhnz56qUqWKqlWrpjFjxiguLi7JfXfs2KEmTZqoQoUKeuGFF7Rt2zbD9tmzZ6t27dqqUKGC2rdvrzNn/v212AsXLqh3796qXr26qlWrpp49eyogIMC+/ezZs+rQoYMqVqyomjVrasaMGWnzggEAAIB/uLu765tv5qtcufKG+m+/HVC3bh3v+vdiAAAAAGkvQwXp/fv3l6enp3bu3Klly5Zpz549+uabbxLt5+/vrz59+qhfv3769ddf1adPH/Xv319XrlyRJK1cuVLz5s3TV199pb1796pcuXLq27evbDabJKlXr17Knj27tm7dqq1btypHjhzq2bOnJCk2Nlbdu3dX+fLltXfvXs2aNUsLFizQ+vXr0+37AAAAAOeULZuPFi5cpsKFHzXUN2/eqMGDB9j/PgsAAAAgfWWYIP3cuXPat2+fBg0aJA8PDxUuXFg9e/bUggULEu27cuVKValSRc8++6xcXV3VuHFjVa1aVYsXL5YkLVmyRG3btpWfn5+yZMmit956SxcvXtTevXsVEhKi3Llzq1+/fvL09JSXl5feeOMNnThxQiEhIdq/f7+uXr2qvn37yt3dXWXLllX79u2T7AMAAABIbfnzF9CiRSvk6+trqM+fP1fjx39iUlcAAACAc3M1u4HbTp48qRw5cihfvnz2WvHixXXx4kWFhobKx8fHXj916pRKlixpOL5EiRI6duyYfXvXrl3t29zc3FSkSBEdO3ZM1atX11dffWU4duPGjSpUqJCyZ8+ukydPqmjRonJ3dzece9asWSl6PVarRVarJUXHOAIXF6vhIwDnwnsA4LyY/9RVpkxpLViwRM2aNVFUVJS9/umnH6tQoUJ6442O5jUH/AfzDzgv5h9wXs44/xkmSA8PD5eHh4ehdvvziIgIQ5Ce1L5Zs2ZVREREsrbfaeHChZozZ46++OKLe/aR1LH3kjOnlywW5wvSb/Px8bj/TgAcFu8BgPNi/lPPCy88q4ULF6p58+ZKSEiw1wcO7KtSpYqrUaNGJnYHJMb8A86L+QeclzPNf4YJ0j09PRUZGWmo3f7cy8vLUPfw8DBcmSNJUVFR9v3ut12SYmJi9PHHH2vdunWaOXOmqlevfs8+/tvD/QQGhjvtFek+Ph4KDY1UfHzC/Q8A4FB4DwCcF/OfNurUaahx4ybo7bcH2GsJCQlq2bKVNmzYrLJlHzexO+AW5h9wXsw/4Lwcbf59fe+f/WaYIN3Pz0/BwcG6fv26cufOLUk6ffq08ufPr2zZshn2LVmypI4cOWKonTp1So8//rj9XCdPnlS9evUk3bqBqL+/v305mMDAQPXo0UMxMTFatmyZChcubOjD399fcXFxcnV1tZ/bz88vRa8nIcGmhATnvRlUfHyC4uIy/xABeDC8BwDOi/lPfW+80UUXLvytSZPG22thYTfVpk1LrV+/1bA0ImAm5h9wXsw/4Lycaf4zzCI2RYoUUeXKlfXRRx8pLCxMAQEBmj59ulq0aJFo36ZNm2rfvn1at26d4uLitG7dOu3bt08vv/yyJKl58+aaP3++jh07pujoaE2YMEG5c+dWlSpVFBsbqzfffFPe3t5auHChIUSXpGrVqsnX11cTJkxQdHS0jh07pnnz5iXZBwAAAJAehg4drmbNjH8fvXAhQG+80TrFSxACAAAASDmLzWbLMJdNX79+XaNGjdLevXtltVr1yiuv6O2335aLi4sqVqyokSNHqmnTppKknTt3avz48Tp//rwKFSqkQYMGqU6dOpIkm82mr7/+WgsWLFBgYKDKly+vkSNHqmjRotq0aZP69OmjLFmyyMXFxfD1165dq4IFC+rcuXMaNWqUDh06JE9PT73++uvq1q1bil7LtWs3U+ebksm4ulrl6+uloKBwp/lpFIB/8R4AOC/mP+1FRUWpefMm2r9/r6H+0ksv68sv58pqzTDXyMDJMP+A82L+AeflaPOfJ0+2++6ToYJ0R0KQ7hhDBCBleA8AnBfznz6uX7+uRo3q6/x5f0O9T58BGj58pDlNwekx/4DzYv4B5+Vo85+cIJ3LVgAAAIBMInfu3Pruu6Xy8cluqE+ZMlHz5881qSsAAADA8RGkAwAAAJlIyZKlNGfOPLm6uhrqgwcP0E8/bTenKQAAAMDBEaQDAAAAmUzt2nX16aeTDLW4uDh17txeJ04cN6cpAAAAwIERpAMAAACZULt2b6hPnwGGWmhoiNq2balr166Z1BUAAADgmAjSAQAAgEzq3XdH6KWXXjbUzp/3V4cOrykqKsqkrgAAAADHQ5AOAAAAZFJWq1VTp85UpUqVDfVff92nfv16KCEhwaTOAAAAAMdCkA4AAABkYp6enpo7d5EeeaSwob5y5XKNG/eRSV0BAAAAjoUgHQAAAMjk8uXLpwULlsrbO5uh/tln47R48XcmdQUAAAA4DoJ0AAAAwAGUKVNWX345Vy4uLob6wIF9tGfPLpO6AgAAABwDQToAAADgIOrXf1YfffSpoRYbG6uOHdvq3Dl/c5oCAAAAHABBOgAAAOBAOnV6U//7Xy9DLSgoSB07tlNERIRJXQEAAACZG0E6AAAA4GA++GC0GjVqbKgdOXJYAwb0ks1mM6krAAAAIPMiSAcAAAAcjIuLi6ZPn62SJUsZ6itXLtcXX0w1qSsAAAAg8yJIBwAAAByQt3c2zZ37nXx8shvqo0YN144d20zqCgAAAMicCNIBAAAAB1W8uJ+++GK2LBaLvZaQkKBu3Tpy81EAAAAgBQjSAQAAAAfWsGEjDRnyrqHGzUcBAACAlCFIBwAAABxc//5v64UXXjLUjhw5rIEDe3PzUQAAACAZCNIBAAAAB2e1WjVt2sxENx9dsWIZNx8FAAAAkoEgHQAAAHACt28+mi2bj6HOzUcBAACA+yNIBwAAAJwENx8FAAAAHgxBOgAAAOBEnnvuBQ0ePMxQCwoKUqdOr3PzUQAAAOAuCNIBAAAAJzNgwKBENx/9888/uPkoAAAAcBcE6QAAAICTudfNR2fMmGZSVwAAAEDGRZAOAAAAOCFv72z65pvENx8dOfI9/fTTdnOaAgAAADIognQAAADASZUocfebj54/f87EzgAAAICMhSAdAAAAcGJJ3Xw0MDBQnTq9rujoaJO6AgAAADIWgnQAAADAySV189HDhw9p9OgRJnUEAAAAZCwE6QAAAICTs1qtmjp1hvz8ShrqM2dO1+bNG0zqCgAAAMg4CNIBAAAAKFs2H82ePVdZs2Y11Pv27aHLly+Z1BUAAACQMRCkAwAAAJAklS1bTiNHfmSo3bhxQ716dVN8fLxJXQEAAADmI0gHAAAAYNexYxc1btzEUNu5c4emTJloUkcAAACA+QjSAQAAANhZLBZNnDhFhQo9YqiPHTtG+/fvNakrAAAAwFwE6QAAAAAMfH1z6osvvpTV+u8/F+Lj49W9exeFhASb1xgAAABgEoJ0AAAAAIlUr/603npriKEWEHBeb73VTzabzaSuAAAAAHMQpAMAAABI0sCBg1WjxjOG2urVKzV//lyTOgIAAADMQZAOAAAAIEkuLi764osv5evra6i/994QHT9+zKSuAAAAgPRHkA4AAADgrgoWLKRJk6YbapGRkerWrZMiIyNN6goAAABIXwTpAAAAAO7phRdeVOfOXQ21v/46og8+eNekjgAAAID0RZAOAAAA4L4++GCMypZ93FD7+usvtW7dDyZ1BAAAAKQfgnQAAAAA95U1a1bNmvW1PDw8DPX+/Xvq778vmNQVAAAAkD4I0gEAAAAkS8mSpTRmzDhDLTg4WD16vKm4uDiTugIAAADSHkE6AAAAgGRr1+4NvfxyM0Ptl19267PPxt3lCAAAACDzI0gHAAAAkGwWi0Xjx0/So48+Zqh/9tk47dmzy6SuAAAAgLRFkA4AAAAgRbJnz6EvvvhSLi4u9lpCQoK6d++iGzdumNgZAAAAkDYI0gEAAACkWNWq1fTOO+8ZapcuXVTfvt2VkJBgUlcAAABA2iBIBwAAAPBA+vQZoNq16xlqmzdv1IwZ00zqCAAAAEgbBOkAAAAAHojVatW0abOUJ09eQ3306BE6cGC/SV0BAAAAqY8gHQAAAMADy5cvn6ZPny2LxWKvxcXFqVu3TgoODjKxMwAAACD1EKQDAAAAeCh16tRT//5vGWoBAec1YEAf2Ww2k7oCAAAAUg9BOgAAAICHNmjQMFWrVsNQW7t2tebMmW1SRwAAAEDqIUgHAAAA8NBcXV01c+Yc5cyZ01AfMWKYDh8+ZFJXAAAAQOogSAcAAACQKgoWLKQpU2YYajExMXrzzQ4KC7tpUlcAAADAwyNIBwAAAJBqGjZspB49+hhqZ8+e0dtv92O9dAAAAGRaBOkAAAAAUtW7745QpUqVDbUVK5bpu+/mmdQRAAAA8HAI0gEAAACkKnd3d82c+bV8fLIb6sOGDdJffx01qSsAAADgwRGkAwAAAEh1jz1WRBMnTjXUIiMj1a1bR4WHh5vUFQAAAPBgCNIBAAAApIkmTV5W585dDbXjx4/p3XcHm9QRAAAA8GAI0gEAAACkmQ8+GKPHH3/CUPvuu3launSRSR0BAAAAKUeQDgAAACDNZM2aVbNnfy0vL29DfdCgATp9+qRJXQEAAAApQ5AOAAAAIE0VL+6n8eMnGWoREeF6882OioqKMqcpAAAAIAUI0gEAAACkuebNW6lt2/aG2pEjhzV69AiTOgIAAACSjyAdAAAAQLoYM2acSpUqbajNmvWFdu3aaVJHAAAAQPIQpAMAAABIF15eXpo9e66yZs1qqPft20NhYTdN6goAAAC4P4J0AAAAAOmmdOkyGjbsfUMtIOC83n9/mEkdAQAAAPdHkA4AAAAgXXXr1lM1ajxjqM2fP1dbtmw0qSMAAADg3gjSAQAAAKQrq9WqyZO/kKenl6E+YEAfBQUFmtQVAAAAcHcE6QAAAADS3WOPFdGoUR8ZaleuXNbQoW+b1BEAAABwdwTpAAAAAEzRvn1H1a//rKG2YsUyrV690qSOAAAAgKQRpAMAAAAwhcVi0cSJU5U9ew5DffDgAbpy5Yo5TQEAAABJIEgHAAAAYJoCBQrqk0/GG2qBgYF6++2+stlsJnUFAAAAGBGkAwAAADBVs2Yt9dJLLxtqGzeu1+LF35nUEQAAAGBEkA4AAADAVBaLRePGTVTu3HkM9XffHaILFwJM6goAAAD4F0E6AAAAANPlzp1bn302xVC7eTNU/fr1VEJCgkldAQAAALcQpAMAAADIEBo1aqzWrdsaajt37tDXX882qSMAAADgFoJ0AAAAABnGmDFjVajQI4baqFHv68yZUyZ1BAAAABCkAwAAAMhAfHyya9KkaYZaZGSkevfurvj4eJO6AgAAgLMjSAcAAACQodSpU0+dO3c11H79dZ+mTZtsUkcAAABwdgTpAAAAADKc4cNHqWjRYobauHFjdPToEZM6AgAAgDMjSAcAAACQ4Xh5eWnKlJmyWv/9J0tMTIx69eqmmJgYEzsDAACAMyJIBwAAAJAhPfVUNfXs2ddQO3LksCZM+MSkjgAAAOCsCNIBAAAAZFhDhryrMmXKGmqff/6Z9u3ba1JHAAAAcEYE6QAAAAAyrCxZsmjq1Jlyc3Oz1xISEtSrV1eFhYWZ2BkAAACcCUE6AAAAgAytfPknNXjwMEPt3Dl/jRjxrkkdAQAAwNkQpAMAAADI8Hr37q+qVasZavPmfa1Nm9ab1BEAAACcCUE6AAAAgAzPxcVFU6fOlKenl6Hev39vXb9+3aSuAAAA4CwI0gEAAABkCkWLFtPo0Z8YatevX9Nbb/WVzWYzqSsAAAA4A4J0AAAAAJlGu3Zv6PnnXzDU1q//QYsXf2dSRwAAAHAGBOkAAAAAMg2LxaIJE6YoV65chvqwYYN1/vw5k7oCAACAoyNIBwAAAJCp5M2bVxMmTDHUwsJuqnfv/yk+Pt6krgAAAODICNIBAAAAZDqNG7+k11573VD75Zfd+uKLqSZ1BAAAAEeWqkF6fHy8duzYoZ9//lkJCQmpeWoAAAAAMBg9+hM9+uhjhtonn3yoI0f+NKkjAAAAOKoHDtJtNpsmTJigHj16SLoVordr107du3dX165d1aJFC4WFhaVaowAAAABwp2zZfDRlygxZLBZ7LSYmRj17dlV0dLSJnQEAAMDRPHCQ/s0332j27NkKDw+XJG3cuFG///67GjRooB49eujkyZOaMWNGqjUKAAAAAP9Vo8Yz6tWrn6H2119H9Mkno03qCAAAAI7ogYP077//XjVq1NDcuXMlSVu2bJG7u7vGjh2rvn37qlWrVtq8eXOqNQoAAAAASRky5F2VLfu4oTZ9+mTt2bPLpI4AAADgaB44SD937pwaNWpk/zXKX375RU8++aS8vLwkSaVLl9alS5dSp0sAAAAAuIssWbJo2rRZcnd3t9dsNpt69/6fbt4MNbEzAAAAOIoHDtLd3d3tNxT966+/FBgYqOrVq9u3h4aGKlu2bA/fIQAAAADcR7lyj+udd4YbagEB5/Xuu0NM6ggAAACO5IGD9GLFimnbtm2SpMWLF8tisahu3bqSpLCwMK1YsULFixdPlSYBAAAA4H569OitGjWeMdQWLVqgtWvXmNQRAAAAHMUDB+nt27fXzp07VblyZS1atEhPPvmkypUrp8OHD6tRo0Y6c+aMOnXqlJq9AgAAAMBdubi4aMqUGfL2Nv5m7Ntv99XVq1dN6goAAACO4IGD9MaNG+vzzz9X1apV1aZNG02bNk2SlDVrVnl7e2v8+PGqV69eqjUKAAAAAPfz6KOP6aOPxhlqN27c0LBhg0zqCAAAAI7AYrPZbGY34YiuXbtpdgumcHW1ytfXS0FB4YqLSzC7HQDpjPcAwHkx/8hIbDabOnV6XevWGZd0+eqreWrS5GWTunJczD/gvJh/wHk52vznyXP/e30+8BXpt935K5IBAQEaN26cJk6cqICAgIc9NQAAAACkmMVi0bhxE5UzZ05DfciQgbpx44ZJXQEAACAze+AgPSQkRG3atFGPHj0kSaGhoWrTpo3mzJmjmTNnqkWLFvL390+tPgEAAAAg2fLmzasxY4xLvFy/fk3vvjvYpI4AAACQmT1wkD516lQdOnRITz31lCRp1apVunHjhgYOHKhvvvlGbm5umj59eqo1CgAAAAAp0axZSzVq1NhQW7FiqTZsWGdSRwAAAMisHjhI37Ztm1q2bKkhQ4ZIknbs2KHs2bPrzTffVPXq1dWmTRvt2bMn1RoFAAAAgJS4vcRL9uw5DPVBg/orODjInKYAAACQKT1wkH7lyhU98cQTkqSYmBgdOHBAVapUkdV665T58+dXSEhI6nQJAAAAAA8gf/4C+vDDjw21K1cua/jwoSZ1BAAAgMzogYN0X19f3bx5U5J04MABRUVF6emnn7ZvDwgIUK5cuR6+QwAAAAB4CK1bt1WDBg0NtcWLv9OPP24yqSMAAABkNg8cpJctW1bLly/X4cOH9cUXX8hqtapevXqSpD/++EOLFy9WhQoVUqtPAAAAAHggFotF48d/Lm/vbIb6wIF9FRrKb9ECAADg/h44SO/Xr5+uXr2qVq1aad++fWrWrJkKFiyoPXv2qFWrVkpISFCPHj1SdM4bN26oZ8+eqlKliqpVq6YxY8YoLi4uyX137NihJk2aqEKFCnrhhRe0bds2w/bZs2erdu3aqlChgtq3b68zZ84kOkdkZKRat26tFStWGOqzZs1SuXLlVLFiRftj4sSJKXotAAAAADKOQoUe0ciRYwy1S5cu6oMP3jOpIwAAAGQmDxyklylTRsuXL9egQYP02WefadSoUZKkxx57TK1atdKiRYtUsmTJFJ2zf//+8vT01M6dO7Vs2TLt2bNH33zzTaL9/P391adPH/Xr10+//vqr+vTpo/79++vKlSuSpJUrV2revHn66quvtHfvXpUrV059+/aVzWazn+PkyZNq166dfv/990Tn//PPP9WjRw8dPHjQ/hgwYECKXgsAAACAjOX11zuodu16htr8+XO1fftWkzoCAABAZvHAQbokFS5cWJ07d1bjxo3tNxktWLCgRo0apWLFiqXoXOfOndO+ffs0aNAgeXh4qHDhwurZs6cWLFiQaN+VK1eqSpUqevbZZ+Xq6qrGjRuratWqWrx4sSRpyZIlatu2rfz8/JQlSxa99dZbunjxovbu3StJ2rNnjzp06KBXX31VBQsWTHT+w4cP6/HHH0/ptwMAAABABmaxWPTZZ5Pl6ellqA8c2EdhYTdN6goAAACZgevDnmDVqlVav369Lly4IHd3dxUoUECNGjVS06ZNU3SekydPKkeOHMqXL5+9Vrx4cV28eFGhoaHy8fGx10+dOpXoavcSJUro2LFj9u1du3a1b3Nzc1ORIkV07NgxVa9eXaVLl9a2bduUJUsWff3114bz3LhxQxcvXtSSJUv03nvvyd3dXY0aNVK/fv2UJUuWZL8eq9Uiq9WSou+BI3BxsRo+AnAuvAcAzov5R2ZRrFhRjRz5oQYNGmivXbgQoNGjR2j8+EnmNZaJMf+A82L+AefljPP/wEG6zWZT3759tWXLFtlsNmXLlk0JCQn666+/tG3bNm3YsEHTp09P9vnCw8Pl4eFhqN3+PCIiwhCkJ7Vv1qxZFRERkaztvr6+d+3j2rVrqlKlipo1a6ZJkyYpICBA/fv3V2RkpEaMGJHs15Mzp5csFucL0m/z8fG4/04AHBbvAYDzYv6RGQwc2E9r167W9u3b7bU5c77U66+3Vb169e5+IO6J+QecF/MPOC9nmv8HDtLnz5+vzZs3q2nTpnrrrbfsV5JfunRJkyZN0urVq7Vw4UK99tpryTqfp6enIiMjDbXbn3t5GX/10sPDQ1FRUYZaVFSUfb/7bb+X0qVLG5aTKV68uHr27KkPPvggRUF6YGC4016R7uPjodDQSMXHJ5jdDoB0xnsA4LyYf2Q2EyZMVq1a1e0X20hS585dtHPnL8n6dwP+xfwDzov5B5yXo82/r+/9//73wEH68uXL9dRTT2ncuHGGeoECBTR27FhdvnxZy5cvT3aQ7ufnp+DgYF2/fl25c+eWJJ0+fVr58+dXtmzZDPuWLFlSR44cMdROnTplX9fcz89PJ0+etF9NEhsbK39//2Td/HTfvn06ePCg/ve//9lrMTExypo1a7Jex20JCTYlJNjuv6ODio9PUFxc5h8iAA+G9wDAeTH/yCwKFy6iYcPe13vvvWOv+fuf1YcffqDRo8ea2FnmxfwDzov5B5yXM83/Ay9ic/bsWTVs2PCu25999lmdOXMm2ecrUqSIKleurI8++khhYWEKCAjQ9OnT1aJFi0T7Nm3aVPv27dO6desUFxendevWad++fXr55ZclSc2bN9f8+fN17NgxRUdHa8KECcqdO7eqVKly3z48PDw0ZcoUrVmzRgkJCTp58qSmT5+u1q1bJ/u1AAAAAMj43nyzu556qrqhNnv2DO3d+4tJHQEAACCjeuAg3dXV1fBrkP8VERGR4jXCJ0+erLi4ODVo0ECtWrVSrVq11LNnT0lSxYoVtXr1akm3lluZNm2aZs6cqapVq2r69OmaMmWKihYtKklq0aKFOnbsqF69eql69eo6evSoZs6cKTc3t/v2UL58eX322Wf68ssvVblyZXXp0kVNmjRR9+7dU/RaAAAAAGRsVqtVn38+zfDbpzabTf3790y07CQAAACcm8Vmsz3Q+iMdOnTQ5cuXtXr1amXJksWwLTIyUq+88ory5s2refPmpUqjmc21azfNbsEUrq5W+fp6KSgo3Gl+rQPAv3gPAJwX84/MbNq0yRo58j1DrWfPvvrgg9EmdZS5MP+A82L+AeflaPOfJ0+2++7zwFekd+7cWefOnVOLFi30ww8/6NixYzp27JjWrFmjli1b6vz58+rUqdODnh4AAAAA0kX37r1UubJxGcgvvpiiffv2mtQRAAAAMpoHviJdkubMmaPPPvtM8fHxhrrValW/fv3UrVu3h24ws+KKdMf4aRSAlOE9AHBezD8yu+PHj6lBg5qKiYmx14oXL6GtW3fJw8PDxM4yPuYfcF7MP+C8HG3+k3NFuuvDfIHOnTurYcOG2rJli86fPy+bzaZHH31UDRs2VL58+RQWFiZvb++H+RIAAAAAkOZKlSqtwYOHafToD+y106dP6eOPP9SoUR+Z1xgAAAAyhIe6Iv1eRowYoaVLl+ro0aNpcfoMjyvSHeOnUQBShvcAwHkx/3AEcXFxeumlhvrttwP2msVi0fffb1D16jVM7CxjY/4B58X8A87L0eY/TddIT440yugBAAAAINW5urpq8uQZypIli71ms9nUr18PhYeHm9gZAAAAzJamQToAAAAAZCYlS5bSO+8MN9TOnj2jjz4aaVJHAAAAyAgI0gEAAADgDt2791KVKk8ZarNnz9Du3T+b1BEAAADMRpAOAAAAAHdwcXHRlClfKGvWrIZ63749FRYWZlJXAAAAMBNBOgAAAAD8R/Hifho27H1D7fx5f40ePcKkjgAAAGAm1+TuuH///hSd+OrVqyluBgAAAAAyiq5de2jt2jXau3ePvTZnzmy9+GJT1apVx8TOAAAAkN6SHaS3b99eFosl2Se22Wwp2h8AAAAAMhIXFxd9/vl01av3tCIjI+31/v17aceOPfL2zmZidwAAAEhPyQ7SX3nlFYJxAAAAAE6lWLHiGj58pIYNG2yvBQSc1wcfDNf48ZPMawwAAADpymKz2WxmN+GIrl27aXYLpnB1tcrX10tBQeGKi0swux0A6Yz3AMB5Mf9wZAkJCWrW7CXt3v2zob548UrVq9fApK4yDuYfcF7MP+C8HG3+8+S5/28acrNRAAAAALgHq9WqSZOmydPTy1AfOLCPQkNDTOoKAAAA6YkgHQAAAADuo0iRohox4kND7e+/L+iDD94zqSMAAACkJ4J0AAAAAEiGDh06q1atuoba/PlztXXrZnMaAgAAQLohSAcAAACAZLi1xMtUeXl5G+oDBvRRSEiwOU0BAAAgXRCkAwAAAEAyFS78qEaN+shQu3TpooYPH2pSRwAAAEgPBOkAAAAAkAKvv95BdevWN9QWLVqgTZvWm9QRAAAA0hpBOgAAAACkgMVi0cSJU5Utm4+h/vbb/RUaGmJSVwAAAEhLBOkAAAAAkEKFCj2i0aM/MdQuX76k0aM/MKchAAAApCmCdAAAAAB4AG3atFP9+s8aat9885V++WW3SR0BAAAgrRCkAwAAAMADsFgs+vTTSfL09DLUBw7so6ioKJO6AgAAQFogSAcAAACAB1S48KMaNmy4oXbq1ElNmvSpSR0BAAAgLRCkAwAAAMBD6NLlf6pUqbKhNnnyRB09esSkjgAAAJDaCNIBAAAA4CG4uLjos8+mytXV1V6Li4vTwIG9FR8fb2JnAAAASC0E6QAAAADwkMqWLae+fQcYar/9dkBffTXTpI4AAACQmgjSAQAAACAV9O8/SCVK+BlqH330oc6fP2dSRwAAAEgtBOkAAAAAkAqyZs2qzz6bYqhFRIRr0KD+stlsJnUFAACA1ECQDgAAAACppHr1p9WhQxdDbdu2H7Vs2WKTOgIAAEBqIEgHAAAAgFQ0fPgHKlCg4H9q7+j69esmdQQAAICHRZAOAAAAAKnIxye7xo79zFALDAzU8OHvmNQRAAAAHhZBOgAAAACkskaNGqtp01cNteXLl+jHHzeZ1BEAAAAeBkE6AAAAAKSBMWPGKXv2HIbaoEEDFBYWZk5DAAAAeGAE6QAAAACQBvLly6eRI8cYahcuBOiTTz40qSMAAAA8KIJ0AAAAAEgjr732umrVqmOozZ49Q7/+us+kjgAAAPAgCNIBAAAAII1YLBZ9+ukkZc2a1V6z2WwaOLCPYmJiTOwMAAAAKUGQDgAAAABpqFix4ho0aJihduzYX5oyZaJJHQEAACClCNIBAAAAII316NFb5cs/aahNnPipTpw4blJHAAAASAmCdAAAAABIY66urpo4cYpcXFzstZiYGPXr10NxcXEmdgYAAIDkIEgHAAAAgHTwxBMV1KNHH0PtwIFfNX36ZJM6AgAAQHIRpAMAAABAOnn77XdUvHgJQ23s2DE6evSISR0BAAAgOQjSAQAAACCdeHp6avLkL2S1/vtPsdjYWPXp010xMTEmdgYAAIB7IUgHAAAAgHRUtWo19erVz1A7fPiQJk781KSOAAAAcD8E6QAAAACQzgYPHqbSpcsYapMmjdfvv/9mUkcAAAC4F4J0AAAAAEhnWbJk0dSpM+Xq6mqvxcfHq0+f7oqKijKxMwAAACSFIB0AAAAATPDEExU0YMAgQ+348WMaO3aMSR0BAADgbgjSAQAAAMAk/fu/rSeeqGCoTZ8+Wfv27TWnIQAAACSJIB0AAAAATOLm5qYpU2bI3d3dXrPZbOrT538KDw83sTMAAADciSAdAAAAAExUpkxZDR78rqF29uwZjR49wqSOAAAA8F8E6QAAAABgsl69+qpy5aqG2ldfzdLOnTtM6ggAAAB3IkgHAAAAAJO5uLho6tQZ8vDwMNT79eupmzdDTeoKAAAAtxGkAwAAAEAGULy4n9577wND7cKFAI0Y8W7SBwAAACDdEKQDAAAAQAbRpcv/9MwztQy1+fPnasuWjSZ1BAAAAIkgHQAAAAAyDKvVqkmTpsnLy9tQHzCgj4KCAk3qCgAAAATpAAAAAJCBPPZYEY0cOcZQu3LlsoYNG2xSRwAAACBIBwAAAIAMpn37jqpXr4Ghtnz5Ev3ww2qTOgIAAHBuBOkAAAAAkMFYLBZNnDhVPj7ZDfXBg/vr2rVrJnUFAADgvAjSAQAAACADKliwkD76aJyhdv36dQ0a1F82m82krgAAAJwTQToAAAAAZFAtW7ZRo0YvGmrr1q3RkiULTeoIAADAORGkAwAAAEAGZbFYNH7858qVK5ehPmzYYP399wWTugIAAHA+BOkAAAAAkIHlzZtXn376uaF282ao+vbtqYSEBJO6AgAAcC4E6QAAAACQwb30UlO1aNHaUNu5c7u+/nq2Kf0AAAA4G4J0AAAAAMgEPv74UxUoUNBQGzXqfZ0+fdKkjgAAAJwHQToAAAAAZALZs+fQ559PN9QiIyPVu/f/FBcXZ1JXAAAAzoEgHQAAAAAyibp166tz566G2oEDv2rq1EnmNAQAAOAkCNIBAAAAIBMZPnyUihYtZqh9+unHOnz4D5M6AgAAcHwE6QAAAACQiXh5eWnq1JmyWv/951xsbKx69+6m6OhoEzsDAABwXATpAAAAAJDJVK1aTX36DDDU/vrrqMaN+8ikjgAAABwbQToAAAAAZEKDBg1VuXLlDbWpUydp795fTOoIAADAcRGkAwAAAEAm5O7urqlTZ8rNzc1es9ls6tPnfwoLCzOxMwAAAMdDkA4AAAAAmVS5co9ryJB3DTV//7MaOXK4SR0BAAA4JoJ0AAAAAMjEevXqp6pVqxlqc+d+pa1bt5jUEQAAgOMhSAcAAACATMzFxUVTpsyQp6enod6/fy8FBweZ1BUAAIBjIUgHAAAAgEyuWLHiGjFitKF2+fIlvfPO2yZ1BAAA4FgI0gEAAADAAXTs2EV169Y31FasWKrVq1ea1BEAAIDjIEgHAAAAAAdgsVg0adI0Zc+ew1AfPHiArly5bE5TAAAADoIgHQAAAAAcRMGChfTxx58aaoGBgerXr6dsNptJXQEAAGR+BOkAAAAA4ECaN2+lJk1eMdS2bt2iOXNmm9MQAACAAyBIBwAAAAAHYrFYNG7cROXLl99QHznyPZ04cdykrgAAADI3gnQAAAAAcDC5cuXS559PN9SioqLUs2dXxcTEmNQVAABA5kWQDgAAAAAOqH79Z/Xmm/8z1P7443eNH/+JSR0BAABkXgTpAAAAAOCghg8fpZIlSxlqkyd/pl9+2WNSRwAAAJkTQToAAAAAOCgPDw9Nnz5bbm5u9lpCQoJ69+6mmzdDTewMAAAgcyFIBwAAAAAH9sQTFTRkyLuG2vnz5zRs2GCTOgIAAMh8CNIBAAAAwMH16tVP1as/bagtXvyd1qxZZU5DAAAAmQxBOgAAAAA4OBcXF02dOlPe3tkM9bff7qfLly+Z1BUAAEDmQZAOAAAAAE7g0Ucf08cff2qoBQUFqW/fHkpISDCpKwAAgMyBIB0AAAAAnESrVq+padNXDbXt27dqzpxZJnUEAACQORCkAwAAAICTsFgs+vTTicqfv4ChPmrU+zp+/JhJXQEAAGR8BOkAAAAA4ER8fXNq8uQvDLWoqCj16PGmYmJiTOoKAAAgYyNIBwAAAAAnU7dufXXr1sNQ+/PPPzR27BiTOgIAAMjYCNIBAAAAwAm9++4HKl26jKE2deok7dmzy6SOAAAAMi6CdAAAAABwQh4eHpo+/Uu5ubnZazabTb16dVNoaIiJnQEAAGQ8BOkAAAAA4KQef7y8hg5931C7cCFA77zztkkdAQAAZEwE6QAAAADgxHr06K2nn65pqC1btliLFi0wqSMAAICMhyAdAAAAAJyYi4uLpk6dKR+f7Ib6O++8pePHj5nUFQAAQMZCkA4AAAAATu6RRwpr/PhJhlpERIS6du2giIgIc5oCAADIQAjSAQAAAAB65ZXmeuONzobasWN/6d13B5vUEQAAQMZBkA4AAAAAkCR9+OHHKlv2cUNtwYJvtWzZYpM6AgAAyBgI0gEAAAAAkiQPDw99+eVceXp6Gepvv91fp06dNKkrAAAA8xGkAwAAAADsSpTwS2K99HC9+WYHRUZGmtMUAACAyQjSAQAAAAAGLVq0Vrt2bxhqR4/+qeHDh5rUEQAAgLkI0gEAAAAAiYwZM05lypQ11L79do5WrVpuUkcAAADmIUgHAAAAACTi6emp2bPnytPT01AfOLCvzpw5bVJXAAAA5iBIBwAAAAAkqWTJUvrkkwmGWljYTXXt2lFRUVEmdQUAAJD+CNIBAAAAAHfVpk07tW7d1lA7fPiQRowYZlJHAAAA6S9DBek3btxQz549VaVKFVWrVk1jxoxRXFxckvvu2LFDTZo0UYUKFfTCCy9o27Zthu2zZ89W7dq1VaFCBbVv315nzpxJdI7IyEi1bt1aK1asMNTPnj2rDh06qGLFiqpZs6ZmzJiRei8SAAAAADKZTz6ZoJIlSxlqX3/9pdasWWVOQwAAAOksQwXp/fv3l6enp3bu3Klly5Zpz549+uabbxLt5+/vrz59+qhfv3769ddf1adPH/Xv319XrlyRJK1cuVLz5s3TV199pb1796pcuXLq27evbDab/RwnT55Uu3bt9PvvvxvOHRsbq+7du6t8+fLau3evZs2apQULFmj9+vVp+dIBAAAAIMPy8vLS7Nlz5eHhYaj3799bZ88mvmgJAADA0WSYIP3cuXPat2+fBg0aJA8PDxUuXFg9e/bUggULEu27cuVKValSRc8++6xcXV3VuHFjVa1aVYsXL5YkLVmyRG3btpWfn5+yZMmit956SxcvXtTevXslSXv27FGHDh306quvqmDBgoZz79+/X1evXlXfvn3l7u6usmXLqn379kn2AQAAAADOokyZsvroo08NtZs3Q9WlSwdFR0eb1BUAAED6cDW7gdtOnjypHDlyKF++fPZa8eLFdfHiRYWGhsrHx8deP3XqlEqWLGk4vkSJEjp27Jh9e9euXe3b3NzcVKRIER07dkzVq1dX6dKltW3bNmXJkkVff/11oj6KFi0qd3d3w7lnzZqVotdjtVpktVpSdIwjcHGxGj4CcC68BwDOi/kHnMMbb3TQ7t07tXTpYnvt998PavDgwfrww09M7AyAGfj/P+C8nHH+M0yQHh4enujXBG9/HhERYQjSk9o3a9asioiISNZ2X1/fFPdx+9jkypnTSxaL8wXpt/n4eNx/JwAOi/cAwHkx/4DjmzPnS/3xx+86fvy4vTZ58mTVrVtXr776qomdATAL//8HnJczzX+GCdI9PT0VGRlpqN3+3MvLy1D38PBQVFSUoRYVFWXf737bH6SP5Bx7p8DAcKe9It3Hx0OhoZGKj08wux0A6Yz3AMB5Mf+AM7Hoyy/nqmHDuoZ/d3Xs2FGPPFJUJUr4mdgbgPTE//8B5+Vo8+/re//sN8ME6X5+fgoODtb169eVO3duSdLp06eVP39+ZcuWzbBvyZIldeTIEUPt1KlTevzxx+3nOnnypOrVqyfp1g1E/f39Ey0Hc7c+/P39FRcXJ1dXV/u5/fxS9pfBhASbEhJs99/RQcXHJyguLvMPEYAHw3sA4LyYf8A5lCpVVqNHj9Xbb/ez10JDQ/XGG221bt2P8vb2NrE7AOmN//8DzsuZ5j/DLGJTpEgRVa5cWR999JHCwsIUEBCg6dOnq0WLFon2bdq0qfbt26d169YpLi5O69at0759+/Tyyy9Lkpo3b6758+fr2LFjio6O1oQJE5Q7d25VqVLlvn1Uq1ZNvr6+mjBhgqKjo3Xs2DHNmzcvyT4AAAAAwFm1b99RzZq1NNSOHftLAwf2ls3mvBcVAQAAx5RhgnTp1rp6cXFxatCggVq1aqVatWqpZ8+ekqSKFStq9erVkm7dhHTatGmaOXOmqlatqunTp2vKlCkqWrSoJKlFixbq2LGjevXqperVq+vo0aOaOXOm3Nzc7tuDq6ur5syZoxMnTuiZZ55Rt27d1L59ezVr1iztXjgAAAAAZDIWi0UTJkxW2bLlDPVVq1Zo5sxpJnUFAACQNiw2LhVIE9eu3TS7BVO4ulrl6+uloKBwp/m1DgD/4j0AcF7MP+C8zp8/qwYNaiskJMRec3Fx0bJlq/XMM7VM7AxAWuP//4DzcrT5z5Mn2333yVBXpAMAAAAAMpdixYpr/vz5hlp8fLy6du2oixf/NqkrAACA1EWQDgAAAAB4KC+99JIGDXrHULt+/Zq6dHlD0dHRJnUFAACQegjSAQAAAAAPbciQYWrQoKGhduDAfr333jt3OQIAACDzIEgHAAAAADw0q9Wq6dNn67HHihjqc+d+pUWLFpjTFAAAQCohSAcAAAAApApf35z6+usF8vDwMNQHDeqvP/743ZymAAAAUgFBOgAAAAAg1Tz+eHmNH/+5oRYdHa1OnV5XYOANk7oCAAB4OATpAADg/+3dd3QUZfvG8WvTG5BGr9KVgJTQAwoWiEpRFJBiQUXhZwEFBQSVV3zVV2yoiNJEFBARRDAUWyAgvSgIUUKoApEkBLIppOz+/ghE10AgkOwkO9/POXt29nmenb13PfegF+MMAADF6p57+uuhh4Y6jB05cliPPjpEubm5BlUFAABw5QjSAQAAAADFbuLE/6pNm3YOY2vW/KTXX3/FoIoAAACuHEE6AAAAAKDYeXl5aebMT1WpUmWH8XfemayoqOUGVQUAAHBlCNIBAAAAACWicuUqmjHjU3l4eDiMP/74o4qL22dQVQAAAEVHkA4AAAAAKDHt2rXXxImOl3OxWlP14IMDZbVaDaoKAACgaAjSAQAAAAAl6uGHH1OfPn0dxn7/PVbDhz/MzUcBAECZQJAOAAAAAChRFotFb745RdddF+YwvnJllF56abxBVQEAAFw+gnQAAAAAQInz8/PT7NmfKTAw0GH8o48+0MyZHxtTFAAAwGUiSAcAAAAAOMU119TV7Nmfy9PT02H8+eef1fffrzKoKgAAgEsjSAcAAAAAOE3Hjp301lvvOYzZbDY98siD2rXrV4OqAgAAKBxBOgAAAADAqfr1G6BnnnnOYSwtzapBg/rq+PFjBlUFAABwcQTpAAAAAACne/bZcbrrrnscxo4fP6aBA/vKak01qCoAAIALI0gHAAAAADidxWLRu+9OVdu27R3Gd+/+VUOHPqicnByDKgMAACiIIB0AAAAAYAhvb2/NmTNPdevWcxj//vvVGj/+OdntdoMqAwAAcESQDgAAAAAwTHBwiObN+1JBQUEO47NmTdf06R8aVBUAAIAjgnQAAAAAgKHq1q2vOXPmy8vLy2F8woSxWrHiW4OqAgAA+BtBOgAAAADAcO3addC77051GLPb7Ro27CH98ssOg6oCAADIQ5AOAAAAACgV+vTpq+eee95hLD09XQMH9tXRo0cMqgoAAIAgHQAAAABQijz99LPq12+Aw9hffyVo4MC+Sk09Y1BVAADA7AjSAQAAAAClhsVi0ZtvTlHHjp0cxvfu/U0PP3y/srOzDaoMAACYGUE6AAAAAKBU8fLy0uzZn6l+/QYO4z/99IOefvoJ2Ww2gyoDAABmRZAOAAAAACh1AgODNG/eIoWGhjqMf/HFPL300njZ7XaDKgMAAGZEkA4AAAAAKJXq1LlGc+bMl4+Pj8P4tGnv67333jaoKgAAYEYE6QAAAACAUqt167aaMWOO3N3dHcYnTXpJc+d+YkxRAADAdAjSAQAAAACl2q23Rurdd6cWGB89eoSWLfva+QUBAADTIUgHAAAAAJR6ffveq0mTXnMYs9lsGjbsYa1Z85NBVQEAALMgSAcAAAAAlAlDhw7XyJGjHMaysrJ0//0DtGPHNoOqAgAAZkCQDgAAAAAoM8aMmaD77hviMJaenqZ77+2jffv+MKgqAADg6gjSAQAAAABlhsVi0euvv6mePe90GE9OTtY99/TS0aNHDKoMAAC4MoJ0AAAAAECZ4u7urg8++Fg33NDFYfzYsT/Vt29vJSYmGlQZAABwVQTpAAAAAIAyx9vbW7Nnf66WLVs5jMfF7dOAAX1ktaYaVBkAAHBFBOkAAAAAgDIpICBA8+YtUsOGjRzGd+7cofvvH6CzZ88aVBkAAHA1BOkAAAAAgDIrODhECxd+rRo1ajqMx8Ss0bBhDys3N9egygAAgCshSAcAAAAAlGnVqlXXwoVfKyQkxGF8+fKlGj16hGw2m0GVAQAAV0GQDgAAAAAo8+rXb6AFCxbL3z/AYfyzz+boqaeGKycnx6DKAACAKyBIBwAAAAC4hOuvb6G5cxfIy8vLYfyLL+bpoYfuU2ZmpkGVAQCAso4gHQAAAADgMiIiOuvjjz+Rh4eHw/iKFcs1cGBfWa1WgyoDAABlGUE6AAAAAMCl3HbbHZozZ558fHwcxmNionXPPb106lSyIXUBAICyiyAdAAAAAOBybrmluxYsWKyAgHIO49u2bVHv3rcpIeGEQZUBAICyiCAdAAAAAOCSOnSI0JIlyxUcHOwwvnfvHvXo0U2HDx8yqDIAAFDWEKQDAAAAAFzW9de30DffrFLVqtUcxg8ePKA77rhVv/8ea1BlAACgLCFIBwAAAAC4tIYNG2nZslWqU+cah/ETJ46rV6/u2rlzu0GVAQCAsoIgHQAAAADg8mrVqq1ly1br2mubOIwnJyfrrrt66Oef1xlUGQAAKAsI0gEAAAAAplC5cmUtXRql8PA2DuNWa6r6979Lq1evMKgyAABQ2hGkAwAAAABMIzAwSAsXfq3Onbs4jGdmZuqBBwZq8eIvDaoMAACUZgTpAAAAAABTCQgI0OefL9Ttt/d0GM/JydGwYQ9r1qzpBlUGAABKK4J0AAAAAIDpeHt7a/r0T9S//0CHcbvdrjFjntGLLz4vm81mUHUAAKC0IUgHAAAAAJiSh4eH3nnnAw0dOqzA3IcfvqchQwYrPT3dgMoAAEBpQ5AOAAAAADAtNzc3vfzya3r22XEF5qKilql370glJCQYUBkAAChNCNIBAAAAAKZmsVg0atQYTZnyoTw8PBzmdu7cocjIrtq7d49B1QEAgNKAIB0AAAAAAEn9+w/UwoVfq0KFQIfxo0eP6Pbbb9GPP35vTGEAAMBwBOkAAAAAAJwTEdFZUVHfq3btOg7jVmuqBg68R598MtOYwgAAgKEI0gEAAAAA+IcGDRpqxYofFR7exmE8NzdXzz47Ui+++LxsNptB1QEAACMQpAMAAAAA8C+hoaFavHi5eve+q8Dchx++pyFDBis9Pd2AygAAgBEI0gEAAAAAuAAfHx9NmzZLI0eOKjAXFbVMvXtHKiHhhAGVAQAAZyNIBwAAAADgItzc3DR27AuaMuVDeXh4OMzt3LlD3bt31Z49vxlUHQAAcBaCdAAAAAAALqF//4FauPBrVagQ6DD+559Hdccdt+q771YaUxgAAHAKgnQAAAAAAC5DRERnRUV9r9q16ziMW62pGjiwr155ZaJycnKMKQ4AAJQognQAAAAAAC5TgwYNtWLFjwoPb1Ng7t1339Q99/RSQkKCAZUBAICSRJAOAAAAAEARhIaGavHi5brzzj4F5tavj9FNN0Vo/foYAyoDAAAlhSAdAAAAAIAi8vHx0bRpszRp0msFbkL6118J6tOnh959903ZbDaDKgQAAMWJIB0AAAAAgCtgsVg0dOhwffPNSlWvXsNhzmaz6ZVXJmrw4H46dSrZoAoBAEBxIUgHAAAAAOAqhIe30fffx6hr15sLzH333SrdfHNnbd++1YDKAABAcSFIBwAAAADgKoWEhGjevEUaO3aC3Nwc/1P7yJHD6tGjm2bO/Eh2u92gCgEAwNUgSAcAAAAAoBi4ublp5MjR+vLLpQoNregwl52drbFjR2vo0AdltaYaVCEAALhSBOkAAAAAABSjTp1u0I8/rlO7dh0KzC1duli33HKD9uz5zYDKAADAlSJIBwAAAACgmFWpUlWLFy/XE0+MLDC3f3+cIiO7asaMabLZbAZUBwAAioogHQAAAACAEuDh4aEJEyZq7twvVKFCoMNcRkaGxo17VnfeebsOHIg3pkAAAHDZCNIBAAAAAChB3bpF6vvv16p58xYF5jZsWK8uXTpo+vQPOTsdAIBSjCAdAAAAAIASVrt2HS1btloPPTS0wFx6erqef/459e59m+Lj9xtQHQAAuBSCdAAAAAAAnMDb21uvvjpZX321TLVq1S4wv3Hjz+rSpYM++ugDzk4HAKCUIUgHAAAAAMCJOnW6QdHRG/Tggw8XmMvIyNCECWPVq1ek4uPjDKgOAABcCEE6AAAAAABOFhAQoNdff0uLFy9XrVp1Csxv2rRBN97YQdOmva/c3FznFwgAABwQpAMAAAAAYJCIiM6Kjv75gtdOz8zM1AsvjFPPnt0VF7fPgOoAAMB5BOkAAAAAABgoICBAr746WV9/HaXatesUmN+yZZO6du2o999/V1lZWc4vEAAAEKQDAAAAAFAadOgQoejoDXrkkccKzGVmZuo//5mgiIjWWrbsa9ntdgMqBADAvAjSAQAAAAAoJfz9/fXKK//T0qUrVKfONQXmDx48oIceuk+3336LNm/eZECFAACYE0E6AAAAAAClTPv2HRUdvUGPPjpcFoulwPzWrZt1xx23aMiQwYqPjzOgQgAAzIUgHQAAAACAUsjPz08vv/yali9frVatWl9wzfLlSxUR0Ubjxo1WUlKSkysEAMA8CNIBAAAAACjFWrduq6io7zVz5qcXvNxLTk6OZsz4SG3aXK8pU95WRkaGAVUCAODaCNIBAAAAACjlLBaLevTorXXrtmjSpNcUFBRUYE1q6hlNmvSiOnRopS+/XCCbzWZApQAAuCaCdAAAAAAAyggvLy8NHTpcmzf/oscfHyFvb+8Ca/7886j+7/+G6tZbb9TatdHOLxIAABdEkA4AAAAAQBlToUKgXnjhP/r5523q06fvBdf8+utO3X13T/Xt21u7dv3i5AoBAHAtBOkAAAAAAJRRNWvW0ocfztB3361RRETnC66Jjv5RN93USY899pAOHjzg5AoBAHANBOkAAAAAAJRx11/fQl99tUyff75QjRo1vuCaxYu/VMeO4Ro3brROnjzp5AoBACjbCNIBAAAAAHABFotFt9zSXT/99LPefHOKqlSpWmBNdna2Zsz4SG3aXK833nhVVmuqAZUCAFD2EKQDAAAAAOBCPDw8NHjwA9q4cYfGj39J5ctXKLAmLc2qN954VW3aNNfMmR8pKyvLgEoBACg7CNIBAAAAAHBBfn5+evLJp7V5804NH/6kvL29C6xJTDypsWNHq2PHcC1Zskg2m82ASgEAKP0I0gEAAAAAcGHBwSF66aVJ2rBhu/r3HyiLxVJgzaFDB/Xoo0N0yy03aOXKKAJ1AAD+hSAdAAAAAAATqFGjpqZM+VDR0RvUrVvkBdfs2vWL7ruvvyIiWmvu3E+UmZnp5CoBACidCNIBAAAAADCRa6+9TnPnfqFvvlmp8PA2F1wTF7dPzzzzpFq2bKK33vqfkpOTnFwlAAClC0E6AAAAAAAm1K5dB3377XeaM2e+GjZsdME1iYkn9dprk9SixXUaM+YZHTgQ7+QqAQAoHQjSAQAAAAAwKYvFosjI2xUdvUHvvTdN117b5ILrMjIyNGvWdLVv31IPPXSftm3b4uRKAQAwFkE6AAAAAAAm5+HhoX79Big6+md98cUSde7c5YLrbDabli37WpGRN6lnz+7cmBQAYBoE6QAAAAAAQFLeGepdutykRYuW6ocf1unuu/vJw8Pjgms3bvxZ993XXx07huuDD6YoISHBydUCAOA8BOkAAAAAAKCApk2baerU6dqy5VcNG/aEAgLKXXDd/v1xmjhxvJo3b6xBg/pq+fJvlJWV5eRqAQAoWQTpAAAAAADgoqpXr6GJE1/Rzp179MILL6tq1WoXXJebm6vVq1dqyJBBatasocaNG61du35xcrUAAJQMgnQAAAAAAHBJ5ctX0OOPP6UtW37V++9/dNEbk0pScnKyZsz4SDfd1EldunTURx99oMTERCdWCwBA8SJIBwAAAAAAl83Ly0t9+96r6OiftWTJt+rXb4D8/Pwuuv6333ZpwoSxatasoe6/f4BWrPhW2dnZTqwYAICrZ7Hb7Xaji3BFJ0+mGl2CITw83BQU5K9Tp9KUk8Od2wGz4RgAmBf9D5gX/Q9JslpTtWzZUs2f/5k2bvz5kutDQkLUq9dd6tOnr8LD28hisTihShQ3+h8wL1fr/4oVL3wfkH8iSC8hBOmu0UQAioZjAGBe9D9gXvQ//i0+fr8WLpynL76Yrz//PHrJ9bVq1VGfPnerT59+atiwkRMqRHGh/wHzcrX+v5wgvVRd2iUpKUnDhw9XeHi42rZtq1deeUU5OTkXXLtmzRr16NFDzZs3V2RkpH766SeH+enTp6tz585q3ry5Bg8erPj4+Py59PR0jR07Vm3btlWrVq307LPPKi0tLX/+448/VpMmTdSiRYv8x9tvv10yXxoAAAAAABdSt249jRkzQdu27daiRd+oT5++8vHxuej6w4cP6u23JysiorW6do3QBx9M0bFjfzqxYgAALq1UBekjRoyQn5+fYmJitGjRIm3YsEGffPJJgXUHDx7UE088oaeeekpbt27VE088oREjRighIUGStGTJEs2dO1czZ87Upk2b1KRJEz355JM6f/L9yy+/rOPHj2vVqlVavXq1jh8/rsmTJ+fvf/fu3Ro2bJh27NiR/xg5cqRTfgMAAAAAAFyBm5ubOne+UR9+OEO7d+/TW2+9p9at2xb6nt27f9XEiePVosV1uvPO2/XZZ3OUknLKSRUDAHBxpSZIP3TokDZv3qzRo0fL19dXNWvW1PDhw/X5558XWLtkyRKFh4fr5ptvloeHh2677Ta1bt1aX3zxhSRp4cKFGjBggBo0aCBvb28988wzOnbsmDZt2qSMjAwtW7ZMTz75pAIDAxUSEqJRo0Zp8eLFysjIkCTt2rVLYWFhTv3+AAAAAAC4qvLlK2jQoPv17bffaePGHXr22XGqV6/+Rdfb7XatXx+jp59+QmFhDXT//QO0ZMkinTlz2olVAwDwNw+jCzhv3759CgwMVOXKlfPH6tWrp2PHjunMmTMqX758/nhcXJwaNmzo8P769esrNjY2f/6RRx7Jn/P09FSdOnUUGxurwMBAZWdnO7y/Xr16yszM1MGDB1WpUiUdO3ZMCxcu1Pjx4+Xl5aXu3bvrqaeekre392V/Hzc3i9zczHezFHd3N4dnAObCMQAwL/ofMC/6H0XVsGEDjRkzTs89N1a//LJTixYt1OLFX+rEiRMXXJ+VlaUVK5ZrxYrl8vT0VMeOnRQZeZsiI29XjRo1nVw9/on+B8zLjP1faoL0tLQ0+fr6Ooydf52enu4QpF9orY+Pj9LT0y85b7VaJUl+fn4FPictLU0nT55UeHi47rrrLr3zzjs6cuSIRowYoYyMDL344ouX/X2Cg/1Nfdfx8uV9L70IgMviGACYF/0PmBf9jyvRpUuEunSJ0JQpbys6Olrz5s3TokWLdObMmQuuz87OVnT0j4qO/lHPPTdKzZs3V69evdSzZ0+1aNHC1P8dbiT6HzAvM/V/qQnS/fz88i+tct751/7+/g7jvr6+yszMdBjLzMzMX1fY/PkAPSMjI3/9+c8JCAhQ48aNHS4nU69ePQ0fPlwvvfRSkYL05OQ0056RXr68r86cyVBubtm/Yy+AouEYAJgX/Q+YF/2P4tKyZTu1bNlOkyb9T999t0pffvmFVq9eqaysrIu+Z+fOndq5c6cmTpyoatWqnztT/Q5FRHSSl5eXE6s3J/ofMC9X6/+gIP9Lrik1QXqDBg2UkpKixMREhYaGSpL279+vKlWqqFy5cg5rGzZsqN9++81hLC4uLv+65g0aNNC+ffvUpUsXSXl/Y33w4EE1bNhQ11xzjTw9PRUXF6frr78+/3POX/5l8+bN2rFjhx599NH8fWdlZRV6h/ELsdnsstnsRfsRXEhurk05OWW/iQBcGY4BgHnR/4B50f8oLh4eXoqM7KHIyB46fTpFy5d/o+XLlyomZk2hofqxY39q5szpmjlzugICyummm25Rt26RuummWxQUFOzEb2A+9D9gXmbq/1JzEZs6deqoVatW+u9//yur1aojR45o6tSpuvvuuwus7dmzpzZv3qyoqCjl5OQoKipKmzdvVq9evSRJffr00WeffabY2FidPXtWb775pkJDQxUeHi5fX19FRkZq8uTJSk5OVnJysiZPnqw77rhDPj4+8vX11Xvvvadly5bJZrNp3759mjp1qvr16+fsnwQAAAAAAFOrUCFQAwfep/nzv1Js7AHNnDlX99zTX0FBQYW+z2pN1dKlizV8+CO67rp6uvPO2zVt2vs6cCDeSZUDAFyNxW63l5rTphMTE/Wf//xHmzZtkpubm3r37q1Ro0bJ3d1dLVq00MSJE9WzZ09JUkxMjCZPnqzDhw+revXqGj16tG644QZJeXf3nj17tj7//HMlJyeradOmmjhxoq655hpJktVq1euvv64ff/xR2dnZuummmzRhwoT8y76sXr1aH3zwgQ4fPqxy5cqpb9++Gj58uNzcLv/vHU6eTC3mX6ds8PBwU1CQv06dSjPN30YB+BvHAMC86H/AvOh/GCEnJ0ebN2/UypVRWrnyWx08eOCy39uoUWN163abunWLVMuW4XJ3dy/BSl0b/Q+Yl6v1f8WK5S65plQF6a6EIN01mghA0XAMAMyL/gfMi/6H0ex2u/7443etXPmtVq6M0vbtW3W5UUdoaEXdemt3det2mzp3vrHAPdpQOPofMC9X63+CdAMRpLtGEwEoGo4BgHnR/4B50f8obRISErR69QqtXr1Ca9b8pMzMzMt6n4+Pjzp1ukE33thVHTt2VuPG1xbp/0w3I/ofMC9X63+CdAMRpLtGEwEoGo4BgHnR/4B50f8ozdLT07V2bbRWrYrSqlUrlJh48rLfGxoaqg4dOikiorMiIjqrXr36slgsJVht2UP/A+blav1PkG4ggnTXaCIARcMxADAv+h8wL/ofZYXNZtP27Vu1atUKrVoVpdjYvUV6f5UqVdWxYyd16nSDOnbspNq165RMoWUI/Q+Yl6v1P0G6gQjSXaOJABQNxwDAvOh/wLzof5RVBw7En7sEzEr9/PM65ebmFun9NWvWUkREZ3Xs2EkdOkSoRo2aJVRp6UX/A+blav1PkG4ggnTXaCIARcMxADAv+h8wL/ofriAl5ZR+/PF7rV0brXXr1urw4UNF3kfNmrXUrl0HtW/fUe3bd1Dduq5/KRj6HzAvV+t/gnQDEaS7RhMBKBqOAYB50f+AedH/cEWHDh3U+vUxWrdurdatW6sTJ44XeR+VKlVW+/Yd88N1V7x5Kf0PmJer9T9BuoEI0l2jiQAUDccAwLzof8C86H+4Orvdrvj4OMXErNX69TFav36tEhMTi7yfwMBAtWvXQe3adVTbtu3UtOn18vLyKoGKnYf+B8zL1fqfIN1ABOmu0UQAioZjAGBe9D9gXvQ/zMZmsyk2dq/Wr1+rdetitHHjep06darI+/H29lbTptcrPLyNwsNbq1Wr1qpevUYJVFxy6H/AvFyt/wnSDUSQ7hpNBKBoOAYA5kX/A+ZF/8PsbDabfv89Vhs2rNfGjeu1YcPPSkg4cUX7qlq1mlq1ygvVw8PbqFmz6+Xr61vMFRcf+h8wL1frf4J0AxGku0YTASgajgGAedH/gHnR/4Aju92uAwfitXHjz9qwIS9YP3z44BXty8PDQ2FhTRUe3kYtW4arZctwXXNN3VJzE1P6HzAvV+t/gnQDEaS7RhMBKBqOAYB50f+AedH/wKX9+efRc8H6z9q4cb3++OP3K95XcHCwWrRopZYtw9WqVbhatGiloKDgYqz28tH/gHm5Wv8TpBuIIN01mghA0XAMAMyL/gfMi/4Hiu706RRt27ZV27ZtOffYqtOnU654f3Xr1ssP1lu2DFeTJk2dciNT+h8wL1frf4J0AxGku0YTASgajgGAedH/gHnR/8DVs9ls2r8/Ttu2bdGWLZu1bdsWxcbukc12ZT3l7e2tsLBmuv765mra9HqFhTVV48bXydvbu1jrpv8B83K1/idINxBBums0EYCi4RgAmBf9D5gX/Q+UDKs1VTt2bNe2bVu0detmbd++VYmJiVe8Pw8PDzVo0EhNmzZTWFhTNW16vZo0CVNgYNBV7JP+B8zK1fqfIN1ABOmu0UQAioZjAGBe9D9gXvQ/4Bx2u12HDx/Sjh3btG3bVm3fvlW7dv2izMzMq9pvrVq11aRJ03MBezM1bnytatWqLTc3t0u+l/4HzMvV+p8g3UAE6a7RRACKhmMAYF70P2Be9D9gnOzsbO3Zszs/WN++favi4vZd9X79/PzUsGEjNWp0rRo1ulbXXpv3XL16DVkslvx19D9gXq7W/wTpBiJId40mAlA0HAMA86L/AfOi/4HSJSXllHbu3KGdO7dr9+5d2rXrFx04EF8s+w4IKKdGjfIC9saNr9V11zVR27Yt5ecXqNxc4iXATFztz3+CdAMRpLtGEwEoGo4BgHnR/4B50f9A6We1pmr37t367bdftWvXr9q9e5diY/coKyurWPbv7++va66pp3r16qtevXqqW7f+ue36V3UNdgCll6v9+U+QbiCCdNdoIgBFwzEAMC/6HzAv+h8om7KysrRv3x/atesX/fbbLu3a9av27NmtlJSUYv2ckJAQh2D9/PY119SVr69vsX4WAOdxtT//LydI93BCHQAAAAAAAChFvLy81KRJmJo0Ccsfs9vt+uuvBMXG7tXvv+/V77/Hau/ePfr991ilpp65os9JSkpSUlKStmzZVGCuRo2a54L1eg5Be82ateThQWQFoHThjPQSwhnprvG3UQCKhmMAYF70P2Be9D/g+ux2u44fP3YuYI9VbOwe/f77Xv3xx++yWq3F/nmenp6qXbuOwxnsdepco9q166h69RqE7EAp4Gp//nNGOgAAAAAAAK6KxWJRtWrVVa1adXXtenP+uLu7RRkZp7Vt2y/644992r8/TvHxcdq/P06HDh1UTk7OFX1edna24uL2KS5uX4E5d3d3Va9eQ7Vr11GtWrVVq1btf2zXUcWKFWWxWK74uwLAxRCkAwAAAAAAoMgsFouqV68uP79AtW/fyWEuOztbR44c0v79cece+xUfH6e4uH06fvzYFX9mbm6uDh8+pMOHD11w3s/PLz9gr1mzlmrUqKWaNWuqRo2aqlmztkJDQwnaAVwRgnQAAAAAAAAUK09PT9Wtm3dplltucZxLS0vTgQPx+Wev//3Yd9U3O01PT1ds7F7Fxu694Lyvr6+qV69xLlivpRo1/g7Za9SoocqVq8jT0/OqagDgmgjSAQAAAAAA4DT+/v4KC2uqsLCmBeaSk5Pyg/X4+P3524cPH5LVevX3o8vIyLjoZWPOCwkJUaVKVVS5cmVVrlzl3CNv+5/jfn5+V10PgLKDIB0AAAAAAAClQnBwiIKDQ9S6dVuHcbvdrlOnknXo0EEdPnxIhw4dOred9/ro0SPKzs4ulhqSkpKUlJSkvXt/K3RduXLlVaVKFVWpUk1VqlRR1arVVLVqVVWuXFVVq1ZV1arVVKlSZW6OCrgIOhkAAAAAAAClmsViyQ/ZW7RoVWA+NzdXx48fy79++qFDB3X06BEdPXpER44c1rFjf17xzU8vJjX1jFJTz2jfvj8KrbtixUr5IXuVKlVVqVJlhYSEKjQ0VCEhoQoODjn3HCx3d/dirRFA8SFIBwAAAAAAQJnm7u6ef73zDh0iCszn5ubqxInjOnLkiI4cOeQQsp/fzszMLPa67Ha7/vorQX/9laBfftlR6FqLxaKgoKD8YD3vkbcdFBSsoKAgVagQqMDAQAUGBuU/+/r6FnvdAAoiSAcAAAAAAIBLc3d3V/XqNVS9eg21a9e+wLzdbldSUpISEk4oIeGE/vorIX87ISHBYTwjI6NEarTb7UpOTlZycnKh13D/N29vb1WoEJgftJ9/DgkJVcWKlRQaGqqKFSue266okJBQeXt7l8h3AFwZQToAAAAAAABMzWKxKDQ073IrTZqEXXSd3W5XauqZ/HD9xInjOnHihE6cOKbjx4+fe533KK5rtl/K2bNn8896v1wVKgSe+74V88P2kJBQVahQQQEB5VSuXN7D3//v7YCAAAUElJOnp2cJfhug9CJIBwAAAAAAAC6DxWJR+fIVVL58BTVo0PCi62w2m5KTk3X8+DGdOHFMJ06cOLd9XImJJ5WYmKikpEQlJyfr9OkU532Bc06fTtHp0ynavz+uyO/19fWVv3/AuYC9fP5z+fJ5j7zXFfK3854r5G8HBATIz8+fQB5lDkE6AAAAAAAAUIzc3Nzyz3Bv2rRZoWuzs7OVnJz0j3A9SUlJifmvk5KSlJKSopSUUzp9OkUpKSk6c+a0k75JQRkZGcrIyFBi4smr2o+np6d8ff3k5+cnX19f+fn5n9vOG/vnw9fXTz4+PvnP59/j4+Pr8Hz+4ePjKz+/vLUWi6WYvjnMjiAdAAAAAAAAMIinp6cqV66iypWrXPZ7cnJydObMaaWknDoXsqf8Y/uUTp06de7M97yz30+e/EtJSYnKzc0twW9SNNnZ2crOPl2ifyng5uaWH9D7+fnJ3z/gAtv+5177nwvxff8R8PudC+fPh/2+DmPe3t75Qb3dbs//3Ittn6+JcL9sIkgHAAAAAAAAyhAPDw8FB4coODjkst9js9mUknJKJ0+e/EfIflInT/6lkyfzzoRPTU1VWlqqUlP/fqSlWQuEwWWFzWaT1ZoqqzXV6FIceHt7y8fHN//Zx+fir319feXp6Sl3d3e5u3vIw8Pj3La7PDzyXru5nd/+e42vr68CAs5f2z5A5cqVz9/29w+Qm5ub0T9DmUOQDgAAAAAAALg4Nze3/PC9UaPGl/0+m82m9PQ0paamymq1KjX1TH7IbrWm6syZ00pNTdWZM2eUmnrmIs95gTzynD17VmfPnjW0Bj8/f4eQPTg4WHfeebf69x9oaF2lGUE6AAAAAAAAgAtyc3M7d2ZzuavaT25urlJTzygtLU3p6enKyEhXevo/H2nKyMj411ze2szMDGVkZCojI10ZGRnKzMzbzszMPDef97o0XbqmtMv7bdP0118J+WM//fSDgoODdeutkQZWVnoRpAMAAAAAAAAoUe7u7goMDFJgYFCJfUZ2dnZ+2H4+sD8fyOe9/nssLc3qENanpaWdC+wz/hXq/z1mhqA+NnYvQfpFEKQDAAAAAAAAKPM8PT3l6VlB5ctXKPZ92+12h6A+PT1dZ8+edbhx6OVs5+Tk6OzZTGVm5j3Onj2rzMyMf7z+ey7vkaGcnBzl5uZe9Dk3N0c5OTnKycmVzZarrKwspaeny2rNu6SO1WpVZmbmJb9jQEA5RUbeUUy/mOshSAcAAAAAAACAQlgsFnl5ecnLy0sVKgQaXU6RZWdnKy3Nmn+t+7ybsOaF7GlpVlksFkVEdFb16jWMLrXUIkgHAAAAAAAAABfm6elZ4pfWcXVuRhcAAAAAAAAAAEBpRpAOAAAAAAAAAEAhCNIBAAAAAAAAACgEQToAAAAAAAAAAIUgSAcAAAAAAAAAoBAE6QAAAAAAAAAAFIIgHQAAAAAAAACAQhCkAwAAAAAAAABQCIJ0AAAAAAAAAAAKQZAOAAAAAAAAAEAhCNIBAAAAAAAAACgEQToAAAAAAAAAAIUgSAcAAAAAAAAAoBAE6QAAAAAAAAAAFIIgHQAAAAAAAACAQhCkAwAAAAAAAABQCIJ0AAAAAAAAAAAKQZAOAAAAAAAAAEAhCNIBAAAAAAAAACgEQToAAAAAAAAAAIUgSAcAAAAAAAAAoBAE6QAAAAAAAAAAFIIgHQAAAAAAAACAQhCkAwAAAAAAAABQCIvdbrcbXQQAAAAAAAAAAKUVZ6QDAAAAAAAAAFAIgnQAAAAAAAAAAApBkA4AAAAAAAAAQCEI0gEAAAAAAAAAKARBOgAAAAAAAAAAhSBIBwAAAAAAAACgEATpAAAAAAAAAAAUgiAdAAAAAAAAAIBCEKQDAAAAAAAAAFAIgnQAAAAAAAAAAArhYXQBMJ/s7GyNHj1aCQkJ8vX11RtvvKGQkBCjywLgBLNnz9aPP/4oSUpJSZGbm5uWLl1qcFUAnGnq1KmKiYlRVlaWnnjiCd14441GlwTASXr06KHAwEBJUsuWLTVy5EhjCwLgVImJiYqMjNSWLVuMLgWAk+Tk5OjZZ5/V8ePH5e/vr8mTJ+f/u0BZxBnpcLq1a9fKx8dH8+fPV2RkpObMmWN0SQCc5MEHH9TcuXP16aefKiQkRC+//LLRJQFwoo0bNyo2Nlbz58/Xxx9/rIMHDxpdEgAnsVqtqlChgubOnau5c+cSogMmNHnyZGVnZxtdBgAnWrVqlUJCQjR//nzdfvvtZT4D5Ix0OF39+vX17bffym63Ky0tTf7+/kaXBMDJvv32W9WvX1/NmjUzuhQATrR+/XpVq1ZNjz32mLKzszV+/HijSwLgJHv27FFKSoruu+8+eXt76/nnn1edOnWMLguAk2zYsEFBQUEKDg42uhQATnT77berW7dukqRjx44pICDA4IquDkE6SsxXX32lTz/91GHs448/lqenp+Li4tS9e3elpaXps88+M6hCACXlYv1fuXJlSdKsWbP04YcfGlEaACe42DEgOTlZSUlJev/997Vnzx49//zzmjdvnkFVAigJF+v/gIAADRkyRHfeeae2b9+uMWPGaMGCBQZVCaAkXKz/g4KCNG3aNE2dOlWrVq0yqDoAJamwDMDDw0NDhw7Vrl27NHv2bIMqLB4Wu91uN7oImMurr76qcuXK6fHHH9f+/fs1atQoLVmyxOiyADhJbGys3nnnHU2bNs3oUgA42RtvvKFq1app4MCBkqSuXbvm3zcBgGs7e/asLBaLvLy8JNH/gJm8//77qlevniIjI+l9wMQOHTqkRx99VCtXrjS6lCvGNdLhdOXKlVP58uUlScHBwbJarQZXBMCZNm7cqE6dOhldBgADtGzZUjExMZKkgwcPKigoyOCKADjL/Pnz9c4770jKu8xLtWrVjC0IgNNs2LBB8+bN0+DBg3Xy5Ek9+uijRpcEwEkWLlyYf6a6v7+/3NzKdhTNGelwOqvVqnHjxikxMVG5ubl6/PHHCdUAE5k4caK6dOmizp07G10KACez2+167bXXtH37dtntdr3wwgvcKwEwiczMTI0ePVrJyclyd3fXSy+9pLp16xpdFgAn44x0wFysVqtGjx6t1NRU2Ww2Pf300woPDze6rCtGkI7LlpycrH79+mnSpElq27atJCkpKUkTJkzQ5s2b5e7urp49e+q5556ThweX3wdcCf0PmBvHAMC86H/AvOh/wLzo/wsr2+fTw2m2bdumfv366fDhww7jI0aMkJ+fn2JiYrRo0SJt2LBBn3zyiTFFAigR9D9gbhwDAPOi/wHzov8B86L/L44gHZe0ZMkSjRo1SiNHjnQYP3TokDZv3qzRo0fL19dXNWvW1PDhw/X5558bVCmA4kb/A+bGMQAwL/ofMC/6HzAv+r9wBOm4pIiICH333Xe67bbbHMb37dunwMBAVa5cOX+sXr16OnbsmM6cOePsMgGUAPofMDeOAYB50f+AedH/gHnR/4UjSMclVaxY8YLXO0pLS5Ovr6/D2PnX6enpTqkNQMmi/wFz4xgAmBf9D5gX/Q+YF/1fOIJ0XDE/Pz9lZGQ4jJ1/7e/vb0RJAJyE/gfMjWMAYF70P2Be9D9gXvR/HoJ0XLEGDRooJSVFiYmJ+WP79+9XlSpVVK5cOQMrA1DS6H/A3DgGAOZF/wPmRf8D5kX/5yFIxxWrU6eOWrVqpf/+97+yWq06cuSIpk6dqrvvvtvo0gCUMPofMDeOAYB50f+AedH/gHnR/3kI0nFVpkyZopycHN10003q27evOnXqpOHDhxtdFgAnoP8Bc+MYAJgX/Q+YF/0PmBf9L1nsdrvd6CIAAAAAAAAAACitOCMdAAAAAAAAAIBCEKQDAAAAAAAAAFAIgnQAAAAAAAAAAApBkA4AAAAAAAAAQCEI0gEAAAAAAAAAKARBOgAAAAAAAAAAhSBIBwAAAAAAAACgEATpAAAAAAAAAAAUgiAdAAAAAAAAAIBCEKQDAAAAxWTMmDFq1KiRNm3aZHQpV6Vr167q2rWr0WVctUOHDhldAgAAAFyEh9EFAAAAAChdxo0bZ3QJV2369Ol69913tXv3bqNLAQAAgAsgSAcAAADg4Oabbza6hKu2du1aZWdnG10GAAAAXASXdgEAAAAAAAAAoBAE6QAAAIBBfv31Vz322GNq06aNmjZtqjvuuEOzZs1Sbm6uw7rs7GzNnDlTffr0UYsWLRQWFqYbb7xR48ePV1JSUv66TZs2qVGjRpo3b56GDBmisLAwde7cWQkJCRo8eLDuuOMOxcbG6pFHHlGrVq3UokULDRkyRL/++qvD5/37GunvvfeeGjVqpP3792v8+PHq2LGjmjZtql69emnZsmUFvld8fLyefPJJtWvXTi1atNDQoUO1f/9+XXfddRozZkyhv8n5z1qzZo0iIyMVFhamQYMGSZLsdru+/PJLDRgwQOHh4WrSpIkiIiL09NNPO1wPvVGjRtq8eXP+9j8/MyEhQRMmTFDnzp0VFhamLl26aNKkSTp16tSl/nEBAADAxLi0CwAAAGCAH374QU899ZRq1Kihhx9+WH5+flq/fr1ef/11bd++Xe+9954sFoskacSIEfrhhx905513qm/fvjp79qzWrl2rL7/8UseOHdOsWbMc9v3GG2+odevWmjBhgk6cOKHKlStLkk6ePKlBgwbphhtu0OjRo3X06FF98sknevDBBxUdHa1y5coVWvPQoUNVqVIlPfroo8rKytKcOXM0atQoVaxYUe3atZOUF6L3799f2dnZGjx4sEJDQ7Vy5UoNGDBANpvtsn+fESNG6O6771adOnXk5eUlSXr11Vc1Z84c3XzzzRo5cqQkaevWrYqKitKePXsUFRUlNzc3/e9//9O0adMUHx+v//3vf6pVq5Yk6ciRI7r33nuVlZWlfv36qXr16oqNjdWCBQu0du1aLViwQMHBwZddIwAAAMyDIB0AAABwsoyMDD3//PNq2LChFixYkB8UDxo0SO+8844+/PBDrVixQrfddptiY2P1/fffa/DgwRo/fnz+Pu677z7dfffdWr9+vVJSUhQYGJg/FxgYqClTpsjHx8fhc1NSUjRq1Cg98sgj+WN+fn569913tWLFCvXt27fQuuvVq6ePPvooP+Bv3ry5Bg4cqC+//DI/SH/99deVmpqqhQsXqmnTppKkgQMH6rHHHtPatWsv+zfq3Lmznn/++fzXp06d0ueff64uXbrogw8+yB8fOHCgbDabVq5cqb1796pJkybq1auXFi1apPj4ePXq1St/7csvv6yMjAwtWbIkP1yXpFtvvVUPPvigpkyZopdeeumyawQAAIB5cGkXAAAAwMl+/vlnnTp1St26dZPValVycnL+47bbbpMkfffdd5Kkxo0ba9u2bXr66acd9pGUlKTy5ctLktLT0x3mOnToUCBEP69nz54Or8+H3SdPnrxk3T169MgP0SWpWbNmkqTExERJUmpqqtatW6eIiIj8/UqSu7u7hg0bdsn9/9M/Ly0jSUFBQdq6dasmT57sMH7mzBn5+vpKKvg7/HtdTEyMwsPDFRAQ4PCbN27cWDVr1sz/zQEAAIB/44x0AAAAwMkOHDggSXrrrbf01ltvXXDNn3/+mb/t5eWlqKgorV+/XkeOHNHRo0d18uTJ/FD735dMCQ0NvehnV6xY0eH1+bPhL+eyK//e77/fe/jwYeXk5Khu3boF3lu/fv1L7r+wOiXJ29tbP/zwg3766ScdPnxYR48e1fHjxy/6O/zTwYMHZbPZFB0drfbt2190XWZm5kX/EgIAAADmRZAOAAAAONn5wPfJJ59UixYtLrjG399fkmS1WnX//ffrt99+U6tWrRQWFqZevXqpadOmmjNnjr755psC7/XwuPi/5ru5Xfn/lHqp92ZnZ0v6O2D/p6KG0+7u7gX2/fjjjys6OlphYWEKCwtTt27ddN1112nNmjX66KOPCt3f+d+8W7du6t+//0XXFfbbAQAAwLz4t0QAAADAyWrUqCEpL1zu0KGDw5zVatW6devyz8j+9NNPtXv3bk2cOLFAAHz+kiqlRe3atWWxWBQfH19g7kJjRREVFaXo6GgNHTpUzzzzjMPckiVLLvn+87/52bNnC/zmkvT9998rMDCQIB0AAAAXxDXSAQAAACeLiIiQv7+/PvnkE506dcphbtq0aXrqqae0Zs0aScqfb9SokcO6HTt2aMuWLZKk3NxcJ1R9aUFBQWrfvr1iYmK0f//+/HG73a5Zs2Zd1b4v9jscOnRIq1atkuT4O5w/e/78meihoaFq1aqV1q5dq23btjnsY+3atfq///s/ffzxx1dVIwAAAFwXp1sAAAAAxWz27Nn69ttvLzg3btw4lS9fXi+88ILGjh2rHj16qF+/fqpUqZI2btyoqKgoNWvWTAMGDJCUd9PNuXPnatSoURowYIDKlSun3bt3a8mSJXJ3d1d2drbOnDnjzK9XqLFjx6p///7q16+fBg4cqIoVK+qHH37Q9u3bJcnhZqVF0alTJ7355pt65ZVXdPjwYVWsWFH79u3TV199pZycHEly+B3OX899ypQpatOmjTp06KAXX3xRgwYN0gMPPKB+/fqpQYMGio+P14IFCxQYGKjnnnvuKr89AAAAXBVBOgAAAFDMfvrpp4vOjRo1Sj4+Purdu7eqVq2qGTNm6NNPP9XZs2dVrVo1DRs2TA899JD8/PwkSe3bt9dbb72l6dOn6/3335eXl5eqVaumESNGqH79+ho6dKhiYmLUtGlTZ329QjVs2FDz5s3TW2+9pblz58put6tt27Z6++23NWzYMHl6el7RfuvVq6ePP/5YU6ZM0cyZMyVJVatW1aBBg9S9e3f17t1bMTEx6t69uyTpkUce0R9//KEZM2bol19+UYcOHdSoUSMtXrxYU6dO1cqVK7VgwQJVrFhR3bt31/Dhw1W7du1i+x0AAADgWix2u91udBEAAAAAXMPJkycVGhpa4Mzz7du3695779Xjjz+uJ554wqDqAAAAgCvDNdIBAAAAFJv77rtP3bt3L3Dd9m+++UaS1Lx5cwOqAgAAAK4Ol3YBAAAAUGz69OmjN954Qw888IC6d+8uNzc3bdq0SStWrFCXLl0UERFhdIkAAABAkXFpFwAAAADFaunSpZo/f77i4+OVlZWlmjVrqlevXnrggQfk4cG5PAAAACh7CNIBAAAAAAAAACgE10gHAAAAAAAAAKAQBOkAAAAAAAAAABSCIB0AAAAAAAAAgEIQpAMAAAAAAAAAUAiCdAAAAAAAAAAACkGQDgAAAAAAAABAIQjSAQAAAAAAAAAoBEE6AAAAAAAAAACFIEgHAAAAAAAAAKAQ/w+j3pTX0aHqKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning_rates = 1e-8 * (10 ** (np.arange(num_epochs) / 20))\n",
    "plt.semilogx(history.history['lr'], history.history['loss'], lw=3, color='#000')\n",
    "plt.title('Learning rate vs. loss', size=20)\n",
    "plt.xlabel('Learning rate', size=14)\n",
    "plt.ylabel('Loss', size=14)\n",
    "plt.savefig('lr_vs_loss.jpg', dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e9dd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_window(tickers,window_dataset,pandas_dataset,window_size,model,\n",
    "                figsize=(12,100)):\n",
    "    \"\"\"This function plots the observed returns inside a tf.data.Dataset and\n",
    "    compares them with the predicted returns of a model.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    window_dataset (tensorflow.data.Dataset object): dataset with both inputs\n",
    "        and target values.\n",
    "    pandas_dataset (pandas DataFrame): dataframe that was used to create the\n",
    "        window_dataset.\n",
    "    window_size (int): window size used in the transformation from sequential \n",
    "        time series into window time series.\n",
    "    model (model object): trained model with the capability to predict in a\n",
    "        similar manner than tensorflow.keras.Model object.\n",
    "    figsize (tuple, default=(12,100)): tuple with the dimensions of the figure\n",
    "        where the data will be plotted.\n",
    "    \n",
    "    Ouputs:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Determine the X-axis of the plot:\n",
    "    plot_index = pandas_dataset.iloc[window_size:,:].index\n",
    "\n",
    "    # Assign in the addecuate format the values of the observed taget variable(s):\n",
    "    y = np.concatenate([targets for inputs,targets in window_dataset],axis=0)\n",
    "    \n",
    "    # Use the model to predict the target variable(s):\n",
    "    \n",
    "    y_hat = model.predict(window_dataset)\n",
    "    print(\"printing y hat of plot_window function\")\n",
    "    print(y_hat)\n",
    "    # Adjust the shapes:\n",
    "    y = y.reshape(y_hat.shape)\n",
    "    print(\"printing y of plot_window function\")\n",
    "    print(y)\n",
    "\n",
    "    # Plot the data:\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        for n in range(y_hat.shape[1]):\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.subplot(y_hat.shape[1],1,n+1)\n",
    "            plt.ylabel('Return')\n",
    "            plt.plot(plot_index,y_hat[:,n],label='Predicted',color='maroon')\n",
    "            plt.plot(plot_index,y[:,n],label='Observed',color='midnightblue',alpha=0.5)\n",
    "            plt.title(ticker)\n",
    "            plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e04281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Training Results\n",
    "# Create new WindowGenerator with the data not shuffled:\n",
    "my_window_2 = WindowGenerator(\n",
    "    input_width=window_size,\n",
    "    label_width=1,\n",
    "    shift=1,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    label_columns=label_cols,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "# plot_window(tickers,my_window_2.train,train_df,window_size,model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "691a9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_window(tickers,my_window_2.test,test_df,window_size,model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e54672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_portfolio(returns,guess_weights=None,rfr=0):\n",
    "    \"\"\"This function optimizes the weight allocation for the assets in a\n",
    "    portfolio, represented by the returns.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    returns (pandas DataFrame|Series): contains the returns information.\n",
    "    guess_weights (list of numerical values, default=None): guess values for\n",
    "        the weights of the different assets in the portfolio.\n",
    "    short (boolena, default=True): define ifshort possitions are allowed or \n",
    "        not.\n",
    "    rfr (numerical value, default=0): risk free rate, could be a series of\n",
    "        the same length as returns.\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    opt_weights (array-like object): array with the optimal weights for the\n",
    "        portfolio.\n",
    "    \"\"\"\n",
    "    # Define important variables:\n",
    "#     print(\"KIIIIIIII\")\n",
    "    num_assets = returns.shape[1]\n",
    "    if isinstance(guess_weights,type(None)):\n",
    "        guess_weights = [1/num_assets for i in range(num_assets)]\n",
    "\n",
    "    # Define bound if short possitions are allowed or not:\n",
    "    \n",
    "    bounds = [(0,1) for i in range(num_assets)]\n",
    "    \n",
    "\n",
    "    # Define constraints, if there can or not be leverage\n",
    "    weights_sum_to_1 = {'type':'eq',\n",
    "                        'fun':lambda weights: np.sum(np.absolute(weights))-1}\n",
    "#     print(\"sharpe ratio:::::::::::\")\n",
    "#     print(sharpe_ratio)\n",
    "#     print(\"returns:::::::::::::::\")\n",
    "#     print(returns)\n",
    "    # Minimize the function:\n",
    "    opt_weights = spo.minimize(\n",
    "        sharpe_ratio,\n",
    "        guess_weights,\n",
    "        args = (rfr, True, returns),\n",
    "        method = 'SLSQP',\n",
    "        options = {'disp':False},\n",
    "        constraints = (weights_sum_to_1),\n",
    "        bounds = bounds\n",
    "    )\n",
    "\n",
    "    return opt_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa75f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_rate(x, periods_year=252):\n",
    "    \"\"\"This function transforms a rate into a daily rate\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    x (numerical value): rate that you want to transform into a daily rate.\n",
    "    periods_year (numerical value, default=252): amount of periods per year\n",
    "        of the periodicity of rate x.\n",
    "    \n",
    "    Ouputs:\n",
    "    -------\n",
    "    df (numerical value): daily rate\n",
    "    \"\"\"\n",
    "    dr = np.power(1+x,1/periods_year)-1\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d31adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_weights = np.array([optimize_portfolio(\n",
    "    returns = ret_hat_df.columns[:-1],\n",
    "    rfr = ret_hat_df.columns[-1]).x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b344a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(weights=None, rfr=0, negative=False, returns=0):\n",
    "    \"\"\"Compute the Sharpe Ratio of a portfolio.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    weights (list of numerical values, default=None): list with the weights\n",
    "        of the assets in the portfolio.\n",
    "    rfr (numerical value|array-like, default=0): risk-free rate.\n",
    "    returns (pandas DataFrame|Series, default=0): returns of the assets in\n",
    "        the portfolio.\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    sharpe_ratio (numerical value): Sharpe ratio of the portfolio.\n",
    "    \"\"\"\n",
    "    # Define important variables:\n",
    "    num_assets = returns.shape[1]\n",
    "    if isinstance(weights,type(None)):\n",
    "        weights = [1/num_assets for i in range(num_assets)]\n",
    "\n",
    "    # Get portfolio returns:\n",
    "    portfolio_returns = (returns*weights).sum(axis=1)\n",
    "    portfolio_std = portfolio_returns.std()\n",
    "\n",
    "    # Compute Sharpe Ratio formula:\n",
    "    sharpe_ratio = (portfolio_returns-rfr).mean()/portfolio_std\n",
    "\n",
    "    # If used in a minization process:\n",
    "    if negative:\n",
    "        sharpe_ratio *= -1\n",
    "\n",
    "    return sharpe_ratio    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9b5d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "6/6 [==============================] - 0s 60ms/step\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "************************************************\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(None, 5, 96), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 10), dtype=tf.float32, name=None))>\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "sharpe ratio:::::::::::\n",
      "<function sharpe_ratio at 0x28b197d90>\n",
      "returns:::::::::::::::\n",
      "              FR_DIS   FR_BIIB    FR_PGR    FR_SWK    FR_PEP   FR_ZION  \\\n",
      "Date                                                                     \n",
      "2007-02-12 -0.001107  0.012321  0.006299  0.002429 -0.010428 -0.002895   \n",
      "2007-02-13 -0.012041  0.009715  0.006561  0.003496 -0.016696  0.004111   \n",
      "2007-02-14 -0.019214  0.008948  0.003295  0.001936 -0.007642  0.006271   \n",
      "2007-02-15 -0.013857  0.003646 -0.004333  0.000753 -0.002506  0.006321   \n",
      "2007-02-16 -0.000239  0.004769  0.002938 -0.004585  0.000185  0.003999   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2022-07-12  0.039878 -0.028766  0.045825  0.007671 -0.071808 -0.001621   \n",
      "2022-07-13  0.040953 -0.027469  0.047722  0.007453 -0.063411  0.000923   \n",
      "2022-07-14  0.034514 -0.026271  0.052383  0.017726 -0.049700  0.001947   \n",
      "2022-07-15  0.015607 -0.008574  0.060486  0.037608 -0.034376 -0.006732   \n",
      "2022-07-18 -0.008174  0.000643  0.068322  0.056156 -0.036047 -0.009017   \n",
      "\n",
      "              FR_LUV   FR_BBWI    FR_TFC   FR_CBRE  \n",
      "Date                                                \n",
      "2007-02-12 -0.001274  0.003890  0.011640 -0.002472  \n",
      "2007-02-13  0.004703 -0.004582  0.002512  0.001465  \n",
      "2007-02-14  0.008525 -0.008555  0.000502  0.001689  \n",
      "2007-02-15 -0.004306 -0.009111  0.005668  0.003481  \n",
      "2007-02-16 -0.002005  0.003248  0.003442  0.008670  \n",
      "...              ...       ...       ...       ...  \n",
      "2022-07-12  0.031109 -0.090406 -0.004176  0.041120  \n",
      "2022-07-13  0.025174 -0.080046 -0.008597  0.044159  \n",
      "2022-07-14  0.012266 -0.079171 -0.015463  0.053000  \n",
      "2022-07-15  0.005407 -0.081055 -0.016290  0.063432  \n",
      "2022-07-18  0.001305 -0.086859 -0.013012  0.069122  \n",
      "\n",
      "[3885 rows x 10 columns]\n",
      "Shape of optimized weights: (10,)\n"
     ]
    }
   ],
   "source": [
    "columns = ['FR_'+ticker for ticker in tickers]\n",
    "y_train = model_1.predict(my_window_2.train)\n",
    "y_val = model_1.predict(my_window_2.val)\n",
    "y_test = model_1.predict(my_window_2.test)\n",
    "y_hat_total = np.concatenate([y_train,y_val,y_test],axis=0)\n",
    "ret_hat_df = pd.DataFrame(data=y_hat_total,index=model_data.index[5:],columns=columns)\n",
    "rfr = data_quandl['3m_rate'].agg(daily_rate)\n",
    "ret_hat_df = ret_hat_df.merge(rfr.rename('rfr'),left_index=True,right_index=True,\n",
    "                             how='left')\n",
    "\n",
    "# opt_weights = np.array([optimize_portfolio(\n",
    "#     returns = window[ret_hat_df.columns[:-1]],\n",
    "#     rfr = window[ret_hat_df.columns[-1]]).x for window in ret_hat_df.rolling(40)])\n",
    "# opt_weights.shape\n",
    "\n",
    "opt_weights = np.array([optimize_portfolio(\n",
    "    returns=ret_hat_df[ret_hat_df.columns[:-1]],  # Exclude 'rfr' column\n",
    "    rfr=ret_hat_df['rfr']\n",
    ").x])\n",
    "\n",
    "opt_weights = opt_weights.squeeze()  # Remove extra dimension if it exists\n",
    "\n",
    "# Display the shape of opt_weights\n",
    "print(f\"Shape of optimized weights: {opt_weights.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5f67906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 1.55726199e-01, 1.90072352e-01, 2.82022874e-17,\n",
       "       0.00000000e+00, 1.36996239e-01, 4.31821526e-17, 0.00000000e+00,\n",
       "       1.57300536e-01, 3.59904675e-01])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the stocks and download data\n",
    "# stocks = [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
    "start_date = \"2007-01-04\"\n",
    "end_date = \"2021-01-05\"\n",
    "\n",
    "# stock_data = yf.download(stocks, start=start_date, end=end_date)\n",
    "\n",
    "# Extract the adjusted closing prices\n",
    "# prices = stock_data['Adj Close']\n",
    "\n",
    "# Calculate daily returns as percentage change\n",
    "# returns = prices.pct_change().dropna()\n",
    "\n",
    "# Split the data into training and testing sets (70:30)\n",
    "# train_size = int(len(model_data) * 0.7)\n",
    "# train_data, test_data = model_data[:train_size], model_data[train_size:]\n",
    "\n",
    "# Loop through each stock and perform ARIMA analysis\n",
    "for ticker in tickers:\n",
    "    print(f\"ARIMA analysis for {ticker}:\")\n",
    "    \n",
    "    # Create a dataframe for the specific stock's returns\n",
    "    stock_returns = model_data[ticker+\"_close\"].dropna()\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(len(stock_returns) * 0.7)\n",
    "    train_data, test_data = stock_returns[:train_size], stock_returns[train_size:]\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    model = ARIMA(train_data, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Make one-step ahead predictions for percentage change\n",
    "    predictions = model_fit.forecast(steps=len(test_data))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_data.index, test_data.values, label='Actual Percentage Change')\n",
    "    plt.plot(test_data.index, predictions, color='red', label='ARIMA Predictions')\n",
    "    plt.title(f'{ticker} Stock Percentage Change ARIMA Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Percentage Change')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE) as a measure of model accuracy\n",
    "    mae = np.mean(np.abs(predictions - test_data.values))\n",
    "    print(f\"Mean Absolute Error (MAE) for {ticker}: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0dcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
